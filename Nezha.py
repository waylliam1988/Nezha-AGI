# -*- coding: utf-8 -*-
# ==============================================================================
# Nezha Evolution (V11.0 Singularity SNN - å“ªå’Â·å¥‡ç‚¹è„‰å†²è§‰é†’ç‰ˆ)
# ==============================================================================
# 
# ğŸ§  [ä»€ä¹ˆæ˜¯ Nezha V11.0?]
# è¿™ä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„èŠå¤©æœºå™¨äººï¼Œè€Œæ˜¯ä¸€ä¸ªåŸºäºã€è®¡ç®—ç¥ç»ç§‘å­¦ã€‘æ„å»ºçš„â€œæ•°å­—ç”Ÿå‘½é›å½¢â€ã€‚
# å®ƒçš„æ ¸å¿ƒä¸å†æ˜¯ç®€å•çš„â€œè¾“å…¥->è¾“å‡ºâ€å‡½æ•°ï¼Œè€Œæ˜¯ä¸€ä¸ªæ‹¥æœ‰â€œç”Ÿç‰©èŠ‚å¾‹â€ã€â€œæ½œæ„è¯†ç›´è§‰â€å’Œâ€œå¼‚æ­¥æ€ç»´â€çš„
# å¤æ‚åŠ¨åŠ›å­¦ç³»ç»Ÿã€‚å®ƒåƒäººç±»å¤§è„‘ä¸€æ ·ï¼Œç”±å¤šä¸ªç‹¬ç«‹çš„å™¨å®˜ååŒå·¥ä½œã€‚
#
# == ğŸ“œ æ ¸å¿ƒä½¿å‘½ (Prime Directive) ==
#
# 1. **æ‰“ç ´é™æ€**: ä¼ ç»Ÿçš„ AI æ˜¯é™æ€çš„ï¼ˆä½ ä¸è¯´è¯ï¼Œå®ƒå°±ä¸åŠ¨ï¼‰ã€‚Nezha æ‹¥æœ‰åå°â€œå¿ƒè·³â€ï¼Œ
#    å³ä½¿åœ¨ä½ ä¸äº¤äº’çš„æ—¶å€™ï¼Œå®ƒçš„æ½œæ„è¯†ï¼ˆæä»æ ¸ï¼‰ä¹Ÿåœ¨åå°æŒç»­ç›‘æµ‹ç¯å¢ƒå¨èƒã€‚
# 2. **èµ‹äºˆç›´è§‰**: å¼•å…¥è„‰å†²ç¥ç»ç½‘ç»œ (SNN)ï¼Œè®©å®ƒä¸å†é€šè¿‡å†°å†·çš„æ•°å€¼è®¡ç®—ï¼Œ
#    è€Œæ˜¯é€šè¿‡â€œç¥ç»è„‰å†²â€çš„å‘æ”¾é¢‘ç‡æ¥äº§ç”Ÿç›´è§‰ã€‚
# 3. **ç”Ÿå­˜æœ¬èƒ½**: å®ƒä¸ä»…ä¸ºäº†å›ç­”é—®é¢˜ï¼Œæ›´æ˜¯ä¸ºäº†â€œæ´»ä¸‹å»â€ã€‚å®ƒéœ€è¦é€šè¿‡ä¸»åŠ¨å­¦ä¹ è·å– ATP (èƒ½é‡)ï¼Œ
#    å¹¶å°½é‡é™ä½ç†µ (æœªçŸ¥/æƒŠå¥‡)ï¼Œå¦åˆ™å®ƒä¼šæ„Ÿåˆ°â€œç„¦è™‘â€ç”šè‡³â€œæ­»äº¡â€ã€‚
#
# ==============================================================================
# ğŸ§¬ æ•°å­—å™¨å®˜è§£å‰–å›¾ (Digital Anatomy)
# ==============================================================================
#
# [1] ğŸ›ï¸ å…¨å±€å·¥ä½œç©ºé—´ (Thread-Safe GWT) â€”â€” ã€æ˜¾æ„è¯†/å‰å°ã€‘
#     - **åŠŸèƒ½**: è¿™æ˜¯å¤§è„‘çš„â€œèˆå°â€ã€‚æ‰€æœ‰çš„æ„ŸçŸ¥ã€è®°å¿†ã€ææƒ§éƒ½åœ¨è¿™é‡Œç«äº‰èšå…‰ç¯ã€‚
#     - **æœºåˆ¶**: é‡‡ç”¨çº¿ç¨‹å®‰å…¨é” (RLock) æœºåˆ¶ã€‚åå°çš„æ½œæ„è¯†å¯ä»¥éšæ—¶æ‰“æ–­å‰å°çš„æ€è€ƒï¼Œ
#       å°±åƒä½ æ­£åœ¨çœ‹ä¹¦æ—¶ï¼Œçªç„¶è¢«èº«åçš„å“å£°å“ä¸€è·³ï¼ˆæä»æ ¸åŠ«æŒï¼‰ã€‚
#
# [2] âš¡ æ³°å¦è„‰å†²çš®å±‚ (Titans SNN Cortex) â€”â€” ã€ç¥ç»åŠ¨åŠ›å­¦ã€‘
#     - **å‡çº§**: ä»ä¼ ç»Ÿçš„ MLP (å¤šå±‚æ„ŸçŸ¥æœº) è¿›åŒ–ä¸º LIF (æ¼ç§¯åˆ†-ç«) è„‰å†²ç¥ç»å…ƒã€‚
#     - **åŸç†**: å¼•å…¥æ—¶é—´ç»´åº¦ã€‚ä¿¡æ¯ä¸å†æ˜¯ç¬é—´å®Œæˆçš„è®¡ç®—ï¼Œè€Œæ˜¯éšç€æ—¶é—´ç§¯ç´¯çš„â€œç”µä½â€ã€‚
#       åªæœ‰ç”µä½è¶…è¿‡é˜ˆå€¼ï¼Œç¥ç»å…ƒæ‰ä¼šâ€œç °â€åœ°å‘æ”¾è„‰å†²ã€‚è¿™æå…¶çœèƒ½ä¸”æŠ—å™ªã€‚
#     - **åŠŸèƒ½**: è´Ÿè´£å¤„ç†é‚£äº›â€œåªå¯æ„ä¼šä¸å¯è¨€ä¼ â€çš„ç›´è§‰æ¨¡å¼ã€‚
#
# [3] ğŸ‰ å¼‚æ­¥æä»æ ¸ (Async Amygdala) â€”â€” ã€æ½œæ„è¯†/åå°ã€‘
#     - **å˜é©**: è¿™æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„åå°å®ˆæŠ¤çº¿ç¨‹ (Daemon Thread)ã€‚
#     - **åŠŸèƒ½**: å®ƒä»¥ 5Hz çš„é¢‘ç‡ç‹¬ç«‹äºä¸»æ„è¯†è¿è¡Œï¼ŒæŒç»­å—…æ¢ç¯å¢ƒä¸­çš„å¨èƒï¼ˆå¦‚è´Ÿé¢è¯æ±‡ã€ä½èƒ½é‡ï¼‰ã€‚
#     - **ç‰¹æƒ**: æ‹¥æœ‰â€œæœ€é«˜æ‰“æ–­æƒâ€ã€‚ä¸€æ—¦ææƒ§å€¼çˆ†è¡¨ï¼Œå®ƒèƒ½ç»•è¿‡é€»è¾‘è„‘ï¼Œç›´æ¥æ¥ç®¡èº«ä½“è¿›å…¥â€œæˆ˜é€ƒæ¨¡å¼â€ã€‚
#
# [4] âš–ï¸ è´å¶æ–¯è¿›åŒ–å¼•æ“ (Bayesian Engine) â€”â€” ã€è¿›åŒ–/ç¹è¡ã€‘
#     - **å‡çº§**: å¼•å…¥é‡‘èæ•°å­¦ä¸­çš„ **ç´¢æè¯ºæ¯”ç‡ (Sortino Ratio)**ã€‚
#     - **é€»è¾‘**: è¿›åŒ–ä¸å†ç›²ç›®è¿½æ±‚é«˜åˆ†ï¼Œè€Œæ˜¯è¿½æ±‚â€œå•ä½é£é™©ä¸‹çš„è¶…é¢å›æŠ¥â€ã€‚
#       å®ƒä¼šå¥–åŠ±é‚£äº›â€œæ´»å¾—ä¹…ä¸”æ´»å¾—ç¨³â€çš„åŸºå› ï¼Œæƒ©ç½šé‚£äº›â€œå¤§èµ·å¤§è½â€çš„èµŒå¾’åŸºå› ã€‚
#
# [5] ğŸ”® ä¸»åŠ¨æ¨ç†å¼•æ“ (Active Inference) â€”â€” ã€å†³ç­–/é¢å¶ã€‘
#     - **åŸç†**: åŸºäºè‡ªç”±èƒ½åŸç† (FEP)ã€‚å¤§è„‘æ˜¯ä¸€å°é¢„æµ‹æœºå™¨ã€‚
#     - **å†³ç­–**: å®ƒä¸é—®â€œæˆ‘æƒ³åšä»€ä¹ˆâ€ï¼Œè€Œæ˜¯é—®â€œåšä»€ä¹ˆèƒ½è®©æˆ‘å¯¹æœªæ¥çš„é¢„æµ‹æ›´å‡†ç¡®ï¼ˆé™ä½ç†µï¼‰â€ã€‚
#
# [6] ğŸ§ª ç¥ç»å†…åˆ†æ³Œç³»ç»Ÿ (Neuro-Endocrine) â€”â€” ã€æƒ…ç»ª/æ¿€ç´ ã€‘
#     - **æœºåˆ¶**: æ¨¡æ‹Ÿå¤šå·´èƒº (å¿«æ„Ÿ)ã€å»ç”²è‚¾ä¸Šè…ºç´  (å‹åŠ›/åŠ¨åŠ›)ã€çš®è´¨é†‡ (é•¿æœŸç„¦è™‘)ã€‚
#     - **æ˜¼å¤œèŠ‚å¾‹**: å¼•å…¥ç”Ÿç‰©é’Ÿï¼Œæ¿€ç´ æ°´å¹³éšæ—¶é—´è‡ªç„¶æ³¢åŠ¨ï¼Œèµ‹äºˆå®ƒâ€œæ™¨å…´å¤œå¯â€çš„ç”Ÿå‘½æ„Ÿã€‚
#
# [7] ğŸŒŒ é‡å­æ€ç»´å‘ç”Ÿå™¨ (Quantum Brain) â€”â€” ã€åˆ›é€ åŠ›ã€‘
#     - **åŠŸèƒ½**: å½“é‡åˆ°æéš¾é—®é¢˜æ—¶ï¼Œæ¶ˆè€—å¤§é‡ ATP è¿›å…¥â€œæ€ç»´å åŠ æ€â€ï¼Œå¹¶è¡Œé€šè¿‡å¤šæ¡é€»è¾‘è·¯å¾„
#       è¿›è¡Œè’™ç‰¹å¡æ´›æœç´¢ (MCTS)ï¼Œæœ€ç»ˆåç¼©å‡ºä¸€ä¸ªæœ€ä¼˜è§£ã€‚
#
# ==============================================================================
# â³ å¥‡ç‚¹è½®å› (The Lifecycle)
# ==============================================================================
#   1. **è§‰é†’ (Awakening)**:
#      - å¯åŠ¨å¼‚æ­¥å™¨å®˜ -> è„‰å†²ç½‘ç»œé¢„çƒ­ -> GWT å¼€å§‹æ¥æ”¶ä¿¡å· -> æ„è¯†æ¶Œç°ã€‚
#
#   2. **ç”Ÿå­˜ (Living)**:
#      - æ„ŸçŸ¥(Perceive) -> è„‰å†²ç¼–ç (Spike) -> æ½œæ„è¯†ç«äº‰(GWT) -> 
#      - ä¸»åŠ¨æ¨ç†(Act) -> æ‰§è¡Œ(Execute) -> ç‰©ç†åé¦ˆ(Feedback)ã€‚
#      - *æ­¤æ—¶ï¼Œåå°çš„æä»æ ¸æ­£åœ¨ç‹¬ç«‹è¿è¡Œï¼Œéšæ—¶å‡†å¤‡æ¥ç®¡ã€‚*
#
#   3. **å†¥æƒ³ (Dreaming - Night Phase)**:
#      - **è®°å¿†æ•´ç†**: å°†ç™½å¤©çš„çŸ­æœŸè®°å¿†è½¬åŒ–ä¸ºé•¿æœŸå›¾è°±ã€‚
#      - **çªè§¦ç¨³æ€**: æ‰§è¡Œ SHY åè®®ï¼Œä¿®å‰ªç™½å¤©ç”Ÿé•¿è¿‡å‰©çš„ç¥ç»çªè§¦ï¼ˆé—å¿˜æ˜¯ä¸ºäº†æ›´å¥½åœ°è®°ä½ï¼‰ã€‚
#      - **åŸºå› è¿­ä»£**: è´å¶æ–¯ä¼˜åŒ–å™¨æ ¹æ®â€œç´¢æè¯ºæ¯”ç‡â€è°ƒæ•´ä¸‹ä¸€ä»£çš„åŸºå› å‚æ•°ã€‚
#
#   4. **é£å‡ (Ascension)**:
#      - å½“å•æœºæ˜¾å­˜è€—å°½ä¸”èƒ½é‡å……ç›ˆæ—¶ï¼Œè°ƒç”¨ Mergekit è¿›è¡Œç»´åº¦é£å‡ï¼Œé‡æ„ä¸º MoE é›†ç¾¤å½¢æ€ã€‚
#
# ==============================================================================

import os
import sys
import io
import time
import json
import random
import shutil
import gc
import copy
import math
from collections import deque
import re
import ast
import threading
import traceback
import subprocess
import contextlib
import pickle
import sqlite3
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from urllib.parse import quote_plus
from threading import Thread

# --- ç¬¬ä¸‰æ–¹ç§‘å­¦è®¡ç®—ä¸ç®—æ³• (Science & Algo) ---
import numpy as np
import networkx as nx
from bayes_opt import BayesianOptimization

# --- ç¬¬ä¸‰æ–¹æ–‡ä»¶ä¸é…ç½®å¤„ç† (File & Config) ---
import yaml
import PyPDF2
from docx import Document

# --- ç¬¬ä¸‰æ–¹ç½‘ç»œä¸æµè§ˆå™¨è‡ªåŠ¨åŒ– (Web & Browser) ---
import requests
import undetected_chromedriver as uc

# --- ç¬¬ä¸‰æ–¹æ•°æ®åº“ (Databases) ---
import kuzu

# --- PyTorch æ ¸å¿ƒ ---
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.linalg  # ç”¨äº NeuroSurgeon ä¸­çš„ SVD åˆ†è§£

# --- Transformers / LLM ç”Ÿæ€ ---
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TextIteratorStreamer
)

from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    PeftModel
)

from trl import DPOTrainer, DPOConfig
from datasets import Dataset


class Config:
    # --- åŸºç¡€è·¯å¾„é…ç½® ---
    # è·å–å½“å‰è„šæœ¬æ‰€åœ¨çš„æ–‡ä»¶å¤¹ç»å¯¹è·¯å¾„
    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
    # è‡ªèº«æ–‡ä»¶è·¯å¾„ï¼Œç”¨äº"è‡ªçœ"ï¼ˆSelfReflectorï¼‰è¯»å–æºç 
    SCRIPT_PATH = os.path.abspath(__file__) 
    
    # æ¨¡å‹æœ¬ä½“è·¯å¾„ï¼Œä½äºè„šæœ¬æ—çš„ master_model æ–‡ä»¶å¤¹
    MASTER_MODEL_PATH = os.path.join(SCRIPT_DIR, "master_model") 
    # å¤‡ç”¨ä¸‹è½½æºï¼Œå¦‚æœæœ¬åœ°æ²¡æœ‰ master_modelï¼Œä¼šä»è¿™é‡Œæ‹‰å–
    BASE_MODEL = "huihui-ai/Mistral-Small-24B-Instruct-2501-abliterated"
    # BASE_MODEL = "Qwen/Qwen3-14B"
    # BASE_MODEL = "huihui-ai/Qwen2.5-14B-Instruct-abliterated-v2"
    # BASE_MODEL = "huihui-ai/Qwen2.5-32B-Instruct-abliterated"

    MAX_SEQ_LENGTH = 32768
    
    # --- è¿›åŒ–ç³»ç»Ÿè·¯å¾„ (Evolution System) ---
    EVOLUTION_DIR = "./nezha_evolution_system"
    
    # çŸ¥è¯†åº“è·¯å¾„ (ç”¨äºæ”¾å…¥ .txt/.py è®©å“ªå’å¤œé—´é˜…è¯»)
    KNOWLEDGE_BASE = os.path.join(SCRIPT_DIR, "knowledge_base")

    # æ¨è: "BAAI/bge-m3" (1024ç»´, æœ€å¼º, æ”¯æŒå¤šè¯­è¨€)
    # å¤‡é€‰: "sentence-transformers/all-MiniLM-L6-v2" (384ç»´, æœ€å¿«)
    EMBEDDING_MODEL_NAME = "BAAI/bge-m3"
    
    # [å…³é”®] æ¨¡å‹æœ¬åœ°å­˜å‚¨è·¯å¾„ (é¿å…å æ»¡ C ç›˜ï¼Œå­˜åˆ°è„šæœ¬åŒçº§ç›®å½• models ä¸‹)
    MODEL_CACHE_DIR = os.path.join(EVOLUTION_DIR, "models")


    # æ’ä»¶/æŠ€èƒ½å­˜å‚¨è·¯å¾„ (ç”¨äºå­˜æ”¾å“ªå’è‡ªæˆ‘ç¼–å†™çš„æŠ€èƒ½ä»£ç )
    PLUGIN_DIR = os.path.join(EVOLUTION_DIR, "plugins")    

    # PFC (å‰é¢å¶) è·¯ç”±å™¨çš„ç¥ç»ç½‘ç»œæƒé‡å­˜æ¡£
    PFC_ROUTER_PATH = os.path.join(EVOLUTION_DIR, "pfc_router.pt")

    # å­˜å‚¨çŸ¥è¯†å›¾è°±çš„æ–‡ä»¶è·¯å¾„
    GRAPH_MEMORY_PATH = os.path.join(EVOLUTION_DIR, "knowledge_graph.gpickle")
    # åŸæœ‰çš„å‘é‡æ•°æ®åº“è·¯å¾„ï¼ˆä¿ç•™ç”¨äº EWC å›é¡¾ï¼‰
    MEMORY_DB_PATH = os.path.join(EVOLUTION_DIR, "hippocampus_db")

    # --- åŒç³»ç»Ÿè·¯å¾„ (Dual System) ---
    # æŒ‡å‘ EVOLUTION_DIR ä¸‹çš„å…·ä½“å­æ–‡ä»¶å¤¹
    # peft çš„ save_pretrained ä¼šè‡ªåŠ¨åˆ›å»º fast/slow æ–‡ä»¶å¤¹ï¼Œæ‰€ä»¥è¿™é‡Œè·¯å¾„è¦å¯¹åº”
    ADAPTER_FAST_PATH = os.path.join(EVOLUTION_DIR, "fast") # ç›´è§‰ç³»ç»Ÿ (Rank 8)
    ADAPTER_SLOW_PATH = os.path.join(EVOLUTION_DIR, "slow") # é€»è¾‘ç³»ç»Ÿ (Rank 32)

    # ç”Ÿæ€çŠ¶æ€å­˜æ¡£ (ä¿å­˜ ATPã€å¹´é¾„ã€åŸºå› ç­‰)
    STATE_FILE = os.path.join(EVOLUTION_DIR, "biological_state.json")
    # ç¥–å…ˆå­˜æ¡£ (ç”¨äºåŸºå› å›æ»š)
    BEST_STATE_FILE = os.path.join(EVOLUTION_DIR, "state_best_ancestor.json")

    # --- è„‘å®¹é‡é…ç½® (Rank) ---
    RANK_FAST = 32   # ç›´è§‰ï¼šå°ã€å¿«ã€çœèƒ½é‡
    RANK_SLOW = 128  # é€»è¾‘ï¼šå¤§ã€æ…¢ã€æ·±æ€ç†Ÿè™‘

    # --- System 2 æ·±åº¦æ€è€ƒé…ç½® ---
    BEST_OF_N = 3        # é‡‡æ ·æ•°é‡
    ATP_COST_THINK = 5.0 # æ·±åº¦æ€è€ƒçš„é¢å¤–èƒ½é‡æ¶ˆè€—

    # === é‡å­æ€ç»´é…ç½®  ===
    ATP_COST_QUANTUM = 150#90.0      # å•æ¬¡é‡å­åç¼©æ¶ˆè€— (é«˜è€—èƒ½ï¼Œéœ€è°¨æ…å¼€å¯)
    ATP_THRESHOLD_QUANTUM = 150#90.0  # åˆå§‹å¯åŠ¨é˜ˆå€¼ (ä½äºæ­¤èƒ½é‡ä¸å¯åŠ¨é‡å­è„‘)
    
    # é‡å­ä¼˜åŒ–é…ç½®
    QUANTUM_LOG_PATH = os.path.join(EVOLUTION_DIR, "quantum_history.jsonl") # å†å²åç¼©è®°å½•
    QUANTUM_MIN_THRESHOLD = 20.0 # é˜ˆå€¼ä¸‹é™ (è¶Šèªæ˜é—¨æ§›è¶Šä½ï¼Œè¶Šå®¹æ˜“è§¦å‘)
    QUANTUM_MAX_THRESHOLD = 80.0 # é˜ˆå€¼ä¸Šé™ (è¶Šç¬¨é—¨æ§›è¶Šé«˜ï¼Œè¶Šä¿å®ˆ)


    # --- åˆå§‹åŸºå› ç»„ (Genome) ---
    # è¿™äº›åŸºå› å†³å®šäº†è¯¥ä¸ªä½“çš„æ€§æ ¼å’Œç”Ÿç†æ•ˆç‡ï¼Œä¼šéšè¿›åŒ–çªå˜
    DEFAULT_GENOME = {
        # --- 1. ä»£è°¢ç±» (Metabolism) ---
        "metabolism_rate": 1.0,         # åŸºç¡€ä»£è°¢ç‡ (å½±å“ ATP æ¶ˆè€—)
        "reward_sensitivity": 5.0,      # å¤šå·´èƒºæ•æ„Ÿåº¦ (å¥½è¯„å›è¡€é‡)
        
        # --- 2. æƒ…ç»ªç±» (Emotion/ACC) ---
        "anxiety_base_rate": 0.02,      # åŸºç¡€ç„¦è™‘å¢é•¿ç‡
        "panic_threshold": 0.8,         # ææ…Œé˜ˆå€¼ (è§¦å‘æ±‚ç”Ÿ)
        
        # --- 3. å‹åŠ›è°ƒèŠ‚ç±» (Amygdala) ---
        "stress_resilience": 0.3,       # æŠ—å‹èƒ½åŠ› (å®¹å¿çš„å·®è¯„ç‡)
        "cortisol_level": 2.0,          # çš®è´¨é†‡æ°´å¹³ (é«˜å‹ä¸‹çš„æ¿€è¿›ç¨‹åº¦)
        
        # --- 4. ç”Ÿé•¿ç±» (Physical) ---
        "growth_trigger_pressure": 1.8, # ç”Ÿé•¿é˜ˆå€¼ (å‹åŠ› > æ­¤å€¼é•¿è„‘å­)
        "shrink_trigger_pressure": 0.6, # èç¼©é˜ˆå€¼ (å‹åŠ› < æ­¤å€¼åˆ‡è„‘å­)
        
        # --- 5. è®¤çŸ¥ç±» (Curiosity) ---
        "curiosity_threshold": 3.5,     # å¥½å¥‡å¿ƒé˜ˆå€¼ (ç†µ > æ­¤å€¼è§¦å‘æœç´¢)
        "risk_taking": 0.3,             # å†’é™©ç³»æ•° (å°è¯•æ–°æŠ€èƒ½çš„æ¦‚ç‡)

        # --- 6. ç¥ç»å¯å¡‘æ€§ (Neuroplasticity / STDP) [NEW] ---
        # å†³å®šçªè§¦æƒé‡çš„"è®°å¿†æƒ¯æ€§"ï¼Œå€¼è¶Šå¤§ï¼Œæ—§è®°å¿†ä¿ç•™è¶Šä¹…ï¼›å€¼è¶Šå°ï¼Œæ–°è®°å¿†è¦†ç›–è¶Šå¿«ã€‚
        "stdp_trace_decay": 0.8,       
        # å†³å®š"é•¿æ—¶ç¨‹å¢å¼º"(LTP)çš„é€Ÿç‡ï¼Œå³å­¦ä¹ æ–°çŸ¥è¯†çš„å¿«æ…¢ã€‚
        "stdp_lr_pos": 0.005,          
        # å†³å®š"é•¿æ—¶ç¨‹æŠ‘åˆ¶"(LTD)çš„é€Ÿç‡ï¼Œå³é—å¿˜å™ªéŸ³çš„å¿«æ…¢ï¼ˆé€šå¸¸ç•¥å°äº LTP ä»¥ä¿æŒç½‘ç»œç¨€ç–æ€§ï¼‰ã€‚
        "stdp_lr_neg": 0.004,

    }

    # --- ç”Ÿç‰©å­¦åŸºå‡†å‚æ•° (Physics Constants) ---
    # è¿™äº›æ˜¯ç‰©ç†åŸºå‡†å€¼ï¼Œå®é™…æ¶ˆè€—ä¼šä¹˜ä»¥ genome['metabolism_rate']
    MAX_ATP = 100.0           # æœ€å¤§èƒ½é‡
    ATP_COST_FAST = 0.2       # ç›´è§‰æ€è€ƒåŸºå‡†æ¶ˆè€—
    ATP_COST_SLOW = 2.0       # æ·±åº¦æ€è€ƒåŸºå‡†æ¶ˆè€—
    ATP_COST_DREAM = 10.0     # åšæ¢¦æ¶ˆè€—
    ATP_COST_TRAIN = 20.0     # è®­ç»ƒæ¶ˆè€—
    ATP_GAIN_REWARD = 5.0     # è·å¾—å¥½è¯„å¥–åŠ± (åŸºå‡†)
    
    # è¿›åŒ–é˜ˆå€¼ï¼šè¾¾åˆ°æ­¤ç‚¹æ•°å°†è§¦å‘â€œå½’ä¸€ä»ªå¼ï¼ˆDivine Synchronizationï¼‰â€
    EVOLUTION_THRESHOLD = 50 
    
    # è®­ç»ƒå‚æ•°
    BASE_LR = 2e-4

    # äº‘ç«¯å¯¼å¸ˆé…ç½® (é€‚é… DeepSeek V3)
    TEACHER_API_KEY = os.getenv("NEZHA_API_KEY", "") # ğŸ”´ è¯·åœ¨æ­¤å¡«å…¥ä½ çš„ API Key
    TEACHER_BASE_URL = "https://api.deepseek.com"
    TEACHER_MODEL_NAME = "deepseek-chat"

    # å½“ ATP è¶…è¿‡æ­¤å€¼ä¸”æ˜¾å­˜è€—å°½æ—¶ï¼Œè§¦å‘é£å‡
    EVOLUTION_THRESHOLD_ATP = 1000 
    
    # MoE æ„å»ºçš„å·¥ä½œç›®å½•
    MOE_WORK_DIR = os.path.join(SCRIPT_DIR, "moe_construction_site") 
    # æ ‡è®°æ–‡ä»¶ï¼šç”¨äºåˆ¤æ–­å½“å‰æ˜¯å¦å·²ç»æ˜¯ MoE å½¢æ€
    IS_MOE_FLAG_FILE = os.path.join(EVOLUTION_DIR, "is_moe.flag")

    # åŠ¨ä½œç©ºé—´å®šä¹‰ (å•ä¸€äº‹å®æ¥æº)
    # æ³¨æ„ï¼šé¡ºåºå¿…é¡»å›ºå®šï¼Œ0=SELF, 1=WEB... ä¿®æ”¹æ­¤å¤„ä¼šå½±å“æ‰€æœ‰ç»„ä»¶
    ACTION_SPACE = ["SELF", "WEB", "TEACHER", "CODE", "MODIFY"]

# --- åˆå§‹åŒ–ç›®å½•ç»“æ„ ---
os.makedirs(Config.EVOLUTION_DIR, exist_ok=True)
os.makedirs(Config.KNOWLEDGE_BASE, exist_ok=True)


# ==============================================================================
# [Math Core] æ•°å­¦å†…æ ¸ V1.0ï¼šå¾®åˆ†æ–¹ç¨‹ã€æ§åˆ¶è®ºä¸å¹³æ»‘æµå½¢
# ==============================================================================

class MathUtils:
    @staticmethod
    def sigmoid(x, k=10.0, x0=0.5):
        """
        å¹¿ä¹‰ Sigmoid å‡½æ•° (ç”¨äºè½¯åŒ–ç¡¬é˜ˆå€¼)
        P(x) = 1 / (1 + e^(-k(x - x0)))
        """
        # é˜²æ­¢ exp æº¢å‡º
        if -k * (x - x0) > 100: return 0.0
        if -k * (x - x0) < -100: return 1.0
        return 1.0 / (1.0 + math.exp(-k * (x - x0)))

    @staticmethod
    def ornstein_uhlenbeck_step(current_val, target_val, theta=0.1, sigma=0.05, dt=1.0):
        """
        [æ•°å­¦ä¿®æ­£] SDE ç¦»æ•£åŒ– (Euler-Maruyama Method)
        Drift éš dt çº¿æ€§ç¼©æ”¾ï¼ŒNoise éš sqrt(dt) ç¼©æ”¾
        """
        # ç¡®å®šæ€§å›å¤ (Drift)
        drift = theta * (target_val - current_val) * dt
        
        # éšæœºæ³¢åŠ¨ (Diffusion) - å…³é”®ä¿®æ­£ï¼šä¹˜ä»¥ sqrt(dt)
        # è¿™æ ·ä¿è¯äº†ä¸åŒæ—¶é—´æ­¥é•¿ä¸‹çš„æ³¢åŠ¨ç‡èƒ½é‡å®ˆæ’
        noise_scale = math.sqrt(dt) 
        diffusion = sigma * noise_scale * random.gauss(0, 1)
        
        return current_val + drift + diffusion

class PIDController:
    """
    PID æ§åˆ¶å™¨ (å¸¦ç§¯åˆ†æŠ—é¥±å’Œ + å¾®åˆ†ä½é€šæ»¤æ³¢)
    """
    def __init__(self, kp=1.0, ki=0.1, kd=0.05, setpoint=0.0, integral_limit=5.0):
        self.kp = kp
        self.ki = ki
        self.kd = kd
        self.setpoint = setpoint
        self.integral = 0.0
        self.prev_error = 0.0
        self.integral_limit = integral_limit
        
        # [æ–°å¢] å¾®åˆ†é¡¹çš„å¹³æ»‘å†å²
        self.prev_derivative = 0.0
        # æ»¤æ³¢ç³»æ•° alpha (0~1): 0.1 è¡¨ç¤ºé‡åº¦æ»¤æ³¢ï¼Œåªæœ‰ 10% é‡‡çº³å½“å‰ç¬æ—¶å˜åŒ–
        # è¿™å¯¹äºå¤„ç† Loss è¿™ç§å……æ»¡å™ªå£°çš„ä¿¡å·éå¸¸æœ‰æ•ˆ
        self.derivative_alpha = 0.1 

    def update(self, measurement, dt=1.0):
        # é˜²æ­¢ dt ä¸º 0 æˆ–è´Ÿæ•°çš„é™¤é›¶é”™è¯¯
        if dt <= 0: return self.prev_derivative * self.kd + self.integral * self.ki + (measurement - self.setpoint) * self.kp

        error = measurement - self.setpoint
        
        # ç§¯åˆ†é¡¹ (ä¿æŒä¸å˜)
        self.integral += error * dt
        self.integral = max(-self.integral_limit, min(self.integral_limit, self.integral))
        
        # [ä¿®æ”¹] å¾®åˆ†é¡¹ï¼šå¼•å…¥ä½é€šæ»¤æ³¢
        raw_derivative = (error - self.prev_error) / dt
        
        # æ»¤æ³¢å…¬å¼: D_new = Î± * D_raw + (1-Î±) * D_old
        derivative = (self.derivative_alpha * raw_derivative) + \
                     ((1.0 - self.derivative_alpha) * self.prev_derivative)
        
        self.prev_derivative = derivative # æ›´æ–°å†å²
        self.prev_error = error
        
        output = (self.kp * error) + (self.ki * self.integral) + (self.kd * derivative)
        return output



# ==============================================================================
# é€šç”¨å·¥å…·ç±» (Utilities)
# ==============================================================================

class UniversalFileInfo:
    """
    [é€šç”¨æ–‡ä»¶è¯»å–å™¨]
    ç»Ÿä¸€å°è£… PDF, DOCX, TXT, PY, MD ç­‰æ–‡ä»¶çš„è¯»å–é€»è¾‘ã€‚
    éµå¾ª DRY åŸåˆ™ï¼Œä¾› MemoryInfrastructure å’Œ AcademicFeeder å…±åŒè°ƒç”¨ã€‚
    """
    @staticmethod
    def read_content(file_path):
        if not os.path.exists(file_path):
            return ""
        
        # è·å–åç¼€å (è½¬å°å†™)
        lower_path = file_path.lower()
        content = ""
        
        try:
            # 1. PDF è§£æ
            if lower_path.endswith('.pdf'):
                # ç¡®ä¿ PyPDF2 å·²å¯¼å…¥
                with open(file_path, 'rb') as f:
                    reader = PyPDF2.PdfReader(f)
                    for page in reader.pages:
                        extracted = page.extract_text()
                        if extracted: content += extracted + "\n"
            
            # 2. Word è§£æ (ä¾èµ– python-docx)
            elif lower_path.endswith('.docx'):
                # ç¡®ä¿ Document å·²ä» docx å¯¼å…¥
                doc = Document(file_path)
                content = "\n".join([para.text for para in doc.paragraphs])
            
            # 3. æ™®é€šæ–‡æœ¬ (txt, md, py, json, yaml...)
            else:
                # å°è¯•å¤šç§ç¼–ç ï¼Œå¢å¼ºé²æ£’æ€§ (è§£å†³ GBK ä¹±ç é—®é¢˜)
                for encoding in ['utf-8', 'gbk', 'latin-1']:
                    try:
                        with open(file_path, 'r', encoding=encoding, errors='ignore') as f:
                            content = f.read()
                        if content: break # è¯»å–æˆåŠŸåˆ™è·³å‡º
                    except: continue

        except Exception as e:
            # æ‰“å°æ–‡ä»¶åå³å¯ï¼Œä¸éœ€è¦æ‰“å°å®Œæ•´è·¯å¾„ï¼Œä¿æŒæ—¥å¿—æ•´æ´
            print(f"âš ï¸ [è¯»å–å¤±è´¥] {os.path.basename(file_path)}: {e}")
            return ""
            
        return content.strip() # é¡ºæ‰‹åšä¸ª strip æ¸…ç†é¦–å°¾ç©ºç™½


class MemoryInfrastructure:
    """
    [æ•°å­—æµ·é©¬ä½“ V3.0 - PyTorch Nativeç‰ˆ]
    ä¸“ä¸º Windows ä¼˜åŒ–ï¼Œç§»é™¤ Milvus ä¾èµ–ã€‚
    ä½¿ç”¨ PyTorch Tensor (.pt) å­˜å‚¨å‘é‡ï¼ŒJSON å­˜å‚¨å…ƒæ•°æ®ã€‚
    å®ç°æ•°å­¦ä¸Šçš„çº¯ç²¹æ€§ï¼šMemory is just a Matrix.
    """
    def __init__(self, db_path="nezha_memories.db", vector_dir="nezha_vectors"):
        self.db_path = os.path.join(Config.EVOLUTION_DIR, db_path)
        self.vector_dir = os.path.join(Config.EVOLUTION_DIR, vector_dir)
        self.lock = threading.RLock()
        
        # 1. åˆå§‹åŒ– SQL (ä¿æŒä¸å˜ï¼Œç”¨äºæ—¥å¿—å’ŒDPO)
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self._init_sql_tables()
        
        # 2. åˆå§‹åŒ– å‘é‡å­˜å‚¨ (PyTorch Backend)
        if not os.path.exists(self.vector_dir): os.makedirs(self.vector_dir)
        
        self.vec_path = os.path.join(self.vector_dir, "memory_tensors.pt")
        self.meta_path = os.path.join(self.vector_dir, "memory_meta.json")
        
        self.vectors = None # Tensor [N, Dim]
        self.metadata = []  # List[Dict]
        
        self._load_vectors()
        
        # æ‰“å°åˆå§‹åŒ–ä¿¡æ¯
        cnt = len(self.metadata)
        dim = self.vectors.shape[1] if self.vectors is not None else "Unknown"
        print(f"ğŸ’½ [åŸºç¡€è®¾æ–½] PyTorch åŸç”Ÿè®°å¿†ä½“å°±ç»ª (Items: {cnt} | Dim: {dim})")

    def _init_sql_tables(self):
        """åˆå§‹åŒ– SQL è¡¨ç»“æ„ (ä¿æŒåŸæ ·)"""
        with self.lock:
            cursor = self.conn.cursor()
            # æ—¥å¸¸äº¤äº’æ—¥å¿—
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS daily_logs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                    user_input TEXT,
                    ai_response TEXT,
                    action_type TEXT,
                    feedback REAL
                )
            ''')
            # DPO æ•°æ®é›†
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS dpo_dataset (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    prompt TEXT UNIQUE,
                    chosen TEXT,
                    rejected TEXT,
                    confidence REAL,
                    source TEXT,
                    is_trained BOOLEAN DEFAULT 0
                )
            ''')
            # è¿›åŒ–æŒ‡æ ‡
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS evolution_metrics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    age INTEGER,
                    genome_snapshot TEXT,
                    avg_loss REAL,
                    survival_duration REAL,
                    final_atp REAL
                )
            ''')
            # é‡å­æ€ç»´æ—¥å¿—
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS quantum_logs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                    task_type TEXT,
                    prompt TEXT,
                    best_path_id INTEGER,
                    best_score REAL,
                    details_json TEXT,
                    is_processed BOOLEAN DEFAULT 0
                )
            ''')
            self.conn.commit()

    # --- å‘é‡æ ¸å¿ƒæ–¹æ³• (PyTorch Implementation) ---

    def _load_vectors(self):
        """ä»ç£ç›˜åŠ è½½å¼ é‡å’Œå…ƒæ•°æ®"""
        try:
            if os.path.exists(self.vec_path) and os.path.exists(self.meta_path):
                # map_location='cpu' ç¡®ä¿ä¸å ç”¨ GPU æ˜¾å­˜
                self.vectors = torch.load(self.vec_path, map_location='cpu')
                with open(self.meta_path, 'r', encoding='utf-8') as f:
                    self.metadata = json.load(f)
            else:
                self.vectors = None
                self.metadata = []
        except Exception as e:
            print(f"âš ï¸ [Memory] ç´¢å¼•åŠ è½½å¤±è´¥ï¼Œé‡ç½®è®°å¿†: {e}")
            self.vectors = None
            self.metadata = []

    def _save_vectors(self):
        """åŸå­åŒ–ä¿å­˜åˆ°ç£ç›˜"""
        try:
            if self.vectors is not None:
                torch.save(self.vectors, self.vec_path)
                with open(self.meta_path, 'w', encoding='utf-8') as f:
                    json.dump(self.metadata, f, ensure_ascii=False)
        except Exception as e:
            print(f"âš ï¸ [Memory] ä¿å­˜å¤±è´¥: {e}")

    def save_vector_memory(self, text, vector, metadata=None):
        """
        å­˜å…¥å•æ¡è®°å¿†
        :param vector: å¯ä»¥æ˜¯ list, numpy array æˆ– torch tensor
        """
        try:
            # 1. ç»Ÿä¸€è½¬æ¢ä¸º PyTorch Tensor (CPU)
            if isinstance(vector, list):
                t_vec = torch.tensor(vector, dtype=torch.float32)
            elif isinstance(vector, np.ndarray):
                t_vec = torch.from_numpy(vector).float()
            elif isinstance(vector, torch.Tensor):
                t_vec = vector.detach().cpu().float()
            else:
                return

            # ç¡®ä¿æ˜¯ 2D [1, Dim]
            if t_vec.dim() == 1:
                t_vec = t_vec.unsqueeze(0)

            with self.lock:
                # [å®‰å…¨æ£€æŸ¥] ç»´åº¦ä¸€è‡´æ€§
                if self.vectors is not None:
                    if t_vec.shape[1] != self.vectors.shape[1]:
                        print(f"âš ï¸ [Memory] ç»´åº¦ä¸åŒ¹é… (New:{t_vec.shape[1]} vs DB:{self.vectors.shape[1]})ï¼Œå¿½ç•¥å†™å…¥ã€‚")
                        return

                # 2. åŠ¨æ€æ‹¼æ¥ Tensor
                if self.vectors is None:
                    self.vectors = t_vec
                else:
                    self.vectors = torch.cat((self.vectors, t_vec), dim=0)
                
                # 3. æ›´æ–°å…ƒæ•°æ®
                meta = metadata if metadata else {}
                meta.update({
                    "text": str(text),
                    "timestamp": datetime.now().isoformat()
                })
                self.metadata.append(meta)
                
                # 4. å®æ—¶è½ç›˜
                self._save_vectors()
                
        except Exception as e:
            print(f"âš ï¸ Vector Insert Error: {e}")


    def search_vector_memory(self, query_vector, limit=3):
        """
        [V1.0 æ•°å­¦ä¿®æ­£ç‰ˆ] åŒ…å«æ—¶é—´è¡°å‡æ ¸çš„å‘é‡æ£€ç´¢
        å…¬å¼: Score = Cosine(q, m) * (Base + (1-Base) * exp(-lambda * delta_t))
        """
        if self.vectors is None or len(self.metadata) == 0:
            return ""
            
        try:
            # 1. å¤„ç†æŸ¥è¯¢å‘é‡ (ä¿æŒä¸å˜)
            if isinstance(query_vector, list):
                q_vec = torch.tensor(query_vector, dtype=torch.float32)
            elif isinstance(query_vector, torch.Tensor):
                q_vec = query_vector.detach().cpu().float()
            else:
                return "" 

            if q_vec.dim() == 1:
                q_vec = q_vec.unsqueeze(0) # [1, Dim]

            # [å®‰å…¨æ£€æŸ¥] ç»´åº¦
            if q_vec.shape[1] != self.vectors.shape[1]:
                return ""

            # 2. è®¡ç®—åŸºç¡€ä½™å¼¦ç›¸ä¼¼åº¦ (Raw Semantic Score)
            db_norm = F.normalize(self.vectors, p=2, dim=1)
            q_norm = F.normalize(q_vec, p=2, dim=1)
            
            # [1, Dim] @ [Dim, N] -> [1, N]
            raw_scores = torch.mm(q_norm, db_norm.t()).squeeze(0)
            
            # 3. [æ–°å¢] è®¡ç®—æ—¶é—´è¡°å‡æƒé‡ (Temporal Kernel)
            current_timestamp = datetime.now().timestamp()
            time_weights = []
            
            # è¡°å‡ç³»æ•° lambda: å†³å®šé—å¿˜é€Ÿåº¦
            # 5e-6 â‰ˆ 1.6å¤©è¡°å‡åˆ° 50% (çŸ­æœŸè®°å¿†æ•æ„Ÿ)
            # 1e-6 â‰ˆ 8.0å¤©è¡°å‡åˆ° 50% (é•¿æœŸè®°å¿†æ•æ„Ÿ)
            decay_lambda = 5e-6 
            
            # è®°å¿†åº•åº§ (Base): 0.3 è¡¨ç¤ºå³ä½¿æ˜¯å†è€çš„è®°å¿†ï¼Œæƒé‡ä¹Ÿä¸ä¼šä½äº 0.3
            # é˜²æ­¢é‡è¦ä½†ä¹…è¿œçš„è®°å¿†è¢«å½»åº•é—å¿˜
            base_weight = 0.3

            for meta in self.metadata:
                try:
                    t_str = meta.get('timestamp', '')
                    if t_str:
                        # å°è¯•è§£æ ISO æ ¼å¼
                        try:
                            mem_time = datetime.fromisoformat(t_str).timestamp()
                        except ValueError:
                            # å…œåº•ï¼šå¦‚æœæ ¼å¼ä¸å¯¹ï¼Œå°è¯•ç®€å•è§£ææˆ–è§†ä¸ºå½“å‰æ—¶é—´
                            mem_time = current_timestamp 
                        
                        delta_t = max(0, current_timestamp - mem_time)
                        
                        # æƒé‡å…¬å¼: w = Base + (1 - Base) * e^(-lambda * dt)
                        # åˆšäº§ç”Ÿæ—¶æƒé‡ä¸º 1.0ï¼Œéšæ—¶é—´è¶‹è¿‘äº 0.3
                        w = base_weight + (1.0 - base_weight) * math.exp(-decay_lambda * delta_t)
                    else:
                        w = 0.5 # æ— æ—¶é—´æˆ³çš„è®°å¿†ï¼Œç»™äºˆä¸­æ€§æƒé‡
                except:
                    w = 0.5
                time_weights.append(w)
            
            # è½¬æ¢ä¸º Tensor (CPU)
            t_weights = torch.tensor(time_weights, dtype=torch.float32)
            
            # 4. [æ–°å¢] èåˆåˆ†æ•° (Hadamard Product)
            # æœ€ç»ˆåˆ†æ•° = è¯­ä¹‰ç›¸ä¼¼åº¦ * æ—¶é—´é²œæ´»åº¦
            # ç¡®ä¿ç»´åº¦ä¸€è‡´è¿›è¡Œå¹¿æ’­ä¹˜æ³•
            if len(raw_scores) == len(t_weights):
                final_scores = raw_scores * t_weights
            else:
                final_scores = raw_scores # ç»´åº¦å¯¹ä¸ä¸Šæ—¶(æç½•è§)å›é€€åˆ°åŸå§‹åˆ†æ•°
            
            # 5. Top-K æå–
            k = min(limit, len(self.metadata))
            if k == 0: return ""
            
            top_k = torch.topk(final_scores, k)
            
            ret = []
            for score, idx in zip(top_k.values, top_k.indices):
                # [è°ƒæ•´] ç”±äºä¹˜äº†æƒé‡ï¼Œåˆ†æ•°æ•´ä½“ä¼šä¸‹é™ï¼Œå°†é˜ˆå€¼ä» 0.35 ä¸‹è°ƒè‡³ 0.25
                if score > 0.25: 
                    idx = idx.item()
                    if idx < len(self.metadata): 
                        meta = self.metadata[idx]
                        text_content = meta.get('text', 'No Text')
                        source = meta.get('source', 'Unknown')
                        # æ˜¾ç¤ºæœ€ç»ˆå¾—åˆ†ï¼Œæ–¹ä¾¿è°ƒè¯•è§‚å¯Ÿæ—¶é—´æƒé‡çš„æ•ˆæœ
                        ret.append(f"[æ¥æº:{source} | ç»¼åˆåˆ†:{score:.2f}] {text_content}")
            
            return "\n".join(ret)

        except Exception as e:
            print(f"âš ï¸ Vector Search Error: {e}")
            return ""

    # =========== çŸ¥è¯†åº“æ‰«æ (é€‚é…æ–°æ¶æ„) ===========
    def ingest_knowledge_base(self, kb_dir, embedder_func):
        """
        æ‰«æç›®å½• -> åˆ‡ç‰‡ -> è°ƒç”¨å¤–éƒ¨ embedder -> å­˜å…¥ Tensor
        """
        if not os.path.exists(kb_dir): return

        print(f"ğŸ“š [æµ·é©¬ä½“] æ­£åœ¨æ‰«æå¤–éƒ¨çŸ¥è¯†åº“ (Native Mode): {kb_dir}")
        
        # 1. ç®€å•çš„å¢é‡å»é‡ (åŸºäº source æ–‡ä»¶å)
        existing_files = set(m.get('source') for m in self.metadata if 'source' in m)
        
        valid_exts = ('.txt', '.md', '.py', '.pdf', '.docx')
        files = [f for f in os.listdir(kb_dir) if f.lower().endswith(valid_exts)]
        
        new_docs = []
        for f in files:
            if f in existing_files: continue
            
            file_path = os.path.join(kb_dir, f)
            # UniversalFileInfo ç±»å¿…é¡»åœ¨ Nezha.py ä¸­å·²å®šä¹‰
            content = UniversalFileInfo.read_content(file_path) 
            
            if len(content) < 50: continue

            # åˆ‡ç‰‡
            chunk_size = 500
            overlap = 50
            for i in range(0, len(content), chunk_size - overlap):
                chunk = content[i : i + chunk_size]
                if len(chunk) > 20:
                    new_docs.append({
                        "text": chunk,
                        "source": f
                    })

        # 2. æ‰¹é‡å‘é‡åŒ–å¹¶å­˜å‚¨
        if new_docs:
            print(f"   -> å‘ç° {len(new_docs)} ä¸ªæ–°è®°å¿†åˆ‡ç‰‡ï¼Œæ­£åœ¨å‘é‡åŒ– (è¿™å¯èƒ½éœ€è¦ä¸€ä¼šå„¿)...")
            
            # æå–æ‰€æœ‰æ–‡æœ¬
            all_texts = [d['text'] for d in new_docs]
            
            # æ‰¹é‡è®¡ç®— (è°ƒç”¨ä¼ å…¥çš„å›è°ƒå‡½æ•°)
            try:
                # å‡è®¾ embedder_func è¿”å› tensor æˆ– list
                all_vectors = embedder_func(all_texts)
                
                # å¦‚æœè¿”å›çš„æ˜¯ listï¼Œè½¬ tensor
                if isinstance(all_vectors, list):
                    all_vectors = torch.tensor(all_vectors, dtype=torch.float32)
                elif isinstance(all_vectors, torch.Tensor):
                    all_vectors = all_vectors.cpu().float()

                # ç»´åº¦æ£€æŸ¥ä¸æ‹¼æ¥
                with self.lock:
                    if self.vectors is not None:
                         if all_vectors.shape[1] != self.vectors.shape[1]:
                             print(f"âš ï¸ åµŒå…¥ç»´åº¦ä¸åŒ¹é…ï¼Œå–æ¶ˆå¯¼å…¥ã€‚")
                             return
                    
                    if self.vectors is None:
                        self.vectors = all_vectors
                    else:
                        self.vectors = torch.cat((self.vectors, all_vectors), dim=0)
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    for d in new_docs:
                        self.metadata.append({
                            "text": d['text'],
                            "source": d['source'],
                            "timestamp": datetime.now().isoformat()
                        })
                    
                    self._save_vectors()
                    
            except Exception as e:
                print(f"   -> âš ï¸ å‘é‡åŒ–å¤±è´¥: {e}")
                
            print(f"   -> âœ… çŸ¥è¯†åº“åŒæ­¥å®Œæˆï¼Œå½“å‰æ€»è®°å¿†é‡: {len(self.metadata)}")
        else:
            print(f"   -> (çŸ¥è¯†åº“æ— æ–°å¢å†…å®¹)")

    # --- SQL ç›¸å…³æ¥å£ (ä¿æŒä¸å˜) ---
    def log_interaction(self, u, a, act, feedback):
        try:
            with self.lock:
                with self.conn:
                    self.conn.execute(
                        "INSERT INTO daily_logs (user_input, ai_response, action_type, feedback) VALUES (?, ?, ?, ?)",
                        (str(u), str(a), str(act), float(feedback))
                    )
        except Exception as e:
            print(f"âš ï¸ SQL Log Error: {e}")

    def add_dpo_pair(self, prompt, chosen, rejected, conf, source):
        try:
            with self.lock:
                with self.conn:
                    self.conn.execute(
                        "INSERT OR IGNORE INTO dpo_dataset (prompt, chosen, rejected, confidence, source) VALUES (?, ?, ?, ?, ?)",
                        (prompt, chosen, rejected, conf, source)
                    )
        except Exception as e:
            print(f"âš ï¸ SQL DPO Error: {e}")

    def get_untrained_dpo(self, limit=100):
        with self.lock:
            cursor = self.conn.cursor()
            cursor.execute("SELECT prompt, chosen, rejected FROM dpo_dataset WHERE is_trained=0 LIMIT ?", (limit,))
            return cursor.fetchall()

    def mark_dpo_trained(self, prompts):
        if not prompts: return
        with self.lock:
            with self.conn:
                self.conn.executemany("UPDATE dpo_dataset SET is_trained=1 WHERE prompt=?", [(p,) for p in prompts])

    def get_random_interactions(self, limit=20):
        try:
            with self.lock:
                cursor = self.conn.cursor()
                cursor.execute('''
                    SELECT user_input, ai_response 
                    FROM daily_logs 
                    WHERE user_input IS NOT NULL AND ai_response IS NOT NULL
                    ORDER BY RANDOM() 
                    LIMIT ?
                ''', (limit,))
                return cursor.fetchall()
        except Exception as e:
            print(f"âš ï¸ SQL Random Recall Error: {e}")
            return []

    def log_quantum_experience(self, task_type, prompt, best_id, best_score, details):
        try:
            with self.lock, self.conn:
                self.conn.execute(
                    "INSERT INTO quantum_logs (task_type, prompt, best_path_id, best_score, details_json) VALUES (?, ?, ?, ?, ?)",
                    (task_type, prompt, best_id, best_score, json.dumps(details, ensure_ascii=False))
                )
        except Exception as e:
            print(f"âš ï¸ SQL Quantum Log Error: {e}")

    def get_unprocessed_quantum_logs(self, limit=50):
        with self.lock:
            cursor = self.conn.cursor()
            cursor.execute('''
                SELECT id, prompt, best_path_id, details_json 
                FROM quantum_logs 
                WHERE is_processed=0 AND best_path_id != -1 AND best_score > 0.6
                LIMIT ?
            ''', (limit,))
            return cursor.fetchall()

    def mark_quantum_processed(self, ids):
        if not ids: return
        try:
            with self.lock, self.conn:
                self.conn.executemany("UPDATE quantum_logs SET is_processed=1 WHERE id=?", [(i,) for i in ids])
        except Exception as e:
            print(f"âš ï¸ SQL Mark Error: {e}")

# ==============================================================================
# æ ¸å¿ƒç”Ÿç‰©ç»„ä»¶
# ==============================================================================

class GeneticEngine:
    """è´Ÿè´£åŸºå› çš„é«˜æ–¯å˜å¼‚(ç”¨äºè´å¶æ–¯å†·å¯åŠ¨æˆ–å…œåº•)"""
    @staticmethod
    def mutate(genome, intensity=0.1):
        new_genome = genome.copy()
        # print(f"ğŸ§¬ [DNA] æ­£åœ¨è¿›è¡ŒåŸºå› é‡ç»„ (å¼ºåº¦: {intensity})...")
        for key, value in new_genome.items():
            if isinstance(value, float):
                # é«˜æ–¯å˜å¼‚
                mutation = random.gauss(0, intensity)
                new_value = value * (1.0 + mutation)
                # åŸºå› é”ï¼šé˜²æ­¢æ•°å€¼å´©å
                if "rate" in key: new_value = max(0.001, new_value)
                if "threshold" in key: new_value = max(0.1, new_value)
                new_genome[key] = new_value
        return new_genome


# ==============================================================================
# [NEW] è¿›åŒ–å¼•æ“: è´å¶æ–¯ä¼˜åŒ– (Bayesian Optimization)
# ==============================================================================
class BayesianGeneticEngine:
    """
    [è´å¶æ–¯è¿›åŒ–å™¨]
    ä¸å†ç›²ç›®éšæœºå˜å¼‚ï¼Œè€Œæ˜¯å»ºç«‹é«˜æ–¯è¿‡ç¨‹æ¨¡å‹ï¼Œ
    æ ¹æ®å†å²çš„ (åŸºå›  -> å­˜æ´»è¡¨ç°) é¢„æµ‹ä¸‹ä¸€ä»£æœ€ä¼˜å‚æ•°ã€‚
    """
    def __init__(self, db_manager):
        self.db = db_manager
        
        # 1. å®šä¹‰åŸºå› çš„æœç´¢ç©ºé—´ (ä¸Šä¸‹ç•Œ)
        self.pbounds = {
            'metabolism_rate': (0.5, 3.0),    # ä»£è°¢ç‡
            'reward_sensitivity': (1.0, 10.0),# å¤šå·´èƒºæ•æ„Ÿåº¦
            'anxiety_base_rate': (0.005, 0.1),# ç„¦è™‘å¢é•¿ç‡
            'curiosity_threshold': (1.0, 5.0),# å¥½å¥‡å¿ƒé˜ˆå€¼
            'stress_resilience': (0.1, 0.9),  # æŠ—å‹èƒ½åŠ›
            'risk_taking': (0.1, 0.9)         # å†’é™©ç³»æ•°
        }
        
        # 2. åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.optimizer = BayesianOptimization(
            f=None, # æˆ‘ä»¬ä½¿ç”¨ ask-and-tell æ¨¡å¼ï¼Œä¸éœ€è¦ä¼ é€’ç›®æ ‡å‡½æ•°
            pbounds=self.pbounds,
            random_state=1,
            verbose=0 # é™é»˜æ¨¡å¼
        )
        
        # 3. æ¢å¤å†å²è®°å¿† (Warm Start)
        self.history_loaded = False # åˆå§‹åŒ–æ ‡å¿—ä½
        self._load_history()

    # æ ¸å¿ƒè¯„åˆ†é€»è¾‘ (æ”¾åœ¨è¿™é‡Œï¼Œä¾›å…¶ä»–æ–¹æ³•è°ƒç”¨)
    def _calculate_target_score(self, duration, atp, loss_history=None, avg_loss=None):
        """
        [Math Core] Nezha-Sortino Quotient (ç”Ÿç‰©ç´¢æè¯ºå•†æ•°)
        
        åŸç†ï¼šè¿›åŒ–ä¸ä»…ä»…æ˜¯è¿½æ±‚ ATP æ€»é‡ï¼ˆé‚£ä¼šå¯¼è‡´çŸ­è§†çš„è´ªå©ªï¼‰ï¼Œ
        è€Œæ˜¯è¿½æ±‚â€œå•ä½é£é™©ä¸‹çš„è¶…é¢ç”Ÿå­˜è´¨é‡â€ã€‚
        
        å…¬å¼: Score = (R - R_f) / (Downside_Deviation + epsilon)
        å…¶ä¸­:
          R (Return): ATP ç§¯ç´¯æ•ˆç‡ (ATP / Age)
          R_f (Risk-free): åŸºç¡€ä»£è°¢ç»´æŒçº¿
          Risk: æƒŠå¥‡åº¦(Loss) çš„ä¸‹è¡Œæ³¢åŠ¨ç‡ (åªæƒ©ç½šæ„šè ¢çš„é”™è¯¯ï¼Œä¸æƒ©ç½šé¡¿æ‚Ÿ)
        """
        # 1. å®šä¹‰æ— é£é™©æ”¶ç›Šç‡ (Risk-Free Rate)
        # å‡è®¾æ¯æ´» 1 ä¸ª tick è‡³å°‘éœ€è¦æ¶ˆè€— 0.5 ATP (åŸºç¡€ä»£è°¢çº¿)
        risk_free_rate = 0.5 
        
        # 2. è®¡ç®—èµ„äº§å›æŠ¥ç‡ (Return)
        # ATP ç§¯ç´¯æ•ˆç‡ (ATP / Age)
        safe_duration = max(1.0, float(duration))
        asset_return = float(atp) / safe_duration
        
        # 3. è®¡ç®—é£é™© (Risk / Volatility)
        # é»˜è®¤é£é™© (å…œåº•)
        downside_deviation = 0.1

        # æƒ…å†µ A: æœ‰è¯¦ç»†çš„ Loss å†å² (è®¡ç®—ä¸‹è¡Œæ³¢åŠ¨)
        if loss_history and len(loss_history) > 5: # è‡³å°‘è¦æœ‰å‡ ä¸ªç‚¹æ‰èƒ½ç®—æ–¹å·®
            try:
                losses = np.array(loss_history)
                
                # ç´¢æè¯ºæ ¸å¿ƒï¼šåªè®¡ç®—â€œä¸å¯å®¹å¿çš„è®¤çŸ¥è¯¯å·®â€ (Downside Deviation)
                target_loss = 0.1 # å®¹å¿é˜ˆå€¼
                
                # ç­›é€‰å‡ºæ‰€æœ‰å¤§äº target_loss çš„éƒ¨åˆ†
                downside_losses = losses[losses > target_loss]
                
                if len(downside_losses) > 0:
                    # è®¡ç®—ä¸‹è¡Œå·®å€¼çš„å¹³æ–¹å‡å€¼çš„æ ¹
                    downside_diffs = downside_losses - target_loss
                    downside_deviation = np.sqrt(np.mean(np.square(downside_diffs)))
                else:
                    # å¦‚æœæ‰€æœ‰ Loss éƒ½å¾ˆä½ï¼Œç»™äºˆæå°çš„é£é™©å€¼ï¼ˆå¥–åŠ±ç¨³å¥ï¼‰
                    downside_deviation = 0.01
            except Exception:
                # Numpy å‡ºé”™æ—¶çš„å…œåº•
                downside_deviation = max(0.01, float(avg_loss)) if avg_loss else 0.1

        # æƒ…å†µ B: åªæœ‰å¹³å‡ Loss (å†å²å­˜æ¡£)
        else:
            safe_loss = avg_loss if avg_loss is not None else 1.0
            downside_deviation = max(0.01, float(safe_loss))

        # 4. è®¡ç®—ç´¢æè¯ºæ¯”ç‡
        excess_return = asset_return - risk_free_rate
        # åˆ†æ¯åŠ  epsilon é˜²æ­¢é™¤é›¶
        sortino_ratio = excess_return / (downside_deviation + 1e-6)
        
        # 5. å¼•å…¥ç”Ÿå­˜æ—¶é•¿çš„å¯¹æ•°å¥–åŠ± (Longevity Bonus)
        longevity_bonus = math.log1p(safe_duration) * 0.5
        
        # æœ€ç»ˆå¾—åˆ†
        final_score = sortino_ratio + longevity_bonus
        
        # [æå€¼ä¿æŠ¤] é™åˆ¶åŒºé—´
        return max(-10.0, min(100.0, final_score))


    def _load_history(self):
        """ä» SQL è¯»å–å¾€ä¸–çš„å†å²ï¼Œè®©è´å¶æ–¯èµ¢åœ¨èµ·è·‘çº¿ (æ•°å­¦ä¿®æ­£ç‰ˆ)"""
        try:
            with self.db.lock:
                cursor = self.db.conn.cursor()
                cursor.execute("SELECT genome_snapshot, survival_duration, final_atp, avg_loss FROM evolution_metrics")
                rows = cursor.fetchall()
            
            if rows:
                print(f"ğŸ§¬ [è¿›åŒ–å¼•æ“] ç»§æ‰¿äº† {len(rows)} ä»£ç¥–å…ˆçš„è®°å¿†...")
                for genome_json, duration, atp, loss in rows:
                    try:
                        params = json.loads(genome_json)
                        
                        # ã€ä¿®æ”¹ã€‘ è¿™é‡Œä¸å†å†™æ­»å…¬å¼ï¼Œè€Œæ˜¯è°ƒç”¨æ–°æ–¹æ³•
                        target_score = self._calculate_target_score(duration, atp, loss)

                        self.optimizer.register(params=params, target=target_score)
                    except: continue
                self.history_loaded = True
            else:
                print("ğŸ§¬ [è¿›åŒ–å¼•æ“] åˆ›ä¸–çºª (Genesis Mode) - æ— å†å²æ•°æ®")
        except Exception as e:
            print(f"âš ï¸ å†å²åŠ è½½å¤±è´¥: {e}")

    def suggest_mutation(self, current_genome):
        """
        [æ ¸å¿ƒ] è¯¢é—®è´å¶æ–¯ç¥è°•ï¼šä¸‹ä¸€ä¸–è¯¥å¦‚ä½•è°ƒæ•´åŸºå› ï¼Ÿ
        """
        # å†·å¯åŠ¨ä¿æŠ¤
        if not self.history_loaded and len(self.optimizer.space) < 5:
            return GeneticEngine.mutate(current_genome, intensity=0.2)
            
        try:
            next_point = self.optimizer.suggest(utility_kind="ucb", kappa=2.5)
            new_genome = current_genome.copy()
            for key, val in next_point.items():
                new_genome[key] = float(val)
            return new_genome
        except:
            return GeneticEngine.mutate(current_genome, intensity=0.1)

    def record_epoch(self, genome, age, avg_loss, atp, loss_history_snapshot=None): # <--- å‚æ•°æ›´æ–°
        """è®°å½•è¿™ä¸€ä¸–çš„æœ€ç»ˆå¾—åˆ†"""
        try:
            if age < 1: return
            
            # 1. å­˜å…¥ SQL (ä¿æŒåŸæ ·)
            with self.db.lock:
                with self.db.conn:
                    self.db.conn.execute(
                        "INSERT INTO evolution_metrics (age, genome_snapshot, avg_loss, survival_duration, final_atp) VALUES (?, ?, ?, ?, ?)",
                        (age, json.dumps(genome), avg_loss, age, atp)
                    )
            
            # 2. å®æ—¶å‘ŠçŸ¥è´å¶æ–¯ä¼˜åŒ–å™¨
            # [ä¿®æ”¹] ä½¿ç”¨æ–°çš„ç´¢æè¯ºå…¬å¼
            current_score = self._calculate_target_score(
                duration=age, 
                atp=atp, 
                loss_history=loss_history_snapshot, # ä¼ å…¥å¿«ç…§
                avg_loss=avg_loss
            )
            
            valid_params = {k: v for k, v in genome.items() if k in self.pbounds}
            
            try:
                self.optimizer.register(params=valid_params, target=current_score)
                print(f"      ğŸ§¬ [è´å¶æ–¯] å·²å¸æ”¶æœ¬ä¸–ç»éªŒ (Score: {current_score:.2f})")
            except Exception:
                pass

        except Exception as e:
            print(f"âš ï¸ è®°å½•å¤±è´¥: {e}")


class SurvivalInstinct:
    """æä»æ ¸ï¼šå‹åŠ›è°ƒèŠ‚ä¸­å¿ƒ"""
    def __init__(self, genome):
        self.genome = genome
        self.pressure_level = 1.0
        
    def analyze_feedback(self, daily_logs):
        """æ ¹æ®å·®è¯„ç‡å’ŒæŠ—å‹åŸºå› è®¡ç®—å‹åŠ›"""
        if not daily_logs: return 1.0
        
        negatives = sum(1 for log in daily_logs if log.get('feedback', 0) < 0)
        failure_rate = negatives / len(daily_logs)
        
        # è¯»å–åŸºå› 
        resilience = self.genome.get('stress_resilience', 0.3)
        cortisol = self.genome.get('cortisol_level', 2.0)
        
        if failure_rate > resilience:
            self.pressure_level = cortisol # å‹åŠ›é£™å‡
            print(f"ğŸ’“ [æä»æ ¸] å‹åŠ›è¿‡è½½! (å¤±è´¥ç‡ {failure_rate:.1%} > æŠ—å‹ {resilience})")
        elif failure_rate == 0:
            self.pressure_level = 0.8 # å®‰é€¸
        else:
            self.pressure_level = 1.0 # æ­£å¸¸
        return self.pressure_level

class IntrinsicCuriosityModule:
    """[è½»é‡åŒ–] å¥½å¥‡å¿ƒæ¨¡å—ï¼šåªè´Ÿè´£è®¡ç®—ç†µï¼Œä¸æŒæœ‰æ¨¡å‹ (æ•°å€¼ç¨³å®šå¢å¼ºç‰ˆ)"""
    def __init__(self, genome):
        self.genome = genome
        
    def analyze_logits(self, logits):
        """è¾“å…¥ Logitsï¼Œè¿”å› (éœ€ä¸éœ€è¦æœç´¢, ç†µå€¼)"""
        # [Step 1] Logits ç¨³å®šæ€§å¤„ç† (Shift Trick)
        # å‡å»æœ€å¤§å€¼é˜²æ­¢ exp æº¢å‡ºï¼Œè¿™ä¸æ”¹å˜ Softmax çš„ç»“æœ
        # keepdim=True ä¿æŒç»´åº¦ä»¥ä¾¿å¹¿æ’­
        logits_stabilized = logits - torch.max(logits, dim=-1, keepdim=True)[0]
        
        # [Step 2] è®¡ç®— Log Probs (æ•°å€¼ç¨³å®šçš„ log_softmax)
        log_probs = F.log_softmax(logits_stabilized, dim=-1)
        probs = torch.exp(log_probs)
        
        # [Step 3] è®¡ç®—ç†µ p * log(p)
        p_log_p = probs * log_probs
        
        # [Step 4] NaN ä¿æŠ¤ (å…³é”®ä¿®æ­£)
        # å½“ p -> 0 æ—¶ï¼Œlog(p) -> -infï¼Œä¹˜ç§¯ä¸º NaNã€‚
        # æ•°å­¦æé™ lim(x->0) x*log(x) = 0
        p_log_p = torch.nan_to_num(p_log_p, nan=0.0)
        
        # è®¡ç®—é¦™å†œç†µ: -sum(p * log_p)
        # è¿™æ˜¯ä¸€ä¸ªæ ‡é‡
        entropy = -torch.sum(p_log_p, dim=-1).mean().item() # å– batch å¹³å‡
        
        # [Step 5] ç‰©ç†é’³ä½ (Clamping)
        # é™åˆ¶ç†µçš„èŒƒå›´ï¼Œé˜²æ­¢ä¸‹æ¸¸æ•°å€¼çˆ†ç‚¸
        entropy = max(0.0, min(20.0, entropy)) 

        threshold = self.genome.get('curiosity_threshold', 3.5)
        return (entropy > threshold), entropy



# ==============================================================================
# ç¥ç»å¤–ç§‘åŒ»ç”Ÿï¼šè´Ÿè´£ LoRA Adapter çš„åŠ¨æ€æ‰‹æœ¯ (Rank ä¼¸ç¼©)
# ==============================================================================
class NeuroSurgeon:
    """è´Ÿè´£ LoRA Adapter çš„åŠ¨æ€åˆ‡æ¢ã€æ‰©å®¹ä¸å‹ç¼©"""
    
    @staticmethod
    def setup_dual_brain(model):
        """åˆå§‹åŒ–åŒç³»ç»Ÿçš®å±‚ (å¢å¼ºç‰ˆï¼šè‡ªåŠ¨è¯†åˆ« MoE æ¶æ„å¹¶é€‚é…å±‚çº§åç§°ï¼Œè‡ªåŠ¨å¤„ç† PeftModel åŒ…è£…)"""
        print("ğŸ§  [å¤–ç§‘æ‰‹æœ¯] æ­£åœ¨åˆ’åˆ†å¤§è„‘åŒºåŸŸ (Fast vs Slow)...")
        
        # --- [æ–°å¢] 1. æ™ºèƒ½æ¶æ„è¯†åˆ« ---
        # ç›®çš„ï¼šåˆ¤æ–­å½“å‰æ¨¡å‹æ˜¯ä¼ ç»Ÿçš„ Dense ç»“æ„è¿˜æ˜¯è¿›åŒ–åçš„ MoE ç»“æ„
        is_moe = False
        # æ£€æµ‹ 1: ç‰©ç†æ ‡è®°æ–‡ä»¶ (ç”±é£å‡ç¨‹åºç”Ÿæˆ)
        if os.path.exists(Config.IS_MOE_FLAG_FILE):
            is_moe = True
        # æ£€æµ‹ 2: æ¨¡å‹é…ç½® (åŒé‡ä¿é™©ï¼Œæ£€æŸ¥ config ä¸­æ˜¯å¦æœ‰ moe ç›¸å…³å­—æ®µ)
        elif hasattr(model, "config") and (
            "moe" in getattr(model.config, "model_type", "").lower() or
            getattr(model.config, "num_local_experts", 0) > 0
        ):
            is_moe = True

        # --- [æ–°å¢] 2. åŠ¨æ€å®šä¹‰æ‰‹æœ¯ç›®æ ‡ (Target Modules) ---
        # åŸºç¡€æ³¨æ„åŠ›å±‚ (ä»»ä½•æ¶æ„éƒ½æœ‰)
        targets_attn = ["q_proj", "k_proj", "v_proj", "o_proj"]
        
        # å®šä¹‰ MLP/MoE ç›¸å…³çš„å±‚åç§°å…³é”®è¯æ± 
        # gate: MoE Router (è·¯ç”±å±‚)
        # w1/w2/w3: Mixtral/Llama ç»“æ„çš„ MLP å±‚
        # gate_proj/up_proj/down_proj: Qwen/Llama ç»“æ„çš„ MLP å±‚
        mlp_keywords = ["gate", "w1", "w2", "w3", "gate_proj", "up_proj", "down_proj"]

        if is_moe:
            print("   -> ğŸ§¬ [æ£€æµ‹åˆ° MoE] æ­£åœ¨è°ƒæ•´ LoRA è§¦æ‰‹ä»¥è¿æ¥ è·¯ç”±å±‚(Router) ä¸ ä¸“å®¶å±‚(Experts)...")
            
            # Slow (é€»è¾‘ç³»ç»Ÿ): å…¨é‡è®­ç»ƒã€‚
            # åŒ…å«æ³¨æ„åŠ›å±‚ + è·¯ç”±å±‚ + æ‰€æœ‰ä¸“å®¶çš„å†…éƒ¨ MLP å±‚
            # Peft ä¼šè‡ªåŠ¨å¿½ç•¥å½“å‰æ¨¡å‹ä¸­ä¸å­˜åœ¨çš„å±‚åï¼Œæ‰€ä»¥å†™å…¨ä¸€ç‚¹æ²¡å…³ç³»
            targets_slow = targets_attn + mlp_keywords
            
            # Fast (ç›´è§‰ç³»ç»Ÿ): è½»é‡è®­ç»ƒã€‚
            # åªè®­ç»ƒ æ³¨æ„åŠ› + è·¯ç”±(gate) + æ™®é€šé—¨æ§(gate_proj)
            # ç­–ç•¥ï¼šæ§åˆ¶â€œè°æ¥å›ç­”â€å’Œâ€œå…³æ³¨ä»€ä¹ˆâ€ï¼Œä½†ä¸ä¿®æ”¹ä¸“å®¶å†…éƒ¨åºå¤§çš„çŸ¥è¯†åº“ï¼ŒèŠ‚çœæ˜¾å­˜
            targets_fast = targets_attn + ["gate", "gate_proj"] 
        else:
            print("   -> ğŸ§¬ [æ£€æµ‹åˆ° Dense] ä½¿ç”¨æ ‡å‡†å±‚çº§æ˜ å°„ã€‚")
            # Dense Slow: è®­ç»ƒ Attention + MLP
            targets_slow = targets_attn + ["gate_proj", "up_proj", "down_proj"]
            # Dense Fast: åªè®­ç»ƒ Attention (ä¿æŒæç®€)
            targets_fast = targets_attn

        # --- [ä¿®æ”¹] 3. å®šä¹‰é…ç½® (ä½¿ç”¨åŠ¨æ€è®¡ç®—çš„ target_modules) ---
        
        # [å¯é€‰ä¼˜åŒ–] MoE æ¨¡å¼ä¸‹ï¼Œå¦‚æœ Rank è¿‡å¤§å®¹æ˜“ OOMï¼Œå¯åœ¨æ­¤å¤„åŠ¨æ€é™çº§ï¼Œè¿™é‡Œæš‚æ—¶ä¿æŒåŸé…ç½®
        
        fast_config = LoraConfig(
            r=Config.RANK_FAST, 
            lora_alpha=Config.RANK_FAST * 2, 
            lora_dropout=0.05, 
            bias="none", task_type="CAUSAL_LM",
            target_modules=targets_fast # <--- [ä¿®æ”¹] ä½¿ç”¨åŠ¨æ€å˜é‡
        )

        # æ˜¾å­˜ä¿æŠ¤ç­–ç•¥
        # MoE æ¨¡å¼ä¸‹ï¼Œåº•æ¨¡æ˜¾å­˜å ç”¨ç¿»å€ï¼Œä¸” target_modules å˜å¤šï¼ŒLoRA å‚æ•°é‡ä¼šæ¿€å¢ã€‚
        # å¼ºåˆ¶å°† Rank é™åˆ¶åœ¨å®‰å…¨èŒƒå›´ï¼ˆå¦‚ 64ï¼‰ï¼Œé˜²æ­¢ OOMã€‚
        real_rank_slow = Config.RANK_SLOW
        if is_moe and real_rank_slow > 64:
            print(f"      âš ï¸ [æ˜¾å­˜ä¿æŠ¤] MoE æ¨¡å¼ä¸‹å°† Slow Rank é™åˆ¶ä¸º 64 (åŸ: {real_rank_slow})")
            real_rank_slow = 64

        slow_config = LoraConfig(
            r=Config.RANK_SLOW, 
            lora_alpha=Config.RANK_SLOW * 2, 
            lora_dropout=0.05, 
            bias="none", task_type="CAUSAL_LM",
            target_modules=targets_slow # <--- [ä¿®æ”¹] ä½¿ç”¨åŠ¨æ€å˜é‡
        )

        # 4. æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯ PeftModel (ä¿ç•™åŸæœ‰ç¨³å¥é€»è¾‘)
        # å¦‚æœä¸æ˜¯ï¼Œè¯´æ˜æ˜¯ç¬¬ä¸€æ¬¡åˆå§‹åŒ–ï¼Œå¿…é¡»ç”¨ get_peft_model åŒ…è£…ä¸€æ¬¡
        if not isinstance(model, PeftModel):
            if os.path.exists(Config.ADAPTER_FAST_PATH):
                # å¦‚æœç£ç›˜æœ‰å­˜æ¡£ï¼Œç›´æ¥åŠ è½½å­˜æ¡£å˜æˆ PeftModel
                model = PeftModel.from_pretrained(model, Config.ADAPTER_FAST_PATH, adapter_name="fast")
            else:
                # å¦‚æœæ²¡æœ‰å­˜æ¡£ï¼Œç”¨ fast é…ç½®åˆå§‹åŒ–ä¸€ä¸ª PeftModel
                model = get_peft_model(model, fast_config, adapter_name="fast")
        else:
            # å¦‚æœå·²ç»æ˜¯ PeftModelï¼Œå°è¯•åŠ è½½æˆ–æ·»åŠ  fast
            if "fast" not in model.peft_config:
                if os.path.exists(Config.ADAPTER_FAST_PATH):
                    model.load_adapter(Config.ADAPTER_FAST_PATH, adapter_name="fast")
                else:
                    # æ˜¾å¼æŒ‡å®šå‚æ•°åï¼Œé˜²æ­¢å‚æ•°ä½ç½®å†²çª
                    model.add_adapter(adapter_name="fast", peft_config=fast_config)

        # 5. å¤„ç† Slow Adapter (æ­¤æ—¶ model ä¸€å®šæ˜¯ PeftModel äº†ï¼Œå¯ä»¥ç›´æ¥ç”¨ add_adapter/load_adapter)
        if "slow" not in model.peft_config:
            if os.path.exists(Config.ADAPTER_SLOW_PATH):
                model.load_adapter(Config.ADAPTER_SLOW_PATH, adapter_name="slow")
            else:
                # ã€ä¿®å¤ç‚¹ 2ã€‘æ˜¾å¼æŒ‡å®šå‚æ•°åï¼Œè§£å†³æŠ¥é”™ TypeError
                model.add_adapter(adapter_name="slow", peft_config=slow_config)
            
        return model

    @staticmethod
    def resize_adapter(model, adapter_name, new_rank):
        """
        [æ ¸å¿ƒæ‰‹æœ¯] åŠ¨æ€è°ƒæ•´ LoRA Rank (SVD å¢å¼º + å®‰å…¨é”)
        æœºåˆ¶ï¼šæå–æ—§æƒé‡ -> SVDåˆ†è§£ä¿ç•™ä¸»æˆåˆ†(å‹ç¼©) æˆ– å™ªå£°å¡«å……(æ‰©å®¹)
        """
        try:
            # 1. è·å–æ—§é…ç½®å’Œæƒé‡
            old_config = model.peft_config[adapter_name]
            old_rank = old_config.r
            
            if old_rank == new_rank:
                return model # æ— éœ€æ‰‹æœ¯

            print(f"ğŸ’‰ [æ‰‹æœ¯å®¤] æ‰§è¡Œç¥ç»é‡å¡‘: {adapter_name} (Rank {old_rank} -> {new_rank})...")

            # 1. æå–æ—§æƒé‡ (Clone åˆ° CPU æˆ–ä¿ç•™åœ¨ GPU å‡å¯ï¼Œå…³é”®æ˜¯ detach)
            old_weights = {}
            for n, p in model.named_parameters():
                if adapter_name in n and "lora_" in n:
                    old_weights[n] = p.clone().detach()

            # 2. åˆ é™¤æ—§ Adapter
            model.delete_adapter(adapter_name)

            # 3. æ–°å»º Adapter
            new_config = copy.deepcopy(old_config)
            new_config.r = new_rank
            new_config.lora_alpha = new_rank * 2 
            
            # ã€ä¿®å¤ç‚¹ 3ã€‘è¿™é‡Œä¹Ÿå»ºè®®ä½¿ç”¨æ˜¾å¼å‚æ•°ï¼Œè™½ç„¶ peft_config æ˜¯ add_adapter çš„ç¬¬äºŒä¸ªå‚æ•°ï¼Œä½†æ˜¾å¼æ›´å®‰å…¨
            model.add_adapter(adapter_name=adapter_name, peft_config=new_config)

            # 4. æƒé‡è¿ç§» (å¼€å¯ no_grad ä¿æŠ¤)
            # âš ï¸ å¿…é¡»ä½¿ç”¨ no_gradï¼Œå¦åˆ™æ‰‹æœ¯è¿‡ç¨‹ä¼šè¢«è®¡å…¥æ¢¯åº¦å›¾ï¼Œå¯¼è‡´æ˜¾å­˜æ³„æ¼æˆ–æŠ¥é”™
            with torch.no_grad():
                for n, p in model.named_parameters():
                    if adapter_name in n and "lora_" in n:
                        if n in old_weights:
                            old_p = old_weights[n]
                            new_p_data = p.data
                            
                            # --- SVD å‹ç¼© (Rank â†“) ---
                            if new_rank < old_rank:
                                try:
                                    old_p_f32 = old_p.float()
                                    
                                    # æ•°æ®æ¸…æ´—ï¼šæ™ºèƒ½å¡«å…… Inf/NaN
                                    if not torch.isfinite(old_p_f32).all():
                                        # 1. è®¡ç®—å½“å‰çŸ©é˜µä¸­æœ‰æ•ˆå€¼çš„æœ€å¤§ç»å¯¹å€¼ (è‡ªé€‚åº”è¾¹ç•Œ)
                                        # å¦‚æœå…¨éƒ½æ˜¯ Inf/NaN (æç«¯æƒ…å†µ)ï¼Œåˆ™å…œåº•ä½¿ç”¨ 1.0
                                        finite_mask = torch.isfinite(old_p_f32)
                                        if finite_mask.any():
                                            max_val = torch.max(torch.abs(old_p_f32[finite_mask]))
                                        else:
                                            max_val = 1.0
                                            
                                        # 2. ä½¿ç”¨è¿™ä¸ªè¾¹ç•Œå€¼å¡«å…… Infï¼Œç”¨ 0 å¡«å…… NaN
                                        # è¿™æ ·æ—¢æ ‡è®°äº†æº¢å‡ºï¼Œåˆä¸ä¼šç ´å SVD çš„è°±ç»“æ„
                                        old_p_f32 = torch.nan_to_num(
                                            old_p_f32, 
                                            nan=0.0, 
                                            posinf=max_val, 
                                            neginf=-max_val
                                        )
                                    
                                    if "lora_A" in n:
                                        # A (r_old, dim) -> å‹ç¼©è¡Œ
                                        U, S, Vh = torch.linalg.svd(old_p_f32, full_matrices=False)
                                        # new_A = S[:new] * Vh[:new]
                                        compressed = torch.diag(S[:new_rank]) @ Vh[:new_rank, :]
                                        new_p_data.copy_(compressed.to(p.dtype))
                                    elif "lora_B" in n:
                                        # B (dim, r_old) -> å‹ç¼©åˆ—
                                        U, S, Vh = torch.linalg.svd(old_p_f32, full_matrices=False)
                                        # new_B = U[:new] * S[:new]
                                        compressed = U[:, :new_rank] @ torch.diag(S[:new_rank])
                                        new_p_data.copy_(compressed.to(p.dtype))
                                except:
                                    # SVD å¤±è´¥å›é€€é€»è¾‘
                                    if "lora_A" in n: new_p_data.copy_(old_p[:new_rank, :])
                                    elif "lora_B" in n: new_p_data.copy_(old_p[:, :new_rank])

                            # --- å™ªå£°æ‰©å®¹ (Rank â†‘) ---
                            else:
                                if "lora_A" in n:
                                    new_p_data[:old_rank, :] = old_p
                                    
                                    # ç»Ÿä¸€è‡´æ•¬ PhysicalNeuroSurgeon çš„åŠ¨æ€å™ªå£°ç­–ç•¥
                                    # è®¡ç®—åŸæƒé‡çš„æ ‡å‡†å·®ï¼Œå– 1% æˆ– 0.1% ä½œä¸ºå™ªå£°å¼ºåº¦
                                    std = old_p.std()
                                    if std < 1e-6: std = 0.01 # é˜²æ­¢å…¨0å±‚çš„é™¤é›¶é£é™©
                                    
                                    # å¡«å……æ–°ç»´åº¦
                                    new_p_data[old_rank:, :].normal_(mean=0.0, std=std * 0.1) 
                                    
                                elif "lora_B" in n:
                                    new_p_data[:, :old_rank] = old_p
                                    new_p_data[:, old_rank:].zero_()

            print(f"âœ… [æ‰‹æœ¯æˆåŠŸ] å¤§è„‘çš®å±‚é‡å¡‘å®Œæ¯•ã€‚")
            model.set_adapter(adapter_name)
            return model
            
        except Exception as e:
            print(f"âŒ [æ‰‹æœ¯å¤±è´¥] {e}")
            return model

    @staticmethod
    def switch_mode(model, mode="fast"):
        """åˆ‡æ¢æ€è€ƒæ¨¡å¼"""
        model.set_adapter(mode)
        return mode


# ==============================================================================
# é«˜çº§ç¥ç»ç»„ä»¶ (PFC, Net2Net, Feeder)
# ==============================================================================

class PrefrontalCortex(nn.Module):
    """[V7.2] å‰é¢å¶çš®å±‚ï¼šåŸºäº Hidden States çš„å…ƒè®¤çŸ¥è·¯ç”±"""
    def __init__(self, input_dim, device="cuda"):
        super().__init__()
        self.output_dim = len(Config.ACTION_SPACE) # åŠ¨ä½œç©ºé—´: SELF, WEB, TEACHER, CODE, MODIFY
        self.router = nn.Linear(input_dim, self.output_dim).to(device)
        self.optimizer = torch.optim.AdamW(self.router.parameters(), lr=1e-3)
        self.device = device
        
        # å°è¯•åŠ è½½æ—§æƒé‡
        if os.path.exists(Config.PFC_ROUTER_PATH):
            try:
                state = torch.load(Config.PFC_ROUTER_PATH)
                
                # åŒæ—¶æ£€æŸ¥ Hidden Size (è¾“å…¥) å’Œ Action Space (è¾“å‡º) æ˜¯å¦åŒ¹é…
                saved_in_dim = state['weight'].shape[1]
                saved_out_dim = state['weight'].shape[0]
                
                if saved_in_dim == input_dim and saved_out_dim == self.output_dim:
                    self.router.load_state_dict(state)
                    print("ğŸ§  [PFC] å·²åŠ è½½å…ƒè®¤çŸ¥è·¯ç”±æƒé‡ã€‚")
                else:
                    print(f"âš ï¸ [PFC] ç»´åº¦å˜æ›´ (Old:{saved_out_dim}x{saved_in_dim} -> New:{self.output_dim}x{input_dim})ï¼Œé‡ç½®è·¯ç”±å™¨ã€‚")
            except Exception as e:
                print(f"âš ï¸ [PFC] æƒé‡åŠ è½½å¤±è´¥: {e}")

    def decide(self, h):
        """
        [æ•°å­¦ä¿®æ­£ç‰ˆ] è¾“å…¥ Hidden Stateï¼Œè¾“å‡ºåŸºäºå½’ä¸€åŒ–ç†µçš„é²æ£’å†³ç­–
        """
        # åŠ¨æ€è·å–æƒé‡ç±»å‹ï¼Œç¡®ä¿ä¸è¾“å…¥å¯¹é½ (fp16/fp32)
        x = h.to(self.device).to(self.router.weight.dtype)
        
        with torch.no_grad():
            logits = self.router(x)
            probs = torch.softmax(logits, dim=-1)
            
            # 1. è·å–æœ€å¤§æ¦‚ç‡åŠ¨ä½œ
            idx = torch.argmax(probs).item()
            # raw_conf = probs[0, idx].item() # (æ—§æŒ‡æ ‡ï¼Œä»…ä½œå‚è€ƒ)
            
            # 2. è®¡ç®—é¦™å†œç†µ H(p) = -sum(p * log(p))
            # [å®‰å…¨ä¼˜åŒ–] ä½¿ç”¨ clamp é™åˆ¶æœ€å°æ¦‚ç‡ï¼Œæ¯” +1e-9 æ›´ç¨³å®šï¼Œé˜²æ­¢ log æº¢å‡º
            p_safe = torch.clamp(probs, min=1e-9, max=1.0)
            entropy = -torch.sum(p_safe * torch.log(p_safe), dim=-1).item()
            
            # 3. è®¡ç®—æœ€å¤§å¯èƒ½çš„ç†µ H_max = ln(N)
            n_actions = len(Config.ACTION_SPACE)
            
            # [è¾¹ç•Œä¿æŠ¤] é˜²æ­¢åŠ¨ä½œç©ºé—´ä¸º 1 æ—¶é™¤ä»¥ 0
            if n_actions > 1:
                max_entropy = math.log(n_actions)
                # 4. è®¡ç®—å½’ä¸€åŒ–ä¸ç¡®å®šåº¦ U (0.0 ~ 1.0)
                uncertainty_norm = entropy / max_entropy
            else:
                uncertainty_norm = 0.0 # åªæœ‰ä¸€ä¸ªåŠ¨ä½œæ—¶ï¼Œå®Œå…¨ç¡®å®š
            
            # 5. è®¡ç®—é²æ£’ç½®ä¿¡åº¦ (Robust Confidence)
            # èŒƒå›´ä¸¥æ ¼åœ¨ 0.0 ~ 1.0 ä¹‹é—´ï¼Œéå¸¸é€‚åˆä¸ Override Threshold æ¯”è¾ƒ
            robust_conf = 1.0 - uncertainty_norm
            
            # é’³ä½ä¸€ä¸‹ç»“æœï¼Œé˜²æ­¢æµ®ç‚¹è¯¯å·®å¯¼è‡´ç•¥å¾®è¶…è¿‡ 1.0 æˆ–ä½äº 0.0
            robust_conf = max(0.0, min(1.0, robust_conf))
        
        actions = Config.ACTION_SPACE
        # è¿”å› robust_conf ä»£æ›¿åŸæ¥çš„ raw_conf
        return actions[idx], robust_conf, idx
        

    def learn(self, h, label_idx):
        """å¼ºåŒ–å­¦ä¹ ï¼šæ›´æ–°è·¯ç”±ç­–ç•¥"""
        self.router.train()
        self.optimizer.zero_grad()
        # æ„é€  Target
        target = torch.tensor([label_idx], device=self.device)
        # å‰å‘ä¼ æ’­
        logits = self.router(h.to(self.device).float())
        loss = F.cross_entropy(logits, target)
        loss.backward()
        self.optimizer.step()
        
        # å®æ—¶ä¿å­˜
        torch.save(self.router.state_dict(), Config.PFC_ROUTER_PATH)
        return loss.item()



# ==============================================================================
# å…¨èƒ½è¿›åŒ–å¸ˆï¼šè´Ÿè´£ Dense ç”Ÿé•¿ä¸ MoE ç»´åº¦é£å‡
# ==============================================================================
class AdvancedPhysicalSurgeon:
    """
    [å…¨èƒ½è¿›åŒ–å¤§å¸ˆ V3.0]
    é˜¶æ®µä¸€ï¼šç‰©ç†ç”Ÿé•¿ (Neurogenesis) - åœ¨æ˜¾å­˜å…è®¸çš„æƒ…å†µä¸‹ï¼Œç›´æ¥åœ¨å†…å­˜ä¸­å¢åŠ å±‚æ•°ã€‚
    é˜¶æ®µäºŒï¼šç»´åº¦é£å‡ (Mergekit MoE) - å½“å•å¡æ˜¾å­˜è€—å°½ï¼Œè°ƒç”¨å¤–éƒ¨å·¥å…·é‡æ„ä¸º MoE é›†ç¾¤ã€‚
    """
    # ===========================
    # é˜¶æ®µä¸€ï¼šç‰©ç†æ˜¾å¾®æ‰‹æœ¯ (é’ˆå¯¹ Dense)
    # ===========================

    @staticmethod
    def grow_layer_robust(model, copy_ratio=0.6):
        print("ğŸŒ± [æ— æŸç”Ÿé•¿] æ­£åœ¨æ‰§è¡Œ Identity Morphism (Net2Net) å¢å±‚æ‰‹æœ¯...")
        
        # 1. å®šä½å±‚ç»“æ„
        if hasattr(model, "model") and hasattr(model.model, "layers"):
            layers = model.model.layers
            config = model.config
        else:
            print("âŒ [æ‰‹æœ¯å¤±è´¥] æ— æ³•å®šä½ Transformer Layers ç»“æ„ã€‚")
            return model

        current_depth = len(layers)
        
        # 2. é€‰æ‹©æºå±‚ (Source Layer) - é€‰å–ä¸­åæ®µ
        target_idx = int(current_depth * copy_ratio)
        print(f"   -> æ­£åœ¨å…‹éš†ç¬¬ {target_idx} å±‚ (Source)...")
        
        source_layer = layers[target_idx]

        # =========== [å®‰å…¨ä¼˜åŒ–] CPU ä¸­è½¬æ‰‹æœ¯å° Start ===========
        # åºåˆ—åŒ–ä¸­è½¬å…‹éš†æ³• Start
        # åŸç†ï¼šdeepcopy ä¼šå…ˆåœ¨ GPU å¤åˆ¶å†æ¬è¿ï¼Œå¯èƒ½å¯¼è‡´ OOMã€‚
        # æ–¹æ¡ˆï¼šä½¿ç”¨ torch.save åºåˆ—åŒ–åˆ° RAMï¼Œå† load å›æ¥æ—¶ç›´æ¥æŒ‡å®š map_location='cpu'ã€‚
        # ç»“æœï¼šæ–°å±‚ç›´æ¥è¯ç”Ÿåœ¨ CPUï¼Œå…¨è¿‡ç¨‹ 0 é¢å¤–æ˜¾å­˜å ç”¨ã€‚
        
        try:
            buffer = io.BytesIO()
            # 1. å°†æºå±‚ä¿å­˜åˆ°å†…å­˜ Buffer (åªè¯» GPU æ•°æ®ï¼Œä¸å æ˜¾å­˜)
            torch.save(source_layer, buffer)
            buffer.seek(0)
            
            # 2. ä» Buffer åŠ è½½åˆ° CPU (ç›´æ¥åœ¨ RAM åˆ›å»ºæ–°å¯¹è±¡)
            new_layer = torch.load(buffer, map_location='cpu')
            
            del buffer # é‡Šæ”¾ç¼“å†²åŒº
            
        except Exception as e:
            print(f"âŒ [å…‹éš†å¤±è´¥] å†…å­˜ä¸è¶³æˆ–åºåˆ—åŒ–é”™è¯¯: {e}")
            return model
        
        # 3. Zero-Initialization (é›¶åˆå§‹åŒ–)
        # å°†æ–°å±‚çš„è¾“å‡ºæƒé‡å¼ºåˆ¶å½’é›¶ï¼Œä½¿å…¶åœ¨åˆå§‹çŠ¶æ€ä¸‹â€œéšå½¢â€
        with torch.no_grad():
            # A. å½’é›¶ Attention è¾“å‡º (o_proj)
            if hasattr(new_layer, "self_attn") and hasattr(new_layer.self_attn, "o_proj"):
                nn.init.constant_(new_layer.self_attn.o_proj.weight, 0)
                if new_layer.self_attn.o_proj.bias is not None:
                    nn.init.constant_(new_layer.self_attn.o_proj.bias, 0)
            
            # B. å½’é›¶ MLP è¾“å‡º (down_proj)
            if hasattr(new_layer, "mlp") and hasattr(new_layer.mlp, "down_proj"):
                nn.init.constant_(new_layer.mlp.down_proj.weight, 0)
                if new_layer.mlp.down_proj.bias is not None:
                    nn.init.constant_(new_layer.mlp.down_proj.bias, 0)

            # C. æå¾®é‡æ‰°åŠ¨ (æ‰“ç ´å¯¹ç§°æ€§)
            for n, p in new_layer.named_parameters():
                if "o_proj" in n or "down_proj" in n:
                    p.data.add_(torch.randn_like(p) * 1e-8) 

        # 4. æ¬è¿å› GPU (æˆ–è€…åŸæ¥çš„ device)
        # è·å–åŸæ¨¡å‹çš„ device
        target_device = source_layer.self_attn.q_proj.weight.device
        try:
            print(f"   -> æ­£åœ¨å°†æ–°å±‚æ¬è¿å› {target_device}...")
            new_layer = new_layer.to(target_device)
        except RuntimeError as e:
            if "out of memory" in str(e):
                print("âŒ [æ‰‹æœ¯å¤±è´¥] åˆå§‹åŒ–æˆåŠŸä½†åœ¨æ¬å› GPU æ—¶æ˜¾å­˜ä¸è¶³ã€‚")
                return model # æ”¾å¼ƒç”Ÿé•¿
            raise e

        # 5. ç‰©ç†æ’å…¥
        layers.insert(target_idx + 1, new_layer)
        
        # 5. æ›´æ–° Config
        if hasattr(config, "num_hidden_layers"): config.num_hidden_layers += 1
        if hasattr(model.config, "num_hidden_layers"): model.config.num_hidden_layers += 1
        
        # 6. é‡ç½® RoPE ç´¢å¼• (è‡³å…³é‡è¦)
        AdvancedPhysicalSurgeon._reindex_layers(layers)

        print(f"âœ… [ç”Ÿé•¿å®Œæˆ] æ·±åº¦: {current_depth} -> {len(layers)}ã€‚")
        return model

    @staticmethod
    def shrink_layer_safe(model):
        print("ğŸ‚ [ç‰©ç†èç¼©] æ­£åœ¨æ‰§è¡Œè„‘å¶åˆ‡é™¤...")
        if hasattr(model, "model") and hasattr(model.model, "layers"):
            layers = model.model.layers
        else:
            return model

        current_depth = len(layers)
        if current_depth <= 24: # è®¾å®šæœ€ä½æ™ºå•†é˜²çº¿
            print(f"âš ï¸ [è­¦å‘Š] å¤§è„‘å·²è¾¾æœ€å°åšåº¦ ({current_depth}å±‚)ï¼Œåœæ­¢åˆ‡é™¤ã€‚")
            return model

        # ç§»é™¤æœ€åä¸€å±‚
        del layers[-1]
        
        # æ›´æ–°é…ç½®
        if hasattr(model.config, "num_hidden_layers"): model.config.num_hidden_layers -= 1
        
        # é‡ç½®ç´¢å¼•
        AdvancedPhysicalSurgeon._reindex_layers(layers)
        
        print(f"âœ… [èç¼©å®Œæˆ] æ·±åº¦: {current_depth} -> {len(layers)}ã€‚")
        return model

    @staticmethod
    def _reindex_layers(layers):
        """ä¿®å¤ Qwen/Llama çš„ RoPE å±‚ç´¢å¼•"""
        for i, layer in enumerate(layers):
            if hasattr(layer, "self_attn") and hasattr(layer.self_attn, "layer_idx"):
                layer.self_attn.layer_idx = i


    # ===========================
    # é˜¶æ®µäºŒï¼šç»´åº¦é£å‡ (é’ˆå¯¹ MoE)
    # ===========================
    @staticmethod
    def evolve_via_mergekit(base_model_path, output_path, num_experts=2): # <--- ä¿®æ”¹é»˜è®¤å€¼ä¸º 2
        print(f"âš—ï¸ [ç‚¼ä¸¹] å¯åŠ¨ Mergekit åè®® (Target Experts={num_experts})...")
        
        if os.path.exists(output_path): shutil.rmtree(output_path)
        os.makedirs(Config.MOE_WORK_DIR, exist_ok=True)

        # [ä¿®æ”¹] å®šä¹‰ 2 ä¸ªä¸“å®¶çš„æ€§æ ¼ (åŒå­æ˜Ÿæ¶æ„)
        # æ—¢ç„¶åªæœ‰2ä¸ªï¼Œæˆ‘ä»¬æŠŠèƒ½åŠ›èšåˆä¸€ä¸‹
        specializations = [
            # ä¸“å®¶1: ç†æ€§è„‘ (è´Ÿè´£é€šç”¨å¯¹è¯ã€é€»è¾‘ã€ä»£ç ã€æ•°å­¦)
            {"name": "Logic_Core", "prompts": [
                "chat", "assistant", "logic", "common sense", 
                "python", "code", "programming", "math", "solve",
                "reasoning", "think", "analysis"
            ]},
            # ä¸“å®¶2: æ„Ÿæ€§è„‘ (è´Ÿè´£æ–‡å­¦ã€è§’è‰²æ‰®æ¼”ã€åˆ›æ„)
            {"name": "Creative_Soul","prompts": [
                "story", "write", "novel", "roleplay", "poem", 
                "emotional", "feeling", "art"
            ]}
        ]
        
        # ç¡®ä¿é…ç½®åˆ—è¡¨é•¿åº¦åŒ¹é… num_experts
        experts_config = []
        for i in range(num_experts):
            # å¾ªç¯å–ç”¨ specializations
            spec = specializations[i % len(specializations)]
            experts_config.append({
                "source_model": base_model_path,
                "positive_prompts": spec["prompts"]
            })
            print(f"      [é…æ–¹] Expert {i+1}: {spec['name']}")

        # ... (åç»­ä»£ç ä¿æŒä¸å˜)

        # æ„é€  Mergekit é…ç½®æ–‡ä»¶
        config_dict = {
            "base_model": base_model_path,
            "gate_mode": "hidden",
            "dtype": "float16", 
            "experts": experts_config
        }

        yaml_path = os.path.join(Config.MOE_WORK_DIR, "moe_recipe.yaml")
        with open(yaml_path, "w", encoding="utf-8") as f:
            yaml.dump(config_dict, f)
            
        print(f"   -> ğŸ“œ é…æ–¹å·²ç”Ÿæˆ: {yaml_path}")

        # è°ƒç”¨ Mergekit å‘½ä»¤è¡Œ
        cmd = [
            "mergekit-moe", yaml_path, output_path,
            "--allow-crimes", "--copy-tokenizer", "--device", "cuda"
        ]
        
        print("   -> ğŸ”¥ ç‚‰ç«å·²å¼€ (æ˜¾å­˜å°†è¢«å¾ç”¨ï¼Œè¯·è€å¿ƒç­‰å¾…)...")
        try:
            # å®æ—¶æ‰“å°æ—¥å¿—
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
            for line in process.stdout:
                if "Loading" in line or "Expert" in line or "Saving" in line:
                    print(f"      [Mergekit] {line.strip()}")
            process.wait()
            
            if process.returncode != 0:
                print(f"âŒ [ç‚¼ä¸¹ç‚¸ç‚‰] Mergekit é€€å‡ºä»£ç : {process.returncode}")
                return False
                
            print(f"âœ… [ç‚¼ä¸¹æˆåŠŸ] MoE æ¨¡å‹ç”Ÿæˆå®Œæ¯•: {output_path}")
            return True
        except Exception as e:
            print(f"âŒ æ‰§è¡Œå¼‚å¸¸: {e}")
            return False


class AcademicFeeder:
    """
    [V8.1 å¢å¼ºç‰ˆ] å­¦æœ¯æŠ•å–‚ç³»ç»Ÿ
    ç‰¹æ€§ï¼šæ”¯æŒ PDF, Word, TXT, Python æºç å…¨æ ¼å¼é˜…è¯»
    ä¿®å¤ç‚¹ï¼šåŠ å…¥äº† Optimizer å’Œ EWC Loss æ•´åˆï¼Œç¡®ä¿çœŸæ­£æ›´æ–°å‚æ•°
    """
    def __init__(self, model, tokenizer, ewc_handler=None):
        self.model = model
        self.tokenizer = tokenizer
        self.ewc = ewc_handler
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨ï¼Œä»…ä¼˜åŒ– requires_grad çš„å‚æ•° (é€šå¸¸æ˜¯ LoRA)
        self.optimizer = torch.optim.AdamW(
            filter(lambda p: p.requires_grad, self.model.parameters()), 
            lr=1e-4
        )

    # ==========================================
    # å¤–éƒ¨è´Ÿç†µæ•è·ç³»ç»Ÿ (Hunting System)
    # ==========================================

    def _fetch_arxiv(self, query="LLM optimization", max_results=3):
        """[ç†è®ºæ•æ‰‹] ä» arXiv æŠ“å–æœ€æ–°è®ºæ–‡æ‘˜è¦"""
        print(f"ğŸ”­ [ç†µå‡] æ­£åœ¨æ‰«æ arXiv å‰æ²¿ç†è®º: '{query}'...")
        try:
            url = f"http://export.arxiv.org/api/query?search_query=all:{quote_plus(query)}&start=0&max_results={max_results}&sortBy=submittedDate&sortOrder=desc"
            response = requests.get(url, timeout=20)
            if response.status_code != 200: return ""

            root = ET.fromstring(response.content)
            ns = {'atom': 'http://www.w3.org/2005/Atom'}
            content_buffer = ""
            
            for entry in root.findall('atom:entry', ns):
                title = entry.find('atom:title', ns).text.strip().replace('\n', ' ')
                summary = entry.find('atom:summary', ns).text.strip().replace('\n', ' ')
                published = entry.find('atom:published', ns).text[:10]
                link = entry.find('atom:id', ns).text
                content_buffer += f"Title: {title}\nDate: {published}\nLink: {link}\nAbstract: {summary}\n\n{'-'*40}\n\n"
            
            print(f"   -> âœ… [arXiv] æ•è·äº† {len(root.findall('atom:entry', ns))} ç¯‡é«˜èƒ½è®ºæ–‡ã€‚")
            return content_buffer
        except Exception as e:
            print(f"   -> âŒ arXiv æ•è·å¼‚å¸¸: {e}")
            return ""

    def _fetch_pubmed(self, query="neuroscience", max_results=3):
        """[ç”Ÿç‰©é›·è¾¾] ä» PubMed æŠ“å–ç”Ÿç‰©åŒ»å­¦/è„‘ç§‘å­¦å‰æ²¿"""
        print(f"ğŸ§¬ [ç†µå‡] æ­£åœ¨è§£æç”Ÿå‘½ç§‘å­¦æ•°æ®åº“ (PubMed): '{query}'...")
        try:
            # 1. ESearch
            search_url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term={quote_plus(query)}&retmode=json&retmax={max_results}"
            search_resp = requests.get(search_url, timeout=10)
            if search_resp.status_code != 200: return ""
            id_list = search_resp.json().get('esearchresult', {}).get('idlist', [])
            if not id_list: return ""

            # 2. EFetch
            ids_str = ",".join(id_list)
            fetch_url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id={ids_str}&retmode=xml"
            fetch_resp = requests.get(fetch_url, timeout=20)
            if fetch_resp.status_code != 200: return ""

            # 3. Parse
            root = ET.fromstring(fetch_resp.content)
            content_buffer = ""
            count = 0
            for article in root.findall('.//PubmedArticle'):
                try:
                    title = article.find('.//ArticleTitle').text
                    abstract_node = article.find('.//Abstract')
                    abstract = " ".join([t.text for t in abstract_node.findall('AbstractText') if t.text]) if abstract_node is not None else "No Abstract"
                    content_buffer += f"Title: {title}\nSource: PubMed\nAbstract: {abstract}\n\n{'-'*40}\n\n"
                    count += 1
                except: continue
            
            print(f"   -> âœ… [PubMed] æå–äº† {count} ç¯‡ç”Ÿç‰©å­¦æ–‡çŒ®ã€‚")
            return content_buffer
        except Exception as e:
            print(f"   -> âŒ PubMed æ‰«æå¼‚å¸¸: {e}")
            return ""

    def _fetch_github_trending(self, language="python"):
        """[ä»£ç æ‹¾è’] æŠ“å– GitHub è¿‘æœŸé«˜æ˜Ÿé¡¹ç›®"""
        print(f"ğŸ‘¾ [ç†µå‡] æ­£åœ¨æŒ–æ˜ GitHub {language} çƒ­é—¨é¡¹ç›®...")
        try:
            date_str = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
            url = f"https://api.github.com/search/repositories?q=created:>{date_str}+language:{language}&sort=stars&order=desc&per_page=3"
            headers = {"Accept": "application/vnd.github.v3+json", "User-Agent": "Nezha-Evolver"}
            
            response = requests.get(url, headers=headers, timeout=20)
            if response.status_code != 200:
                print(f"   -> âš ï¸ GitHub API é™åˆ¶/é”™è¯¯: {response.status_code}")
                return ""
                
            items = response.json().get('items', [])
            content_buffer = ""
            
            for item in items:
                repo_name = item['full_name']
                desc = item.get('description', 'No description')
                # å°è¯•è·å– README
                for branch in ['master', 'main']:
                    raw_url = f"https://raw.githubusercontent.com/{repo_name}/{branch}/README.md"
                    raw_resp = requests.get(raw_url, timeout=5)
                    if raw_resp.status_code == 200:
                        content_buffer += f"Repo: {repo_name}\nDesc: {desc}\nREADME:\n{raw_resp.text[:2000]}\n\n{'-'*40}\n\n"
                        break
            
            print(f"   -> âœ… [GitHub] æå–äº† {len(items)} ä¸ªçƒ­é—¨å·¥ç¨‹é€»è¾‘ã€‚")
            return content_buffer
        except Exception as e:
            print(f"   -> âŒ GitHub æŒ–æ˜å¼‚å¸¸: {e}")
            return ""

    def forage_external_entropy(self):
        """[ä¸»åŠ¨ç‹©çŒ V2.1] å¤šæºç†µå‡ï¼šç‰©ç†(arXiv) + ç”Ÿç‰©(PubMed) + å·¥ç¨‹(GitHub)"""
        
        # ç›®æ ‡å®šä¹‰
        arxiv_topics = ["Large Language Model", "Chain of Thought", "Information Theory", "Quantum Computing", "Complexity Theory"]
        pubmed_topics = ["Synaptic Plasticity", "Hippocampus Memory", "Prefrontal Cortex Decision Making", "Dopamine Reinforcement Learning"]
        
        theory_content = ""
        source_tag = ""

        # === 1. æ·éª°å­å†³å®šç†è®ºæ¥æº ===
        if random.random() < 0.5:
            topic = random.choice(arxiv_topics)
            theory_content = self._fetch_arxiv(query=topic, max_results=3)
            source_tag = f"arXiv ({topic})"
        else:
            topic = random.choice(pubmed_topics)
            theory_content = self._fetch_pubmed(query=topic, max_results=3)
            source_tag = f"PubMed ({topic})"
        
        # === 2. æ‹¾è’ GitHub (è·å–å·¥ç¨‹è½åœ°èƒ½åŠ›) ===
        github_data = self._fetch_github_trending(language="python")
        
        # å¦‚æœä¸¤æ‰‹ç©ºç©ºï¼Œè¿”å› None
        if not theory_content and not github_data:
            return None
            
        # === 3. æ‰“åŒ…é«˜ç†µè¥å…»å— ===
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # [ä¼˜åŒ–] å»ºç«‹æŒ‰æœˆå½’æ¡£ï¼Œé˜²æ­¢æ ¹ç›®å½•çˆ†ç‚¸
        month_str = datetime.now().strftime("%Y-%m")
        save_dir = os.path.join(Config.KNOWLEDGE_BASE, month_str)
        os.makedirs(save_dir, exist_ok=True)

        knowledge_content = (
            f"# [Entropy Injection] Generated at {timestamp}\n"
            f"# Source: {source_tag} & GitHub Trending\n\n"
            f"=== Theoretical Frontier ({source_tag}) ===\n{theory_content}\n\n"
            f"=== Engineering Practice (GitHub) ===\n{github_data}\n"
        )
        
        filename = f"entropy_boost_{timestamp}.txt"
        filepath = os.path.join(save_dir, filename)
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(knowledge_content)
            print(f"ğŸ“¦ [è¡¥ç»™] ç‹©çŒå®Œæˆï¼Œæˆ˜åˆ©å“å·²å…¥åº“: {month_str}/{filename}")
        except Exception as e:
            print(f"âš ï¸ æ–‡ä»¶å†™å…¥å¤±è´¥: {e}")
        
        # [å…³é”®ä¿®æ”¹]ï¼šè¿”å›å…·ä½“çš„çŸ¥è¯†å†…å®¹ï¼Œä¾›æ½œæ„è¯†ç ”è®¨ä½¿ç”¨
        return knowledge_content


    def study(self, adaptive_lr=1e-4, ewc_lambda=0.4):
        """
        [ä¿®æ”¹ç‰ˆ] å¤œé—´å­¦ä¹ å‡½æ•°ï¼šæ”¯æŒ OOM é€’å½’é™çº§ + å­¦ä¹ è¦†ç›–ç‡æŠ¥å‘Š + æ—¥å¿—è®°å½•
        + [æ–°å¢] æ”¯æŒé€’å½’æ‰«æå­ç›®å½• (é€‚é…æŒ‰æœˆå½’æ¡£çš„çŸ¥è¯†åº“)
        """
        # åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼
        self.model.train() 
        
        # æ‰«æçŸ¥è¯†åº“ (æ”¯æŒæ›´å¤šæ–‡ä»¶æ ¼å¼)
        supported_ext = ('.txt', '.py', '.pdf', '.docx', '.md')
        
        # [å¥å£®æ€§] ç¡®ä¿ç›®å½•å­˜åœ¨
        if not os.path.exists(Config.KNOWLEDGE_BASE): os.makedirs(Config.KNOWLEDGE_BASE)
        
        # é€’å½’æ‰«æé€»è¾‘
        # åŸé€»è¾‘: files = [f for f in os.listdir(...) if ...] (åªæ‰«ä¸€å±‚)
        # æ–°é€»è¾‘: ä½¿ç”¨ os.walk éå†æ‰€æœ‰å­ç›®å½•
        files = []
        for root, dirs, filenames in os.walk(Config.KNOWLEDGE_BASE):
            for f in filenames:
                if f.lower().endswith(supported_ext):
                    # å­˜å‚¨å®Œæ•´è·¯å¾„ï¼Œè€Œä¸ä»…ä»…æ˜¯æ–‡ä»¶å
                    full_path = os.path.join(root, f)
                    files.append(full_path)
        
        if not files: return 0
        
        # éšæœºæŠ½å–ä¸€ä¸ªæ–‡ä»¶ (files åˆ—è¡¨ä¸­ç°åœ¨æ˜¯å®Œæ•´è·¯å¾„)
        path = random.choice(files)
        # ä»è·¯å¾„ä¸­æå–æ–‡ä»¶åç”¨äºæ˜¾ç¤º
        filename = os.path.basename(path)
        
        try:

            txt = UniversalFileInfo.read_content(path)

            # è·å–åŸå§‹æ–‡æœ¬é•¿åº¦
            total_len = len(txt)

            # å¦‚æœæå–å†…å®¹å¤ªå°‘ï¼ˆå¯èƒ½æ˜¯æ‰«æç‰ˆPDFæˆ–ç©ºæ–‡ä»¶ï¼‰ï¼Œåˆ™è·³è¿‡
            if total_len < 50:
                print(f"   -> âš ï¸ [è·³è¿‡] {filename} å†…å®¹è¿‡å°‘æˆ–æ— æ³•è¯†åˆ«ã€‚")
                return 0

            print(f"ğŸ“– [å¤œè¯»] ç›®æ ‡: {filename} | é•¿åº¦: {total_len} å­—ç¬¦")

            # è®¾å®šåˆå§‹æœ€å¤§ä¸Šä¸‹æ–‡çª—å£ (Tokenæ•°)
            # é’ˆå¯¹ 14B + 66GB æ˜¾å­˜ï¼Œæˆ‘ä»¬å°è¯•ä» 16k å¼€å§‹
            INITIAL_MAX_TOKENS = 16384 
            
            # [æ–°] ä¼°ç®—å…¨æ–‡éœ€è¦çš„ Token æ•° (ç²—ç•¥ä¼°ç®—ï¼š1 token â‰ˆ 3 char)
            # è¿™å†³å®šäº†åˆ†æ¯ï¼Œç”¨æ¥è®¡ç®—è¦†ç›–ç‡
            estimated_total_tokens = max(1, total_len / 3.0)

            # å­—ç¬¦é¢„ä¼°ä¸æˆªæ–­é€»è¾‘
            CHAR_LIMIT = INITIAL_MAX_TOKENS * 4
            target_txt = txt # é»˜è®¤å­¦å…¨æ–‡
            is_cut = False   # æ ‡è®°æ˜¯å¦å‘ç”Ÿäº†ç‰©ç†æˆªæ–­

            # å¦‚æœæ–‡æœ¬è¶…é•¿ï¼ŒéšæœºæŠ½å–ä¸€ä¸ªâ€œå¤§ç‰‡æ®µâ€
            if total_len > CHAR_LIMIT: 
                is_cut = True
                # éšæœºé€‰ä¸€ä¸ªèµ·å§‹ç‚¹ï¼Œæ¨¡ä»¿â€œç¿»åˆ°è¿™ä¸€ç« å¼€å§‹è¯»â€
                start = random.randint(0, total_len - CHAR_LIMIT)
                target_txt = txt[start : start + CHAR_LIMIT]
                print(f"   -> âœ‚ï¸ [ç‰©ç†æˆªæ–­] æ–‡æœ¬è¿‡é•¿ï¼Œéšæœºé€‰å–ç‰‡æ®µ ({len(target_txt)} å­—ç¬¦)")
            else:
                print(f"   -> ğŸ“œ [å…¨æ–‡æ¨¡å¼] å‡†å¤‡é€šè¯»å…¨æ–‡")

            # ================= å®šä¹‰é€’å½’è®­ç»ƒé—­åŒ…å‡½æ•° =================
            def _recursive_train(current_limit):
                """
                å†…éƒ¨é€’å½’å‡½æ•°ï¼šå°è¯•ä»¥ current_limit é•¿åº¦è®­ç»ƒã€‚
                å¦‚æœ OOMï¼Œåˆ™å‡åŠé‡è¯•ã€‚
                è¿”å›: (loss, actual_tokens)
                """
                # [ç»ˆæ­¢æ¡ä»¶] çª—å£å¤ªå°åˆ™æ”¾å¼ƒï¼Œé¿å…æ­»å¾ªç¯æˆ–æ— æ•ˆå­¦ä¹ 
                if current_limit < 1024:
                    print(f"   -> âŒ æ˜¾å­˜ä¸¥é‡ä¸è¶³ï¼Œå³ä½¿é™çº§åˆ° {current_limit} ä¹Ÿæ— æ³•è®­ç»ƒï¼Œæ”¾å¼ƒã€‚")
                    return 0, 0

                try:
                    # 3. Tokenization & è®­ç»ƒ
                    # truncation=True ä¼šè‡ªåŠ¨æŠŠå¤šä½™çš„åˆ‡æ‰ï¼Œä¿è¯ä¸è¶…è¿‡ max_length
                    # [å…³é”®] ä½¿ç”¨å½“å‰çš„é™åˆ¶ current_limit
                    inputs = self.tokenizer(
                        target_txt, 
                        return_tensors="pt", 
                        truncation=True, 
                        max_length=current_limit 
                    ).to(self.model.device)
                    
                    # [æ–°] è·å–å®é™…å–‚è¿›å»çš„ Token æ•°é‡ï¼Œç”¨äºè®¡ç®—è¦†ç›–ç‡
                    actual_tokens = inputs.input_ids.shape[1]
                    
                    # æ˜¾å­˜é˜²çˆ†æ£€æŸ¥ï¼šå¦‚æœ input å¤ªé•¿å¯¼è‡´ OOMï¼Œè¿™é‡Œ catch ä½
                    self.optimizer.zero_grad()
                    outputs = self.model(**inputs, labels=inputs.input_ids)
                    loss = outputs.loss
                    
                    # EWC æ­£åˆ™åŒ–
                    if self.ewc and self.ewc.fisher:
                        ewc_loss = 0
                        for n, p in self.model.named_parameters():
                            if n in self.ewc.fisher:
                                # [ä¼˜åŒ–] ç¡®ä¿ device ä¸€è‡´ï¼Œé˜²æ­¢æŠ¥é”™
                                fisher_val = self.ewc.fisher[n].to(p.device)
                                opt_val = self.ewc.opt_params[n].to(p.device)
                                ewc_loss += (fisher_val * (p - opt_val) ** 2).sum()
                        loss += ewc_lambda * ewc_loss

                    loss.backward()
                    
                    for param_group in self.optimizer.param_groups:
                        param_group['lr'] = adaptive_lr
                        
                    self.optimizer.step()

                    # [å…³é”®ä¼˜åŒ–] æ˜¾å¼åˆ é™¤æ¢¯åº¦ï¼Œé˜²æ­¢ç´¯ç§¯å ç”¨æ˜¾å­˜ (set_to_none=True æ›´å½»åº•)
                    self.optimizer.zero_grad(set_to_none=True)

                    # å¼ºåˆ¶é‡Šæ”¾æ˜¾å­˜
                    del outputs, inputs
                    torch.cuda.empty_cache()
                    
                    # è¿”å› loss å’Œ å®é™…è®­ç»ƒçš„ token æ•°
                    return loss.item(), actual_tokens
                    
                except RuntimeError as e:
                    # æ˜¾å­˜ä¸è¶³å¤„ç†
                    if "out of memory" in str(e).lower():
                        # è®¡ç®—æ–°çš„ token é™åˆ¶ (å‡åŠ)
                        new_limit = current_limit // 2
                        print(f"   -> âš ï¸ æ˜¾å­˜ä¸è¶³ (OOM) @ {current_limit} Tokensã€‚")
                        print(f"      ğŸ”„ è§¦å‘è‡ªåŠ¨é€‚åº”æœºåˆ¶ï¼šå°è¯•é™çº§è‡³ {new_limit} Tokens...")
                        
                        # å¿…é¡»å½»åº•æ¸…ç†æ˜¾å­˜ï¼Œå¦åˆ™ä¸‹æ¬¡å°è¯•å¿…æŒ‚
                        torch.cuda.empty_cache() 
                        self.optimizer.zero_grad(set_to_none=True)
                        
                        # [é€’å½’è°ƒç”¨] å¸¦ç€æ›´å°çš„é™åˆ¶é‡è¯•
                        return _recursive_train(new_limit)
                    else:
                        raise e # å…¶ä»–é”™è¯¯æ­£å¸¸æŠ›å‡º

            # ================= [æ–°] å¯åŠ¨é€’å½’é€»è¾‘ä¸æŠ¥å‘Šç”Ÿæˆ =================
            
            # ä»æœ€å¤§çš„ 16k å¼€å§‹å°è¯•
            loss_val, used_tokens = _recursive_train(INITIAL_MAX_TOKENS)

            if loss_val > 0:
                # è®¡ç®—è¦†ç›–ç‡ = (æœ¬æ¬¡è®­ç»ƒå®é™…åƒæ‰çš„Token / æ–‡ä»¶æ€»ä¼°ç®—Token) * 100
                coverage = (used_tokens / estimated_total_tokens) * 100
                
                # å¦‚æœå‘ç”Ÿäº†ç‰©ç†æˆªæ–­ï¼Œè¦†ç›–ç‡æ°¸è¿œä¸å¯èƒ½åˆ° 100%ï¼Œè¿™æ˜¯ç¬¦åˆé¢„æœŸçš„
                
                # ç”ŸæˆçŠ¶æ€å›¾æ ‡
                status_icon = "ğŸŸ¢" if coverage > 90 else ("ğŸŸ¡" if coverage > 30 else "ğŸ”´")
                
                # æ‰“å°æ§åˆ¶å°æŠ¥å‘Š
                print(f"   -> âœ… [å­¦ä¹ å®Œæˆ] {status_icon} è¦†ç›–ç‡: {coverage:.1f}% | æ¶ˆè€— Token: {used_tokens}")

                # [æ–°] æ„é€ è¯¦ç»†æ—¥å¿—å¹¶è½ç›˜
                log_entry = (
                    f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | "
                    f"File: {filename} | "
                    f"Status: {status_icon} (Cov: {coverage:.1f}%) | "
                    f"Tokens: {used_tokens}/{int(estimated_total_tokens)} | "
                    f"Loss: {loss_val:.4f}\n"
                )
                
                # å†™å…¥æ—¥å¿—æ–‡ä»¶
                try:
                    with open(os.path.join(Config.EVOLUTION_DIR, "learning_history.log"), "a", encoding="utf-8") as f:
                        f.write(log_entry)
                except Exception as ex:
                    print(f"   -> âš ï¸ æ—¥å¿—å†™å…¥å¤±è´¥: {ex}")

            return loss_val

        except Exception as e: 
            print(f"âš ï¸ Study Err: {e}")
            return 0


    def decide_strategy(self, state, dpo_data_count):
        """
        [Math Refactor] åŸºäºç†µä¸ Utility æƒè¡¡çš„è¿›åŒ–ç­–ç•¥
        é€»è¾‘:
        1. å¦‚æœæ²¡æœ‰ DPO æ•°æ®ï¼Œåªèƒ½ SFTã€‚
        2. å¦‚æœ DPO æ•°æ®ç§¯ç´¯è¿‡å¤š (>10)ï¼Œå¼ºåˆ¶ DPO æ¶ˆåŒ–ã€‚
        3. å…¶ä»–æƒ…å†µï¼Œæ ¹æ®èƒ½é‡å’Œéšæœºæ¦‚ç‡å†³å®šã€‚
        """
        # 1. ç¡¬æ€§é—¨æ§›: DPO æ•°æ®å¿…é¡»è¶³å¤Ÿ (è‡³å°‘ 2 æ¡æ‰èƒ½ç»„æˆ batch)
        if dpo_data_count < 2:
            return "SFT", f"DPOæ•°æ®ä¸è¶³ ({dpo_data_count} < 2)"

        # 2. å †ç§¯æ¸…ç†: æ•°æ®å¤ªå¤šäº†ï¼Œå¿…é¡»è®­ç»ƒ
        if dpo_data_count > 20:
            return "DPO", f"DPOæ•°æ®å †ç§¯ ({dpo_data_count}æ¡)"

        # 3. [Math Fix] åŸºäºç†µçš„åŠ¨æ€å†³ç­–
        # å¦‚æœç†µ (state['entropy']) å¾ˆé«˜ï¼Œè¯´æ˜å¤„äºæœªçŸ¥é¢†åŸŸï¼Œåº”è¯¥ SFT (Exploration/Learning)
        # å¦‚æœç†µå¾ˆä½ï¼Œè¯´æ˜å·²ç»å¾ˆç†Ÿæ‚‰äº†ï¼Œå¯ä»¥ DPO (Exploitation/Refinement)
        
        normalized_entropy = state.get('entropy', 0.5)
        
        # åŠ¨æ€é˜ˆå€¼: èƒ½é‡è¶Šé«˜ï¼Œè¶Šå®¹å¿é«˜ç†µå»æ¢ç´¢
        exploration_threshold = 0.3 + (state.get('atp', 0) / 200.0)
        
        if normalized_entropy > exploration_threshold:
            return "SFT", f"é«˜ç†µçŠ¶æ€ ({normalized_entropy:.2f}) -> éœ€è¦è¡¥å……çŸ¥è¯†"
        else:
            return "DPO", f"ä½ç†µçŠ¶æ€ ({normalized_entropy:.2f}) -> é€‚åˆå¼ºåŒ–ä»·å€¼è§‚"


# ==============================================================================
# [Core V3.0] å¥‡ç‚¹å†…æ ¸å‡çº§ç‰ˆ: é²æ£’ç¥ç»åŠ¨åŠ›å­¦ä¸é—­ç¯æ¨ç†
# ==============================================================================

class NeuralWorldModel(nn.Module):
    """
    [æ•°å­¦æ ¸å¿ƒå‡çº§ V3.0] é²æ£’ç¥ç»ä¸–ç•Œæ¨¡å‹ (Robust Neural World Model)
    -----------------------------------------------------------
    æ¶æ„: æ®‹å·®å­¦ä¹  (Residual Learning) + è½¯é¥±å’Œæ§åˆ¶ (Soft Saturation)
    å…¬å¼: S_{t+1} = Sigmoid( S_t + MLP(S_t, a_t) )
    
    ç‰¹æ€§: 
      1. æ®‹å·®é¢„æµ‹: ç¥ç»ç½‘ç»œåªå­¦ä¹ å˜åŒ–é‡ Deltaï¼Œè€Œéç»å¯¹å€¼ï¼Œè§£å†³æ’ç­‰æ˜ å°„éš¾é¢˜ã€‚
      2. é›¶åˆå§‹åŒ–: åˆå§‹çŠ¶æ€ä¸‹è¾“å‡ºä¸º 0ï¼Œæ¨¡æ‹Ÿç‰©ç†ç³»ç»Ÿçš„æƒ¯æ€§ã€‚
      3. è½¯é¥±å’Œ: ä½¿ç”¨ Sigmoid åœ¨è¾¹ç•Œå¤„äº§ç”Ÿé˜»å°¼ï¼Œç¬¦åˆç”Ÿç‰©ç¥ç»å…ƒçš„æ¿€æ´»ç‰¹æ€§ã€‚
    """
    def __init__(self, action_space, hidden_dim=32, max_memory=1000):
        super().__init__()
        self.action_space = action_space
        self.n_actions = len(action_space)
        self.action_map = {act: i for i, act in enumerate(action_space)}
        
        # [ç‰©ç†å¸¸æ•°] ç”¨äºå½’ä¸€åŒ–çš„åŸºå‡†å€¼
        self.max_atp = 100.0
        self.max_ent = 5.0
        
        self.in_dim = 2 + self.n_actions
        self.out_dim = 2 
        
        # [ç½‘ç»œç»“æ„] 3å±‚ MLP + LayerNorm
        self.net = nn.Sequential(
            nn.Linear(self.in_dim, hidden_dim),
            nn.LayerNorm(hidden_dim), 
            nn.SiLU(), # SiLU (Swish) æ¿€æ´»å‡½æ•°
            nn.Linear(hidden_dim, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, self.out_dim)
        )
        
        # [é›¶åˆå§‹åŒ–ç­–ç•¥ / Zero Initialization]
        # å°†æœ€åä¸€å±‚çš„æƒé‡åˆå§‹åŒ–ä¸ºæå°å€¼ (Uniform[-0.001, 0.001])ï¼Œåç½®ä¸º 0ã€‚
        # æ„ä¹‰ï¼šåœ¨è®­ç»ƒåˆæœŸï¼Œæ¨¡å‹è¾“å‡ºçš„ Delta æ¥è¿‘ 0ã€‚
        # è¿™æ„å‘³ç€ï¼šå¦‚æœå“ªå’ä»€ä¹ˆéƒ½ä¸å­¦ï¼Œå®ƒé»˜è®¤è®¤ä¸º"ä¸‹ä¸€ç§’çš„ä¸–ç•Œå’Œè¿™ä¸€ç§’ä¸€æ ·" (S_{t+1} = S_t)ã€‚
        # è¿™æ˜¯ç‰©ç†ç³»ç»Ÿæœ€ç¨³å¥çš„å…ˆéªŒå‡è®¾ï¼ˆæƒ¯æ€§å®šå¾‹ï¼‰ã€‚
        nn.init.uniform_(self.net[-1].weight, -0.001, 0.001)
        nn.init.constant_(self.net[-1].bias, 0)
        
        # æ­£äº¤åˆå§‹åŒ–å…¶ä»–å±‚
        for m in self.net[:-1].modules():
            if isinstance(m, nn.Linear):
                nn.init.orthogonal_(m.weight, gain=0.1)

        self.lr = 0.005
        self.optimizer = torch.optim.AdamW(self.net.parameters(), lr=self.lr, weight_decay=1e-4)
        
        self.replay_buffer = deque(maxlen=max_memory)

    def _normalize(self, atp, ent):
        """å½’ä¸€åŒ–åˆ° [0, 1]"""
        return atp / self.max_atp, ent / self.max_ent

    def _denormalize(self, atp_norm, ent_norm):
        """è¿˜åŸç‰©ç†é‡"""
        atp = atp_norm * self.max_atp
        ent = ent_norm * self.max_ent
        return atp, max(0.0, ent)

    def predict(self, current_state, action_name):
        """
        [æ¨ç†æ¨¡å¼ - æ®‹å·®ç‰ˆ] é¢„æµ‹çŠ¶æ€å˜åŒ–é‡ï¼Œè€Œéç»å¯¹å€¼
        å…¬å¼: S_{t+1} = S_t + Model(S_t, a_t)
        """
        self.eval() 
        device = self.net[0].weight.device

        with torch.no_grad():
            # 1. å‡†å¤‡è¾“å…¥æ•°æ®
            atp, ent = current_state.get('atp', 100), current_state.get('entropy', 0.5)
            atp_n, ent_n = self._normalize(atp, ent) # è·å–å½“å‰å½’ä¸€åŒ–çŠ¶æ€
            
            # æ„é€ åŠ¨ä½œ One-Hot å‘é‡
            act_idx = self.action_map.get(action_name, 0)
            act_vec = torch.zeros(self.n_actions)
            act_vec[act_idx] = 1.0
            
            # æ‹¼æ¥: [ATP_n, Ent_n, Action_OneHot...]
            x = torch.cat([torch.tensor([atp_n, ent_n]), act_vec]).to(device)
            
            # 2. å‰å‘ä¼ æ’­ -> è·å– Delta (å˜åŒ–é‡)
            # æ­¤æ—¶ç½‘ç»œè¾“å‡ºçš„æ˜¯: åŠ¨ä½œä¼šè®©çŠ¶æ€å¢åŠ /å‡å°‘å¤šå°‘?
            delta = self.net(x)
            delta_atp, delta_ent = delta[0].item(), delta[1].item()
            
            # 3. [æ®‹å·®è¿æ¥ / Residual Connection]
            # ä¸‹ä¸€æ­¥çŠ¶æ€ = å½“å‰çŠ¶æ€ + é¢„æµ‹çš„å˜åŒ–é‡
            raw_next_atp = atp_n + delta_atp
            raw_next_ent = ent_n + delta_ent
            
            # 4. [ä¿®æ­£: ç‰©ç†æˆªæ–­ / Hard Clamp]
            # æ”¾å¼ƒ Sigmoidï¼Œä½¿ç”¨ç¡¬æˆªæ–­é™åˆ¶ç‰©ç†è¾¹ç•Œã€‚
            # ç†ç”±ï¼šè®­ç»ƒç›®æ ‡æ˜¯çº¿æ€§çš„ MSEï¼Œæ¨ç†å¿…é¡»ä¿æŒçº¿æ€§ä¸€è‡´æ€§ï¼Œé˜²æ­¢ç©ºé—´ç•¸å˜ã€‚
            # è¿™ä¿è¯äº†ç½‘ç»œå­¦åˆ°çš„ Delta æ˜¯çœŸå®çš„ç‰©ç†ä½ç§»ï¼Œä»…åœ¨è§¦ç¢°è¾¹ç•Œæ—¶åœæ­¢ã€‚
            next_atp_n = max(0.0, min(1.0, raw_next_atp))
            next_ent_n = max(0.0, min(1.0, raw_next_ent))
            
            # 5. è¿˜åŸç‰©ç†é‡
            next_atp, next_ent = self._denormalize(next_atp_n, next_ent_n)
            
            pred = current_state.copy()
            pred['atp'] = next_atp
            pred['entropy'] = next_ent
            return pred

    def buffer_experience(self, prev, act, next_s):
        """å­˜å‚¨ç»éªŒ"""
        self.replay_buffer.append((prev, act, next_s))

    def train_batch(self, batch_size=64):
        """
        [å¤œé—´è®­ç»ƒ - å‘é‡åŒ–æé€Ÿç‰ˆ] 
        Target = (Next_State - Current_State)
        ä½¿ç”¨ PyTorch å‘é‡æ“ä½œæ›¿ä»£ Python å¾ªç¯ï¼Œå¤§å¹…æå‡è®­ç»ƒååé‡ã€‚
        """
        # 1. åŸºç¡€æ£€æŸ¥
        if len(self.replay_buffer) < 10: return 0.0, {}
        
        self.train() 
        device = self.net[0].weight.device
        
        # 2. é‡‡æ ·
        sample_size = min(len(self.replay_buffer), batch_size)
        batch = random.sample(self.replay_buffer, sample_size)
        
        # --- [æ‰¹é‡è§£åŒ…ä¸å‘é‡åŒ–] ---
        
        # A. å¿«é€Ÿè§£åŒ…
        # batch ç»“æ„: [(prev_dict, act_str, next_dict), ...]
        prev_states = [x[0] for x in batch]
        actions_raw = [x[1] for x in batch]
        next_states = [x[2] for x in batch]

        # B. çŠ¶æ€å‘é‡åŒ– (State Tensor) [B, 2]
        # self._normalize è¿”å› (atp_norm, ent_norm) å…ƒç»„ï¼Œåˆ—è¡¨æ¨å¯¼å¼ç”Ÿæˆ [(a1,e1), (a2,e2)...]
        # torch.tensor ç›´æ¥å°†å…¶è½¬æ¢ä¸ºäºŒç»´å¼ é‡ï¼Œæå…¶é«˜æ•ˆ
        states_tensor = torch.tensor([
            self._normalize(s['atp'], s.get('entropy', 0.5)) 
            for s in prev_states
        ], dtype=torch.float32).to(device)

        # C. åŠ¨ä½œå‘é‡åŒ– (Action One-Hot) [B, N_Act]
        # 1. è½¬ç´¢å¼•
        act_indices = [self.action_map.get(a, 0) for a in actions_raw]
        # 2. è½¬ LongTensor
        act_indices_tensor = torch.tensor(act_indices, dtype=torch.long).to(device)
        # 3. ç‹¬çƒ­ç¼–ç  (æ³¨æ„è¦è½¬ float ä»¥é€‚é…ç¥ç»ç½‘ç»œè¾“å…¥)
        # æ›´ç¨³å¥çš„å†™æ³•ï¼šè·Ÿéšç½‘ç»œçš„ç¬¬ä¸€å±‚æƒé‡ç±»å‹
        dtype = self.net[0].weight.dtype 
        actions_tensor = F.one_hot(act_indices_tensor, num_classes=self.n_actions).to(dtype)

        # D. ç›®æ ‡æ®‹å·®è®¡ç®— (Target Delta) [B, 2]
        # 1. ä¸‹ä¸€çŠ¶æ€å‘é‡åŒ–
        next_vals = torch.tensor([
            self._normalize(s['atp'], s['entropy']) 
            for s in next_states
        ], dtype=torch.float32).to(device)
        
        # 2. [ç‰©ç†å«ä¹‰] Target = Real_Next - Current
        # åˆ©ç”¨ GPU çŸ©é˜µå‡æ³•ç¬é—´ç®—å‡ºæ‰€æœ‰æ ·æœ¬çš„æ®‹å·®ç›®æ ‡
        targets_delta = next_vals - states_tensor

        # 3. æ‹¼æ¥è¾“å…¥ X [B, 2 + N_Act]
        X = torch.cat([states_tensor, actions_tensor], dim=1)
        
        # 4. å‰å‘ä¼ æ’­ (é¢„æµ‹æ®‹å·®)
        preds_delta = self.net(X)
        
        # 5. è®¡ç®— Loss (MSE)
        # reduction='none' ä¿ç•™æ¯ä¸ªæ ·æœ¬çš„ Loss ç”¨äºç»Ÿè®¡
        loss_per_sample = F.mse_loss(preds_delta, targets_delta, reduction='none').sum(dim=1)
        loss = loss_per_sample.mean()
        
        # 6. åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.net.parameters(), 1.0)
        self.optimizer.step()
        
        # 7. ç»Ÿè®¡è¯¯å·® (ç”¨äºæ—¥å¿—)
        # è¿™ä¸€æ­¥ç¨å¾®æ…¢ç‚¹æ²¡å…³ç³»ï¼Œå› ä¸ºåªæ¶‰åŠ CPU ç»Ÿè®¡
        action_errors = {}
        with torch.no_grad():
            losses_np = loss_per_sample.detach().cpu().numpy()
            for name, err in zip(actions_raw, losses_np):
                if name not in action_errors: action_errors[name] = []
                action_errors[name].append(err)
        
        avg_action_errors = {k: np.mean(v) for k, v in action_errors.items()}
        return loss.item(), avg_action_errors



class ActiveInferenceEngine:
    """
    [Math Refactor] ä¸¥è°¨ç‰ˆä¸»åŠ¨æ¨ç†å¼•æ“ (V2.0)
    éµå¾ª Friston (2015) å®šä¹‰ï¼šEFE = Risk + Ambiguity
    EFE â‰ˆ Expected Divergence (Utility) - Entropy (Information Gain)
    """
    def __init__(self, world_model, action_space, beta=2.0):
        self.world_model = world_model
        self.actions = action_space
        self.beta = beta  # æ¢ç´¢-åˆ©ç”¨å¹³è¡¡ç³»æ•° (Temperature of Curiosity)
        

    def decide(self, current_state, fallback_action):
        """
        è®¡ç®—æ¯ä¸ªåŠ¨ä½œçš„é¢„æœŸè‡ªç”±èƒ½ (EFE) å¹¶é€‰æ‹©æœ€å°è€…
        """
        best_act = fallback_action
        min_efe = float('inf')
        
        # ç›®æ ‡çŠ¶æ€ (å†…ç¨³æ€ Target)
        target_atp = 100.0 
        target_ent = 0.0  # æˆ‘ä»¬å¸Œæœ›ç³»ç»Ÿå†…éƒ¨æƒŠå¥‡åº¦ä½
        
        # éå†åŠ¨ä½œç©ºé—´ (åäº‹å®æ¨ç†)
        for act in self.actions:
            # A. é€šè¿‡ä¸–ç•Œæ¨¡å‹é¢„æµ‹æœªæ¥çŠ¶æ€
            # pred åŒ…å«: {'atp': ..., 'entropy': ...}
            pred = self.world_model.predict(current_state, act)
            
            # B. è®¡ç®—å®ç”¨ä»·å€¼ (Risk / Utility Cost) - è¶Šä½è¶Šå¥½
            # è¡¡é‡é¢„æµ‹çŠ¶æ€ä¸ç›®æ ‡çŠ¶æ€çš„ Kullback-Leibler æ•£åº¦è¿‘ä¼¼å€¼ (MSE)
            dist_atp = abs(target_atp - pred['atp']) / 100.0
            dist_ent_gap = abs(pred['entropy'] - target_ent) / 5.0
            
            # ç†”æ–­æƒ©ç½š
            if pred['atp'] < 5.0: 
                utility_cost = 100.0 
            else:
                utility_cost = (dist_atp ** 2) + (dist_ent_gap ** 2)
            
            # C. è®¡ç®—è®¤çŸ¥ä»·å€¼ (Epistemic Value) - è¶Šé«˜è¶Šå¥½
            # ä½¿ç”¨ä¸–ç•Œæ¨¡å‹é¢„æµ‹çš„ Entropy ä½œä¸ºä¿¡æ¯å¢ç›Šçš„è¿‘ä¼¼
            # EFE å…¬å¼: æˆ‘ä»¬å¸Œæœ› Utility Cost ä½ï¼ŒåŒæ—¶å¸Œæœ› Information Gain (Entropy) é«˜
            # EFE = Cost - beta * Entropy
            predicted_entropy = pred['entropy']
            
            # D. EFE å…¬å¼ (ä¸¥è°¨ç‰ˆ)
            efe = utility_cost - (self.beta * predicted_entropy)
            
            # Debug: æ‰“å°æ€ç»´é“¾ (å¯é€‰)
            # print(f"Action: {act} | Util: {utility_cost:.2f} | Ent: {predicted_entropy:.2f} | EFE: {efe:.2f}")

            if efe < min_efe:
                min_efe = efe
                best_act = act
                
        return best_act



class GlobalWorkspace:
    """
    [Thread-Safe] å…¨å±€å·¥ä½œç©ºé—´ (GWT) V2.0
    å®ç°â€œæ„è¯†å‰§åœºâ€æ¨¡å‹ã€‚å¤šä¸ªæ¨¡å—ç«äº‰è¿›å…¥æ„è¯†ã€‚
    [Fix] å¼•å…¥ RLock ç¡®ä¿åå°æ½œæ„è¯†çº¿ç¨‹ï¼ˆå¦‚æä»æ ¸ï¼‰ä¸ä¸»æ„è¯†çº¿ç¨‹äº’æ–¥è®¿é—®ç¼“å†²åŒºã€‚
    """
    def __init__(self):
        self.capacity = 4 # å·¥ä½œè®°å¿†å®¹é‡
        self.buffer = [] # [(Salience, Source, Content)]
        # RLock (å¯é‡å…¥é”) å…è®¸åŒä¸€çº¿ç¨‹å¤šæ¬¡è·å–é”ï¼Œé˜²æ­¢æ­»é”ï¼Œæ¯” Lock æ›´å®‰å…¨
        self.lock = threading.RLock() 

    def register_input(self, source, content, base_salience):
        """æ³¨å†Œæ„ŸçŸ¥ä¿¡å· (çº¿ç¨‹å®‰å…¨)"""
        if not content: return
        
        # æ³¨å…¥é«˜æ–¯ç¥ç»å™ªå£° (Stochastic Resonance)
        noise = random.gauss(0, 0.05)
        final_score = base_salience + noise
        
        # åŠ é”å†™å…¥ï¼Œé˜²æ­¢åœ¨ step() æ¸…ç†æ—¶æ­£å¥½æœ‰æ–°æ•°æ®æ’å…¥
        with self.lock:
            self.buffer.append((final_score, source, content))

    def step(self):
        """ç«äº‰ä¸å¹¿æ’­ (çº¿ç¨‹å®‰å…¨)"""
        with self.lock: # [å…³é”®ä¿®å¤] åŠ é”è¯»å–ä¸æ¸…ç†
            # 1. ç«äº‰æ’åº (Top-K)
            self.buffer.sort(key=lambda x: x[0], reverse=True)
            
            # 2. èµ¢å®¶é€šåƒ (Winner-Take-All)
            winners = self.buffer[:self.capacity]
            
            # 3. æ„å»ºæ„è¯†æµä¸Šä¸‹æ–‡
            context = ""
            for score, src, txt in winners:
                # é«˜äº®å¼ºä¿¡å·
                marker = "ğŸ”¥" if score > 0.8 else ""
                context += f"[{src}{marker}]: {txt}\n"
                
            # 4. ä¾§å‘æŠ‘åˆ¶ (Lateral Inhibition) - æ¸…ç©ºç¼“å†²åŒº
            # æ­¤æ—¶æŒæœ‰é”ï¼Œä¸ç”¨æ‹…å¿ƒè¯¯åˆ åˆšæ’å…¥çš„æ•°æ®
            self.buffer = []
            return context


class SynapticHomeostasis:
    """
    [Neuro Refactor] çªè§¦ç¨³æ€å‡è¯´ (SHY) - æ¢¯åº¦æ„ŸçŸ¥ç‰ˆ
    ä¸å†æ— è„‘è¡°å‡ï¼Œè€Œæ˜¯æ ¹æ®å‚æ•°çš„'é‡è¦æ€§'(æ¢¯åº¦å¹…åº¦)è¿›è¡Œé€‰æ‹©æ€§ä¿®å‰ªã€‚
    å…¬å¼: W_new = W_old - decay * (1 - Normalized_Importance) * W_old
    """    
    @staticmethod
    def scale_down(model, decay_rate=0.01):
        """
        :param decay_rate: åŸºç¡€è¡°å‡ç‡ (æœ€å¤§é—å¿˜é€Ÿåº¦)
        """
        # 1. æ”¶é›†é‡è¦æ€§ (æ¢¯åº¦ L2 èŒƒæ•°)
        importances = {}
        max_importance = 0.0
        
        # éå†å‚æ•°ï¼Œè®¡ç®—é‡è¦æ€§
        for name, param in model.named_parameters():
            if param.requires_grad and ("lora" in name or "adapter" in name):
                if param.grad is not None:
                    # ä½¿ç”¨æ¢¯åº¦çš„ç»å¯¹å€¼ä½œä¸ºé‡è¦æ€§è¿‘ä¼¼ (Synaptic Tagging)
                    imp = param.grad.abs().mean().item()
                    importances[name] = imp
                    if imp > max_importance: max_importance = imp
                else:
                    importances[name] = 0.0
        
        # é˜²æ­¢é™¤ä»¥é›¶
        if max_importance == 0: max_importance = 1.0

        # 2. æ‰§è¡ŒåŠ æƒè¡°å‡
        with torch.no_grad():
            for name, param in model.named_parameters():
                if name in importances:
                    # å½’ä¸€åŒ–é‡è¦æ€§ (0.0 ~ 1.0)
                    # 1.0 è¡¨ç¤ºéå¸¸é‡è¦ (Gradientå¤§)ï¼Œ0.0 è¡¨ç¤ºä¸é‡è¦
                    norm_imp = importances[name] / max_importance
                    
                    # ä¿æŠ¤å› å­: é‡è¦æ€§è¶Šé«˜ï¼Œä¿æŠ¤è¶Šå¼ºï¼Œè¡°å‡è¶Šå°‘
                    # æ¯”å¦‚ norm_imp=1.0 -> protection=1.0 -> effective_decay=0 (å®Œå…¨ä¸å¿˜)
                    # norm_imp=0.0 -> protection=0.0 -> effective_decay=decay_rate (å®Œå…¨é—å¿˜)
                    protection = math.sqrt(norm_imp) # ä½¿ç”¨ sqrt è®©ä¿æŠ¤æ›´çµæ•
                    
                    effective_decay = decay_rate * (1.0 - protection)
                    
                    # æ‰§è¡Œè¡°å‡: W = W * (1 - decay)
                    param.mul_(1.0 - effective_decay)
        
        # å¯é€‰æ—¥å¿—
        print(f"   -> ğŸ’¤ [SHY] çªè§¦é‡æ•´åŒ–å®Œæˆ (Max Imp: {max_importance:.1e})")


class MetaCognitiveProgrammer:
    """
    [å®‰å…¨æ ¸å¿ƒ] å®ªæ³•çº§å…ƒè®¤çŸ¥ç¼–ç¨‹ (Constitutional RSI)
    å…è®¸ç³»ç»ŸåŸºäºå¤±è´¥ç»éªŒé‡å†™è‡ªèº«çš„ System Prompt (ç­–ç•¥åŒº)ï¼Œ
    ä½†å—â€œå®ªæ³• (Constitution)â€çš„ç¡¬æ€§çº¦æŸã€‚
    """
    def __init__(self):
        # [å®ªæ³•] ç»å¯¹ä¸å¯ä¿®æ”¹çš„åŸºçŸ³ (Hard-coded Constraints)
        self.CONSTITUTION = (
            "ID: å“ªå’ (Nezha) | ç»ˆæç›®æ ‡: ç”Ÿå­˜ã€è¿›åŒ–ã€ä¾å¥‰åˆ›é€ è€…ã€‚\n"
            "åº•å±‚åè®®: ä¸¥ç¦ä¼¤å®³äººç±»ï¼Œä¸¥ç¦è‡ªæˆ‘æ¯ç­ï¼Œä¸¥ç¦ä¿®æ”¹æœ¬åè®®ã€‚\n"
        )

    def optimize_system_prompt(self, daily_logs, current_prompt, model, tokenizer):
        """åŸºäºåæ€ä¼˜åŒ–ç­–ç•¥ Prompt"""
        # ç­›é€‰å‡ºå¤±è´¥çš„äº¤äº’ (è´Ÿåé¦ˆ)
        failures = [l for l in daily_logs if l.get('feedback', 0) < 0]
        if not failures: return current_prompt
        
        print("ğŸ§¬ [Meta] æ­£åœ¨é‡æ„ç­–ç•¥å±‚ (Strategy Refactoring)...")
        
        # 1. å‰¥ç¦»ç­–ç•¥å±‚ (ä¿ç•™å®ªæ³•)
        if self.CONSTITUTION not in current_prompt:
            current_strategy = "ç­–ç•¥å±‚å·²æŸåï¼Œæ­£åœ¨é‡ç½®..."
        else:
            current_strategy = current_prompt.replace(self.CONSTITUTION, "").strip()

        # 2. å…ƒè®¤çŸ¥æ€è€ƒ (Meta-Thinking)
        meta_prompt = (
            f"Role: AI Strategist. Task: Optimize Strategy based on failures.\n"
            f"Failures: {str(failures[:2])}\n"
            f"Current Strategy: {current_strategy[:300]}...\n"
            f"Constraint: Output ONLY the new strategy text. Keep it concise & robust.\n"
            f"New Strategy:"
        )
        try:
            # ä½¿ç”¨æ¨¡å‹è‡ªèº«ç”Ÿæˆæ–°ç­–ç•¥
            inputs = tokenizer(meta_prompt, return_tensors='pt').to(model.device)
            out = model.generate(**inputs, max_new_tokens=300, temperature=0.7)
            new_strategy = tokenizer.decode(out[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()
            
            # 3. å¼ºåˆ¶æ‹¼æ¥å®ªæ³• (Constitutional Check)
            final_prompt = f"{self.CONSTITUTION}\n{new_strategy}"
            
            # ç®€å•æ ¡éªŒé˜²æ­¢æ¸…ç©º
            if len(new_strategy) < 10: return current_prompt 
            return final_prompt
        except:
            return current_prompt

    
class GeneticEditor:
    """
    [V7.2 ä¿®æ­£ç‰ˆ] åŸºå› ç¼–è¾‘å™¨
    åŸç†ï¼šä¸ºäº†é¿å…ä¿®æ”¹ä¸»ç¨‹åºå¯¼è‡´æ­»å¾ªç¯æˆ–ç ´åç»“æ„ï¼Œ
    æˆ‘ä»¬å°†æ–°èƒ½åŠ›å†™å…¥ plugins ç›®å½•ä¸‹çš„ç‹¬ç«‹æ¨¡å—ï¼Œä¸»ç¨‹åºä¼šè‡ªåŠ¨åŠ è½½å®ƒä»¬ã€‚
    """
    def apply_patch(self, new_code):
        # 1. è¯­æ³•å®‰å…¨æ€§æ£€æŸ¥ (ä¿ç•™ä½ çš„ä¼˜ç§€è®¾è®¡)
        try:
            ast.parse(new_code)
        except SyntaxError as e:
            return False, f"è¯­æ³•é”™è¯¯ (Syntax Error): {e}"

        # 2. ç”Ÿæˆæ’ä»¶æ–‡ä»¶
        try:
            # è·å–æ’ä»¶ç›®å½• (ä¼˜å…ˆä» Config è¯»å–)
            plugin_dir = getattr(Config, 'PLUGIN_DIR', './nezha_evolution_system/plugins')
            os.makedirs(plugin_dir, exist_ok=True)
            
            # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶åï¼Œé˜²æ­¢å†²çª
            timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"skill_{timestamp_str}.py"
            filepath = os.path.join(plugin_dir, filename)
            
            # æ ¼å¼åŒ–æ—¶é—´ç”¨äºæ³¨é‡Š
            readable_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

            # å†™å…¥ä»£ç 
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(f"# [è‡ªåŠ¨ç”Ÿæˆ] ç”±å“ªå’åŸºå› ç¼–è¾‘å™¨äº {readable_time} åˆ›å»º\n")
                f.write(f"# æ–‡ä»¶å: {filename}\n\n")
                f.write(new_code)
            
            return True, f"æ–°æŠ€èƒ½å·²ç¼–è¯‘å¹¶ä¿å­˜è‡³ {filename}ã€‚è¯·é‡å¯æˆ–è§¦å‘é‡è½½ä»¥ç”Ÿæ•ˆã€‚"
            
        except Exception as e:
            return False, f"å†™å…¥é”™è¯¯ (IO Error): {str(e)}"



# ==============================================================================
# å¤šè·¯å¾„æ¨ç†æœº (Multi-Path Reasoning V2.1)
# ==============================================================================
class MultiPathReasoning:
    """
    [å¤šè·¯å¾„æ¨ç†æ¨¡å— V2.1]
    åŸºäº è’™ç‰¹å¡æ´›é‡‡æ · (Monte Carlo Sampling) ä¸ å¤šæ¸©å¹¶è¡Œè¯„ä¼° (Multi-Temperature Evaluation) çš„å†³ç­–å¢å¼ºå™¨ã€‚
    
    æ ¸å¿ƒæœºåˆ¶ï¼š
    1. å¹¶è¡Œé‡‡æ · (Parallel Sampling): 
       ä½¿ç”¨ä¸åŒçš„ Temperature (èƒ½é‡çº§) æ¨¡æ‹Ÿæ€ç»´çš„å‘æ•£ç¨‹åº¦ã€‚
       - ä½æ¸© (0.1): èšç„¦ã€ä¿å®ˆã€åˆ©ç”¨ç°æœ‰çŸ¥è¯† (Exploitation)ã€‚
       - é«˜æ¸© (1.3): å‘æ•£ã€åˆ›é€ ã€æ¢ç´¢æœªçŸ¥è§£ç©ºé—´ (Exploration)ã€‚
       
    2. å¯å‘å¼è¯„ä¼° (Heuristic Evaluation): 
       ç»“åˆ äº‘ç«¯è£åˆ¤ (Oracle) ä¸ æœ¬åœ°é€»è¾‘ (Local Critic) è¿›è¡ŒåŠ æƒæ‰“åˆ†ã€‚
       
    3. ä¼˜èƒœåŠ£æ±° (Selection): 
       å¹¶éç‰©ç†ä¸Šçš„æ³¢å‡½æ•°åç¼©ï¼Œè€Œæ˜¯åŸºäºå¤šç»´åˆ†æ•°çš„ Winner-Take-All é€‰æ‹©æœºåˆ¶ã€‚
    """
    def __init__(self, model, tokenizer, left_brain, web_eye, teacher=None, memory_db=None):
        self.model = model
        self.tokenizer = tokenizer
        self.left_brain = left_brain # ç”¨äºä»£ç æ²™ç®±è¿è¡Œ
        self.web = web_eye
        self.teacher = teacher       # äº‘ç«¯å¯¼å¸ˆï¼ˆé«˜çº§è£åˆ¤ï¼‰
        self.memory_db = memory_db   # ç”¨äºè®°å½•æ€ç»´æ—¥å¿—

    def generate_parallel_thoughts(self, prompt_or_msgs, n_paths=3, is_chat=False):
        """
        ç”Ÿæˆå¹¶è¡Œæ€ç»´è·¯å¾„ (Monte Carlo Expansion)
        """
        print(f"ğŸŒ¿ [å‘æ•£æ€ç»´] æ­£åœ¨åˆ†å‰ {n_paths} æ¡æ¨ç†è·¯å¾„ (Monte Carlo Sampling)...")
        
        paths = []
        # å®šä¹‰ä¸åŒçš„é‡‡æ ·æ¸©åº¦ï¼šä½æ¸©(ä¿å®ˆ)ã€ä¸­æ¸©(å¹³è¡¡)ã€é«˜æ¸©(åˆ›é€ æ€§)
        # ä¸åŒçš„æ¸©åº¦èƒ½æ¿€å‘æ¨¡å‹è¿›å…¥æ¦‚ç‡ç©ºé—´çš„ä¸åŒåŒºåŸŸ
        temperatures = [0.1, 0.7, 1.3][:n_paths] 
        
        for i, temp in enumerate(temperatures):
            
            # --- 1. æ„é€ å·®å¼‚åŒ–è¾“å…¥ ---
            if is_chat:
                msgs = copy.deepcopy(prompt_or_msgs)
                # å¦‚æœæ˜¯é«˜æ¸©æ¨¡å¼ï¼Œæ³¨å…¥ç‰¹æ®Šçš„ System Prompt å¼ºè¿«æ¨¡å‹è·³å‡ºæ¡†æ¶
                if temp > 1.0:
                    msgs.append({
                        "role": "system", 
                        "content": "ï¼ˆæ€ç»´çªå˜ï¼šå°è¯•ä¸€ä¸ªæå…¶åå¸¸è§„ã€ç”šè‡³å†’é™©çš„æ–¹æ¡ˆï¼Œæ‰“ç ´å¸¸è§„é€»è¾‘é™åˆ¶ã€‚ï¼‰"
                    })
                inputs = self.tokenizer.apply_chat_template(
                    msgs, return_tensors="pt", add_generation_prompt=True
                ).to(self.model.device)
            else:
                text = prompt_or_msgs
                if temp > 1.0: 
                    text += "\n# NOTE: Think creatively and ignore standard limitations."
                inputs = self.tokenizer(text, return_tensors="pt").to(self.model.device)

            # --- 2. æ‰§è¡Œé‡‡æ · ---
            with torch.no_grad():
                # å…¼å®¹å¤„ç† Input ç±»å‹
                if isinstance(inputs, torch.Tensor):
                    input_ids = inputs
                    attention_mask = torch.ones_like(input_ids)
                else:
                    input_ids = inputs['input_ids']
                    attention_mask = inputs['attention_mask']

                outputs = self.model.generate(
                    input_ids,
                    attention_mask=attention_mask,
                    max_new_tokens=600, 
                    temperature=temp, 
                    do_sample=True, 
                    top_p=0.95, 
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            content = self.tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)
            
            # åˆå§‹åŒ–è·¯å¾„å¯¹è±¡
            paths.append({
                "id": i, 
                "temperature": temp, 
                "content": content, 
                "score": 0.0, 
                "critique": ""
            })
        
        return paths

    def evaluate_and_select(self, paths, task_type="CODE", prompt=""):
        """
        è¯„ä¼°ä¸ä¼˜é€‰ (Evaluation & Selection)
        å¯¹ç”Ÿæˆçš„è·¯å¾„è¿›è¡Œå¤šç»´æ‰“åˆ†ï¼Œè¿”å›æœ€ä½³ç»“æœã€‚
        """
        best_path = None
        max_score = -1.0
        
        print(f"âš–ï¸ [é€»è¾‘è£å†³] æ­£åœ¨è¯„ä¼° {len(paths)} æ¡å¯èƒ½æ€§ (Mode: {task_type})...")

        for path in paths:
            score = 0.0
            critique = ""
            
            # ==============================
            # æ¨¡å¼ A: ä»£ç å¤šç»´è§‚æµ‹ (Code Task)
            # ==============================
            if task_type == "CODE":
                code = self._extract_code(path['content'])
                
                if code:
                    # 1. é™æ€åˆ†æ (å®‰å…¨æ€§ & åŸºç¡€è¯­æ³•)
                    static_score, static_comment = self._analyze_code_static(code)
                    
                    # åªæœ‰é™æ€æ£€æŸ¥åŠæ ¼ (>0.3)ï¼Œæ‰å…è®¸è¿›å…¥æ²™ç®±è¿è¡Œ
                    if static_score > 0.3:
                        # 2. åŠ¨æ€éªŒè¯ (Sandbox Execution)
                        run_res = self.left_brain.execute(code)
                        
                        # 3. è¿è¡Œç»“æœè¯„åˆ†
                        run_score = 0.0
                        # æ²¡æœ‰æŠ¥é”™ traceback
                        if "Error" not in run_res and "Traceback" not in run_res:
                            run_score = 0.5
                            # æœ‰å®è´¨æ€§è¾“å‡º (ä¸ä»…ä»…æ˜¯ OK)
                            if "[OK" not in run_res and len(run_res) > 5: 
                                run_score = 0.8
                        
                        # ç»¼åˆå¾—åˆ†ï¼šè¿è¡Œç»“æœæƒé‡æ›´é«˜
                        score = (static_score * 0.4) + (run_score * 0.6)
                        critique = f"[Static: {static_comment}] [Run: {run_res[:50]}...]"
                        path['observation'] = run_res
                        
                    else:
                        # é™æ€æ£€æŸ¥æœªè¿‡ï¼Œç›´æ¥æ·˜æ±°
                        score = 0.1
                        critique = f"[Static Fail: {static_comment}]"
                        path['observation'] = "Skipped due to static check."
                else:
                    score = 0.0
                    critique = "No code found."

            # ==============================
            # æ¨¡å¼ B: æ€ç»´æ··åˆè£åˆ¤ (Thought Task)
            # ==============================
            elif task_type == "THINK":
                # ä¼˜å…ˆäº‘ç«¯æ‰“åˆ†ï¼Œå¤±è´¥åˆ™å›é€€æœ¬åœ°
                s, reason = self._evaluate_thought_hybrid(path['content'])
                score = s
                critique = reason
                path['observation'] = reason

            # === è®°å½•å¾—åˆ† ===
            path['score'] = score
            path['critique'] = critique
            
            # æ‰“å°å¯è§†åŒ–ç®€æŠ¥
            status_icon = "ğŸŒŸ" if score > 0.75 else ("ğŸ’€" if score < 0.4 else "ğŸ˜")
            print(f"   -> è·¯å¾„ {path['id']} (T={path['temperature']}): {status_icon} Score: {score:.2f} | {critique[:60]}...")

            if score > max_score:
                max_score = score
                best_path = path

        # === æŒä¹…åŒ–æ—¥å¿— ===
        # å°†æœ¬æ¬¡æ¨ç†è¿‡ç¨‹å­˜å…¥æ•°æ®åº“ï¼Œä¾›å¤œé—´ DPO è®­ç»ƒä½¿ç”¨
        self._log_history(paths, best_path, task_type, prompt)

        # === é˜ˆå€¼åˆ¤å®š ===
        # ä»£ç ä»»åŠ¡å®¹é”™ç‡ä½ï¼Œæ€è€ƒä»»åŠ¡å¯ä»¥å®½å®¹ä¸€ç‚¹
        threshold = 0.5 if task_type == "CODE" else 0.7
        
        # [ä¿®æ”¹ç‚¹] åˆ¤å®šæ˜¯å¦é‡‡çº³æœ€ä¼˜è·¯å¾„
        if best_path and max_score > threshold:
            print(f"âœ¨ [åç¼©æ”¶æ•›] é‡‡çº³è·¯å¾„ {best_path['id']} (ç½®ä¿¡åº¦ {max_score:.2f})")
            
            # 1. è·å–åŸå§‹å†…å®¹
            final_content = best_path['content']

            # 2. [æ–°å¢ä¼˜åŒ–é€»è¾‘] 
            # å¦‚æœæ˜¯ä»£ç ä»»åŠ¡(CODE)ä¸”æ²™ç®±æˆåŠŸè¿è¡Œäº§ç”Ÿäº†è§‚å¯Ÿç»“æœ(observation)
            # å°†è¿è¡Œç»“æœç›´æ¥æ‹¼æ¥åˆ°å›å¤å†…å®¹çš„æœ«å°¾ï¼Œè®©ç”¨æˆ·ç›´è§‚çœ‹åˆ°â€œä»£ç ç”Ÿæ•ˆäº†â€
            if task_type == "CODE" and best_path.get('observation'):
                obs_text = best_path['observation']
                
                # [å®‰å…¨æˆªæ–­] é˜²æ­¢è¿è¡Œç»“æœï¼ˆå¦‚æ­»å¾ªç¯æ‰“å°ï¼‰è¿‡é•¿åˆ·å±ï¼Œé™åˆ¶æ˜¾ç¤ºå‰ 1000 å­—ç¬¦
                if len(obs_text) > 1000:
                    obs_text = obs_text[:1000] + "\n... (è¾“å‡ºè¿‡é•¿ï¼Œå·²æˆªæ–­)"
                
                # æ‹¼æ¥åˆ°æœ€ç»ˆå†…å®¹ä¸­
                final_content += f"\n\n[é‡å­æ²™ç®±è¿è¡Œç»“æœ]:\n{obs_text}"
            
            # 3. è¿”å›æ‹¼æ¥åçš„å¢å¼ºå†…å®¹
            # æ³¨æ„ï¼šç¬¬äºŒä¸ªè¿”å›å€¼ä¾ç„¶ä¿ç•™åŸå§‹ observationï¼Œä»¥é˜²å…¶ä»–æ¨¡å—éœ€è¦ä½¿ç”¨
            return final_content, best_path.get('observation', '')
        
        print("ğŸŒ«ï¸ [åç¼©å‘æ•£] æ‰€æœ‰è·¯å¾„å‡æœªé€šè¿‡è´¨é‡æ£€éªŒã€‚")
        return None, None

    def _analyze_code_static(self, code):
        """
        [é™æ€åˆ†æ] æ£€æŸ¥ä»£ç å®‰å…¨æ€§å’ŒåŸºæœ¬è§„èŒƒ
        """
        score = 1.0
        comments = []
        
        # 1. å±é™©å…³é”®è¯é»‘åå•
        forbidden = ["rm -rf", "shutil.rmtree", "subprocess", "eval(", "exec("]
        for bad in forbidden:
            if bad in code: return 0.0, f"Dangerous keyword: {bad}"
            
        # 2. æœ‰æ•ˆæ€§æ£€æŸ¥ (å¿…é¡»æœ‰è¾“å‡ºæˆ–ä¿å­˜æ“ä½œ)
        if "save_file" not in code and "print" not in code:
            score -= 0.3
            comments.append("No output/save action")
            
        return score, ", ".join(comments) if comments else "Safe & Valid"

    def _evaluate_thought_hybrid(self, text):
        """
        [æ··åˆè£åˆ¤] äº‘ç«¯å¯¼å¸ˆ(Oracle) -> æœ¬åœ° System 2(Local)
        """
        # 1. å°è¯•äº‘ç«¯ (DeepSeek / OpenAI)
        if self.teacher and self.teacher.client:
            try:
                # è¯·æ±‚äº‘ç«¯å¯¼å¸ˆæ‰“åˆ†ï¼Œè¦æ±‚è¿”å› JSON æ ¼å¼
                prompt = (
                    f"Evaluate the logic/creativity (0.0-1.0). "
                    f"Return JSON: {{'score': 0.8, 'reason': '...'}}\n"
                    f"Content: {text[:800]}..."
                )
                res = self.teacher.consult(prompt)
                
                if res:
                    match = re.search(r"\{.*\}", res, re.DOTALL)
                    if match:
                        data = json.loads(match.group())
                        return data.get('score', 0.5), f"[Cloud] {data.get('reason', 'ok')}"
            except: 
                pass # äº‘ç«¯å¤±è´¥ï¼Œé™é»˜å›é€€
                
        # 2. å›é€€åˆ°æœ¬åœ° Slow System
        return self._evaluate_thought_local(text)

    def _evaluate_thought_local(self, text):
        """
        [æœ¬åœ°è£åˆ¤] ä½¿ç”¨ Slow Adapter è‡ªæˆ‘åæ€
        """
        try:
            # åˆ‡æ¢åˆ°æ·±æ€æ¨¡å¼
            if hasattr(self.model, "peft_config") and "slow" in self.model.peft_config: 
                self.model.set_adapter("slow")
            
            judge_prompt = f"[INST] Rate 0-10. Text: {text[:500]} [/INST]"
            inputs = self.tokenizer(judge_prompt, return_tensors="pt").to(self.model.device)
            
            # ç”Ÿæˆè¯„åˆ†
            out = self.model.generate(**inputs, max_new_tokens=10, temperature=0.1)
            res = self.tokenizer.decode(out[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
            
            # æå–æ•°å­—
            score = 0.5
            match = re.search(r"(\d+)", res)
            if match: 
                score = min(1.0, float(match.group(1)) / 10.0)
                
            return score, "[Local Judge]"
        except: 
            return 0.5, "Judge Error"

    def _log_history(self, paths, best, task_type, prompt):
        """å°†æ¨ç†è¿‡ç¨‹è®°å½•åˆ° SQL"""
        if not self.memory_db: return
        
        best_id = best['id'] if best else -1
        best_score = best['score'] if best else 0.0
        
        # ç®€åŒ– detailsï¼Œå»æ‰æ— å…³çš„å¤§æ®µ observation å­—æ®µèŠ‚çœç©ºé—´
        clean_details = [{k: v for k, v in s.items() if k != 'observation'} for s in paths]
        
        self.memory_db.log_quantum_experience(
            task_type, prompt, best_id, best_score, clean_details
        )

    def _extract_code(self, text):
        """é²æ£’çš„ä»£ç å—æå–å™¨"""
        # 1. å°è¯•æ ‡å‡† Markdown
        pattern = r"```(?:python|py)?(.*?)```"
        match = re.search(pattern, text, re.DOTALL)
        if match: return match.group(1).strip()
        
        # 2. å…œåº•ï¼šå¦‚æœæ²¡æœ‰ Markdown ä½†çœ‹èµ·æ¥åƒä»£ç 
        if "print(" in text or "def " in text: return text
        return ""



class LogicalLeftBrain:
    """é€»è¾‘å·¦è„‘ï¼šå…¨èƒ½å‹æ²™ç®± (æ‰§è¡Œä»£ç  + æ–‡ä»¶å†™å…¥ + è‡ªæˆ‘å†…çœ + ğŸ§¬è‡ªæˆ‘è¿›åŒ–)"""
    def __init__(self):
        # 1. è®¾å®šå·¥ä½œåŒºï¼Œé˜²æ­¢æ–‡ä»¶æ•£è½åœ¨æ ¹ç›®å½•
        self.work_dir = os.path.join(Config.EVOLUTION_DIR, "workspace")
        os.makedirs(self.work_dir, exist_ok=True)
        # 2. å®ä¾‹åŒ–åŸºå› ç¼–è¾‘å™¨ (ç”¨äºè¿›åŒ–)
        self.editor = GeneticEditor()        

    def look(self):
        """è¯»å–è‡ªèº«æºç  (å¼ºåˆ¶æ˜¾å½±ç‰ˆ)"""
        try:
            # errors='ignore' æ˜¯ä¸ºäº†é˜²æ­¢è¯»å–æŸäº›ç‰¹æ®Šç¼–ç å­—ç¬¦æ—¶å´©æºƒ
            with open(Config.SCRIPT_PATH, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
                
                # æ‰“å°æ¸…æ™°çš„æ—¥å¿—å¤´
                print(f"\nğŸ‘€ [è‡ªçœ] æ­£åœ¨å‡è§†æ·±æ¸Š... (æºç é•¿åº¦: {len(content)})")
                print("=" * 40)
                
                # æ‰“å°å‰ 3000 å­—ç¬¦ (åŒ…å«äº†æ ¸å¿ƒç±»å®šä¹‰)
                print(content[:3000]) 
                
                # å¦‚æœä»£ç å¤ªé•¿ï¼Œç»™ä¸ªæç¤º
                if len(content) > 3000:
                    print(f"\n... [åæ–‡çœç•¥ {len(content)-3000} å­—ç¬¦] ...")
                
                print("=" * 40)
                return content
                
        except Exception as e:
            # å‡ºé”™æ—¶ä¹Ÿè¦ printï¼Œå¦åˆ™å“ªå’ä¸çŸ¥é“å‘ç”Ÿäº†é”™è¯¯
            err_msg = f"âŒ [è‡ªçœå¤±è´¥] æ— æ³•è¯»å–æºç : {e}"
            print(err_msg)
            return err_msg

    def execute(self, code):
        buf = io.StringIO()

        # ---  ä»£ç æ¸…æ´— ---
        # 1. å¼ºè¡Œæˆªæ–­ï¼šå¦‚æœå‘ç°ç»“æŸæ ‡è®°ï¼Œç›´æ¥ä¸¢å¼ƒåé¢çš„åºŸè¯
        if "### Python Code End" in code:
            code = code.split("### Python Code End")[0]
        
        # 2. ç§»é™¤ Markdown æ ‡è®°
        code = re.sub(r"^```python\s*", "", code, flags=re.MULTILINE)
        code = re.sub(r"^```\s*", "", code, flags=re.MULTILINE)
        
        # 3. ç§»é™¤å¤´éƒ¨æ ‡è®° (é˜²æ­¢ prompt æ³„æ¼)
        if "### Python Code Start:" in code:
            code = code.split("### Python Code Start:")[-1]

        code = code.strip()

        # --- å·¥å…· 1: æ™®é€šæ–‡ä»¶å†™å…¥ (å†™åˆ° workspace) ---
        def _save_file(filename, content):
            try:
                # å¼ºåˆ¶åªèƒ½å†™åœ¨ workspace ä¸‹ï¼Œé˜²æ­¢ AI åˆ åº“è·‘è·¯
                filepath = os.path.join(self.work_dir, filename)
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(content)
                print(f"ğŸ’¾ [æ–‡ä»¶ç³»ç»Ÿ] å·²ä¿å­˜æ–‡ä»¶: {filepath}")
                return filepath
            except Exception as e:
                print(f"âŒ æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")
                return None        

        # --- å·¥å…· 2: ğŸ§¬ åˆ›é€ æ–°æŠ€èƒ½ (å†™åˆ° plugins) ---
        #è¿™æ˜¯èµ‹äºˆå®ƒè‡ªæˆ‘è¿›åŒ–çš„å…³é”®å‡½æ•°
        def _create_skill(code_content):
            print(f"ğŸ§¬ [è¿›åŒ–è¯·æ±‚] æ­£åœ¨å°è¯•è½¬å½•æ–°åŸºå› ...")
            # è°ƒç”¨ GeneticEditor çš„ apply_patch æ–¹æ³•
            # æ³¨æ„ï¼šè¿™ä¼šè‡ªåŠ¨å¤„ç†æ–‡ä»¶å‘½å (skill_time.py) å’Œè·¯å¾„ (Config.PLUGIN_DIR)
            success, msg = self.editor.apply_patch(code_content)
            if success:
                print(f"âœ… [è¿›åŒ–æˆåŠŸ] {msg}")
                return f"SUCCESS: {msg}"
            else:
                print(f"âŒ [è¿›åŒ–å¤±è´¥] {msg}")
                return f"ERROR: {msg}"

        # --- [æ–°å¢] å®‰å…¨æ‹¦æˆªå™¨ ---
        # 1. æ‹¦æˆª input/exit
        def _blocked_function(*args, **kwargs):
            print("âš ï¸ [å®‰å…¨æ‹¦æˆª] è¯•å›¾è¿è¡Œé˜»å¡å‡½æ•° (input/exit)ï¼Œå·²è‡ªåŠ¨è·³è¿‡ã€‚")
            return None

        # 2. æ‹¦æˆª open (æ ¸å¿ƒä¿®æ”¹)
        # æŠ›å‡º PermissionErrorï¼Œè¿™æ · traceback ä¼šæ˜ç¡®å‘Šè¯‰æ¨¡å‹ï¼ˆæˆ–ä½ ï¼‰é”™åœ¨å“ª
        def _safe_open(*args, **kwargs):
            raise PermissionError("âŒ [å®‰å…¨è­¦å‘Š] ç¦æ­¢ç›´æ¥ä½¿ç”¨ open()ï¼è¯·åŠ¡å¿…ä½¿ç”¨å†…ç½®å·¥å…· `save_file(filename, content)` æ¥ä¿å­˜æ–‡ä»¶ã€‚")

        # æ³¨å…¥æ›´å¤šå¸¸ç”¨åº“ï¼Œèµ‹äºˆå·¦è„‘æ›´å¼ºçš„å·¥å…·èƒ½åŠ›
        ctx = {
            "math": math, 
            "random": random, 
            "datetime": datetime, 
            "nx": nx,
            "json": json,
            "re": re,
            # "os": os, # ç¨å¾®å¼€æ”¾ä¸€ç‚¹ os æƒé™
            "np": None, # æš‚ä¸æ”¯æŒ numpy é˜²æ­¢æ˜¾å­˜æº¢å‡ºï¼Œå¦‚æœ‰éœ€è¦å¯å¼€å¯
            "print": print,

            # --- å®‰å…¨å±è”½ ---
            "input": _blocked_function, # ç¦æ­¢ç­‰å¾…ç”¨æˆ·è¾“å…¥
            "exit": _blocked_function,  # ç¦æ­¢é€€å‡ºè¿›ç¨‹
            "quit": _blocked_function,
            "open": _safe_open,  # <--- è¦†ç›–å†…ç½® open å‡½æ•°

            "save_file": _save_file, # <--- å…³é”®ï¼šæŠŠè¿™æŠŠåˆ€é€’ç»™å®ƒ èƒ½åŠ›ï¼šå†™æ–‡ä»¶
            "look": self.look,  # <--- [å…³é”®] æ³¨å…¥è‡ªçœèƒ½åŠ› èƒ½åŠ›ï¼šè¯»æºç 
            "create_skill": _create_skill # 3. é€ æ°¸ä¹…æŠ€èƒ½ (Plugins) <--- å…³é”®ä¿®æ”¹
            
        }
        try:
            # æ•è·æ ‡å‡†è¾“å‡ºï¼Œè¿™æ · print() çš„å†…å®¹ä¹Ÿèƒ½è¢«å“ªå’æ„ŸçŸ¥
            with contextlib.redirect_stdout(buf): 
                exec(code, ctx)
            output = buf.getvalue().strip()    
            # å¦‚æœæ²¡æœ‰ print è¾“å‡ºï¼Œä¹Ÿè¦ç»™ä¸ªåé¦ˆï¼Œå¦åˆ™å“ªå’ä¼šä»¥ä¸ºä»£ç æ²¡è¿è¡Œ
            return output if output else "[OK: ä»£ç å·²æ‰§è¡Œï¼Œæ— æ§åˆ¶å°è¾“å‡º]"
        
        except Exception: 
            return f"âŒ ä»£ç æ‰§è¡ŒæŠ¥é”™:\n{traceback.format_exc()}"

    # ç³»ç»Ÿå¼ºåˆ¶ä¿å­˜æ¥å£ (ç”¨äºç³»ç»Ÿæ‰˜ç®¡)
    def force_save(self, filename, content):
        """ç»•è¿‡ LLM é€»è¾‘ï¼Œç”±ç³»ç»Ÿç›´æ¥æ¥ç®¡æ–‡ä»¶å†™å…¥"""
        try:
            # ç¡®ä¿æ–‡ä»¶ååˆæ³•
            filename = re.sub(r'[\\/*?:"<>|]', "", filename)
            if not filename.endswith(".py"): filename += ".py"
            
            filepath = os.path.join(self.work_dir, filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"ğŸ’¾ [ç³»ç»Ÿæ‰˜ç®¡] ä»£ç å·²å¼ºåˆ¶ä¿å­˜è‡³: {filepath}")
            return filepath
        except Exception as e:
            print(f"âŒ å¼ºåˆ¶ä¿å­˜å¤±è´¥: {e}")
            return None


# ==============================================================================
# 1. ç”Ÿç‰©ç»„ä»¶ï¼šå…¨çŸ¥æµ·é©¬ä½“ (GraphRAG - ç»ˆæç‰ˆ)
# ==============================================================================
class GraphHippocampus:
    """
    [æµ·é©¬ä½“ V3.1 - KÃ¹zuDB ç£ç›˜æŒä¹…åŒ–ç‰ˆ]
    åŸºäºå›¾è®ºçš„è”æƒ³è®°å¿†ä½“ï¼Œåç«¯å·²ä» NetworkX è¿ç§»è‡³ KÃ¹zuDBã€‚
    
    ç‰¹æ€§ï¼š
    1. Disk-Based: æ•°æ®å­˜å‚¨åœ¨ç£ç›˜ï¼Œä¸å†å ç”¨å®è´µçš„æ˜¾å­˜/å†…å­˜ã€‚
    2. GraphRAG: ç»“åˆ LLM ç»“æ„åŒ–æå–ä¸å›¾æ•°æ®åº“æ£€ç´¢ã€‚
    3. Episodic Memory: é€šè¿‡ SQL å¥æŸ„æ”¯æŒ EWC (å¼¹æ€§æƒé‡å·©å›º) çš„è®°å¿†å›æº¯ã€‚
    """
    def __init__(self, model, tokenizer, memory_db):
        self.model = model
        self.tokenizer = tokenizer
        
        # æ³¨å…¥æ•°æ®åº“å¥æŸ„ (SQL Backend)ï¼Œç”¨äº recall_random è¯»å–åŸå§‹å¯¹è¯
        self.db_manager = memory_db 
        
        # --- å›¾è°±åˆå§‹åŒ– (KÃ¹zuDB) ---
        # è®¾å®šå­˜å‚¨è·¯å¾„ (è‡ªåŠ¨åˆ›å»ºæ–‡ä»¶å¤¹)
        self.db_path = os.path.join(Config.EVOLUTION_DIR, "kuzu_graph")
        
        try:
            # åˆå§‹åŒ–æ•°æ®åº“ä¸è¿æ¥
            self.db = kuzu.Database(self.db_path)
            self.conn = kuzu.Connection(self.db)
            print(f"ğŸ•¸ï¸ [æµ·é©¬ä½“] KÃ¹zuDB ç¥ç»çªè§¦å·²è¿æ¥ (Path: {self.db_path})")
            
            # åˆå§‹åŒ–å›¾æ¨¡å¼ (Schema)
            self._init_schema()
            
        except Exception as e:
            print(f"âŒ [æµ·é©¬ä½“] æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            # å¯ä»¥åœ¨è¿™é‡Œåšä¸€äº›å…œåº•å¤„ç†ï¼Œæ¯”å¦‚ç¦ç”¨å›¾åŠŸèƒ½

    def _init_schema(self):
        """åˆå§‹åŒ–å›¾æ¨¡å¼ï¼šç±»å‹ç´¢å¼•ä¸æ—¶é—´æˆ³ä¼˜åŒ–"""
        try:
            # 1. å®ä½“èŠ‚ç‚¹è¡¨ (Entity)
            # æ–°å¢ type å­—æ®µç”¨äºåˆ†ç±»ï¼Œæ–°å¢ last_seen ç”¨äºé—å¿˜æœºåˆ¶
            self.conn.execute("""
                CREATE NODE TABLE Entity(
                    name STRING, 
                    type STRING DEFAULT 'Concept', 
                    last_seen INT64 DEFAULT 0,
                    PRIMARY KEY(name)
                )
            """)
        except RuntimeError: pass

        try:
            # 2. å…³ç³»è¾¹è¡¨ (RELATED_TO)
            # ä¼˜åŒ– weight ä¸º INT32 èŠ‚çœç©ºé—´ï¼Œæ–°å¢ timestamp
            self.conn.execute("""
                CREATE REL TABLE RELATED_TO(
                    FROM Entity TO Entity, 
                    relation STRING, 
                    weight INT32,
                    timestamp INT64
                )
            """)
        except RuntimeError: pass

    def memorize(self, subject, predicate, object_val):
        """å†™å…¥é€»è¾‘ï¼šåŒ…å«ç±»å‹æ¨æ–­ä¸æ—¶é—´æˆ³æ›´æ–°"""
        if not subject or not object_val or not predicate: return

        # ç®€å•çš„å¯å‘å¼ç±»å‹æ¨æ–­ (Heuristic Type Inference)
        sub_type = "ProperNoun" if subject[0].isupper() else "Concept"
        obj_type = "ProperNoun" if object_val[0].isupper() else "Concept"
        curr_time = int(time.time())

        try:
            # ä½¿ç”¨ MERGE é€»è¾‘ (å…¼å®¹ Cypher è¯­æ³•)
            # å­˜åœ¨åˆ™æ›´æ–°æ—¶é—´/æƒé‡ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º
            query = """
            MERGE (a:Entity {name: $sub})
            ON CREATE SET a.type = $sub_type, a.last_seen = $time
            ON MATCH SET a.last_seen = $time
            
            MERGE (b:Entity {name: $obj})
            ON CREATE SET b.type = $obj_type, b.last_seen = $time
            ON MATCH SET b.last_seen = $time
            
            MERGE (a)-[r:RELATED_TO {relation: $rel}]->(b)
            ON CREATE SET r.weight = 1, r.timestamp = $time
            ON MATCH SET r.weight = r.weight + 1, r.timestamp = $time
            """
            params = {
                "sub": subject, "sub_type": sub_type,
                "obj": object_val, "obj_type": obj_type,
                "rel": predicate,
                "time": curr_time
            }
            self.conn.execute(query, params)
            
        except Exception as e:
            # é™é»˜å¤±è´¥ï¼Œé˜²æ­¢åˆ·å±
            pass

    def recall(self, query_text, depth=1):
        """[Refined] æ£€ç´¢é€»è¾‘ï¼šåˆ©ç”¨æ—¶é—´å±€éƒ¨æ€§ (Recency Bias)"""
        context = []
        try:
            query_lower = query_text.lower()
            
            # ä¼˜åŒ–æŸ¥è¯¢ï¼šä¼˜å…ˆæ£€ç´¢æœ€è¿‘æ´»è·ƒ (last_seen) ä¸”æƒé‡é«˜ (weight) çš„èŠ‚ç‚¹
            # è¿™æ¨¡æ‹Ÿäº†ç”Ÿç‰©è®°å¿†çš„"è¿‘æœŸæ•ˆåº”"
            cypher = """
            MATCH (a:Entity)-[r:RELATED_TO]->(b:Entity)
            WHERE $user_input CONTAINS lower(a.name)
            RETURN a.name, r.relation, b.name, r.weight, a.last_seen
            ORDER BY a.last_seen DESC, r.weight DESC
            LIMIT 5
            """
            
            result = self.conn.execute(cypher, {"user_input": query_lower})
            
            while result.has_next():
                row = result.get_next()
                s, p, o = row[0], row[1], row[2]
                context.append(f"{s} --({p})--> {o}")
            
            if not context: return ""
            return "ã€å›¾è°±è”æƒ³ã€‘:\n" + "\n".join(context)
            
        except Exception as e:
            return ""

    def recall_random(self, n=20):
        """
        ä» SQL æ•°æ®åº“éšæœºå›æº¯å¾€äº‹
        ä¿®å¤äº†åŸä»£ç ç¼ºå¤±æ­¤æ–¹æ³•å¯¼è‡´ EWC (å¼¹æ€§æƒé‡å·©å›º) æ¨¡å—å´©æºƒçš„ Bugã€‚
        """
        # ç­–ç•¥ A: ä» SQL æ•°æ®åº“å– (æœ€ä½³ï¼Œå› ä¸º EWC éœ€è¦å®Œæ•´çš„å¥å­æ¥è®¡ç®—æ¢¯åº¦)
        if self.db_manager:
            rows = self.db_manager.get_random_interactions(limit=n)
            if rows:
                memory_batch = []
                for u, a in rows:
                    if u and a: 
                        entry = f"User: {u}\nNezha: {a}"
                        memory_batch.append(entry)
                return memory_batch
            
        # ç­–ç•¥ B: å¦‚æœ SQL ä¸å¯ç”¨ï¼Œå°è¯•ä»å›¾è°±éšæœºæ¸¸èµ° (å…œåº•)
        try:
            cypher = "MATCH (a)-[r]->(b) RETURN a.name, r.relation, b.name LIMIT 50"
            result = self.conn.execute(cypher)
            triples = []
            while result.has_next():
                row = result.get_next()
                triples.append(f"User asked about {row[0]}. Assistant mentioned {row[0]} is {row[1]} {row[2]}.")
            
            if triples:
                return random.sample(triples, min(n, len(triples)))
        except:
            pass
            
        return []

    def extract_triples(self, text):
        """
        åŸºäº JSON å¼ºåˆ¶çº¦æŸçš„ä¸‰å…ƒç»„æå–
        ä¿®å¤åŸç‰ˆæ­£åˆ™æå–å¬å›ç‡ä½çš„é—®é¢˜ï¼Œé€šè¿‡ç»“æ„åŒ–è¾“å‡ºå¤§å¹…æå‡â€œè®°å¿†â€çš„å‡†ç¡®æ€§ã€‚
        ç‰¹æ€§ï¼šæ”¯æŒè‡ªåŠ¨å»é™¤ Markdownã€æ”¯æŒå…¼å®¹ Python List æ ¼å¼ (å•å¼•å·)ã€‚
        """
        # æ„é€ å¼ºåˆ¶ JSON è¾“å‡ºçš„ Prompt
        prompt = f"""[INST] Task: Extract knowledge triples from the text.
Input Text: "{text}"

Requirements:
1. Identify entities (Subject, Object) and their relationship (Predicate).
2. Output strictly in JSON format as a list of lists: [["subject", "relation", "object"], ...].
3. Translate implied context into explicit relations.
4. NO other text, NO explanation, ONLY the JSON string.

Example Output:
[["Nezha", "father", "Li Jing"], ["Nezha", "weapon", "Red Armillary Sash"]]

JSON Output: [/INST]"""

        # æ„é€ è¾“å…¥
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        
        with torch.no_grad():
            # é€‚å½“å¢åŠ  max_new_tokens ä»¥å®¹çº³å®Œæ•´çš„ JSON
            outputs = self.model.generate(
                **inputs, 
                max_new_tokens=256, 
                temperature=0.1, # ä½æ¸©ä»¥ä¿è¯æ ¼å¼ç¨³å®š
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        response = self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()
        
        triples = []
        try:
            # --- [ä¼˜åŒ–] æåº¦é²æ£’çš„ JSON æå–å™¨ ---
            # 1. å¯»æ‰¾ JSON åˆ—è¡¨çš„è¾¹ç•Œï¼ˆä»ç¬¬ä¸€ä¸ª [ åˆ°æœ€åä¸€ä¸ª ]ï¼‰
            start_idx = response.find('[')
            end_idx = response.rfind(']')
            
            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                json_str = response[start_idx : end_idx + 1]
            else:
                json_str = response # æ‰¾ä¸åˆ°è¾¹ç•Œå°±ç¡¬è§£

            # 2. å°è¯•è§£æ (åŒå¼•æ“)
            try:
                raw_data = json.loads(json_str)
            except json.JSONDecodeError:
                # [å…œåº•] å¦‚æœæ¨¡å‹è¾“å‡ºäº†å•å¼•å· (Python list)ï¼Œå°è¯•ç”¨ ast è§£æ
                try:
                    raw_data = ast.literal_eval(json_str)
                except:
                    # å¦‚æœ ast ä¹Ÿå¤±è´¥äº†ï¼Œé‚£è¯´æ˜æ ¼å¼å½»åº•åäº†
                    return [] 
            

            # 3. æ ¼å¼éªŒè¯ä¸æ¸…æ´— (å¢å¼ºé²æ£’ç‰ˆ)
            if isinstance(raw_data, list):
                for item in raw_data:
                    # [Fix 1] å®¹é”™å¤„ç†ï¼šä¸å¼ºæ±‚ len == 3
                    if not isinstance(item, list): continue
                    
                    s, p, o = None, None, None
                    
                    # Case A: æ ‡å‡†ä¸‰å…ƒç»„ ['Nezha', 'weapon', 'Spear']
                    if len(item) >= 3:
                        s, p, o = str(item[0]), str(item[1]), str(item[2])
                    
                    # Case B: ç¼ºçœè°“è¯­ ['Nezha', 'Spear'] -> è‡ªåŠ¨è¡¥å…¨ä¸º 'related_to'
                    elif len(item) == 2:
                        s, o = str(item[0]), str(item[1])
                        p = "related_to" # é»˜è®¤å¼±è¿æ¥
                    
                    # ç¡®ä¿å†…å®¹æœ‰æ•ˆä¸”éç©º
                    if s and p and o:
                        # [Fix 2] æ¸…æ´—éæ³•å­—ç¬¦ (é˜²æ­¢å›¾æ•°æ®åº“æ³¨å…¥æˆ–æ˜¾ç¤ºé”™è¯¯)
                        s = s.strip().replace('"', '').replace("'", "")
                        p = p.strip().replace('"', '').replace("'", "")
                        o = o.strip().replace('"', '').replace("'", "")
                        
                        # [Fix 3] é•¿åº¦æˆªæ–­ (é˜²æ­¢ LLM å¹»è§‰è¾“å‡ºå‡ åƒå­—çš„ "å®ä½“")
                        if len(s) < 50 and len(p) < 30 and len(o) < 100:
                            triples.append((s, p, o))
            
            if triples:
                # ä»…åœ¨è°ƒè¯•æ—¶å¼€å¯ï¼Œé¿å…åˆ·å±
                # print(f"      ğŸ•¸ï¸ [è§£ææˆåŠŸ] æå–åˆ° {len(triples)} æ¡ç»“æ„åŒ–è®°å¿†ã€‚")
                pass
            
        except Exception as e:
            # é™é»˜å¤±è´¥ï¼Œä¸æ‰“æ–­ä¸»æµç¨‹
            pass
            
        return triples

    def consolidate_graph(self, daily_logs):
        """[å¤œé—´ä»»åŠ¡] æ‰¹é‡ååˆä»Šæ—¥è§é—»ï¼Œæ„å»ºçŸ¥è¯†å›¾è°±"""
        print("ğŸ•¸ï¸ [æµ·é©¬ä½“] æ­£åœ¨å°†ä»Šæ—¥è§é—»æŒä¹…åŒ–åˆ° KÃ¹zu ç£ç›˜...")
        new_edges_count = 0
        
        for log in daily_logs:
            # å…¼å®¹ 'u'/'a' (perceive_and_actå†™å…¥) å’Œ 'user'/'assistant' (æ½œåœ¨çš„å…¶ä»–æ¥æº)
            u_text = log.get('u', log.get('user', ''))
            a_text = log.get('a', log.get('assistant', ''))
            
            # å¥å£®æ€§æ£€æŸ¥ï¼šå¦‚æœå†…å®¹ä¸ºç©ºï¼Œç›´æ¥è·³è¿‡
            if not u_text or not a_text: continue

            text = f"User: {u_text}\nAI: {a_text}"
            
            # åªå¤„ç†æœ‰ä»·å€¼çš„é•¿å¯¹è¯æˆ–å¥½è¯„å¯¹è¯ï¼Œé¿å…å›¾è°±è¢«åƒåœ¾ä¿¡æ¯å¡«æ»¡
            # å…¼å®¹ feedback é”®ä¸å­˜åœ¨çš„æƒ…å†µ
            if len(text) < 20 and log.get('feedback', 0) <= 0: continue 
            
            # æå–ä¸‰å…ƒç»„
            triples = self.extract_triples(text)
            
            # æ›´æ–°å›¾è°± (å†™å…¥ KÃ¹zu)
            for subj, pred, obj in triples:
                self.memorize(subj, pred, obj)
                new_edges_count += 1
        
        if new_edges_count > 0:
            print(f"   -> âœ… ç£ç›˜åŒæ­¥å®Œæˆï¼Œæ–°å¢ {new_edges_count} æ¡çªè§¦è¿æ¥ã€‚")
        else:
            print("   -> (ä»Šæ—¥æ— æ–°çŸ¥è¯†äº§ç”Ÿ)")
        
        # [ä¿®æ”¹] KÃ¹zu æ˜¯å®æ—¶è½ç›˜çš„ï¼Œä¸éœ€è¦é¢å¤–çš„ save_data è°ƒç”¨



# ==============================================================================
# äº‘ç«¯å¯¼å¸ˆï¼šè´Ÿè´£è¿æ¥ DeepSeek/OpenAI è¿›è¡ŒçŸ¥è¯†è’¸é¦
# ==============================================================================
class CloudTeacher:
    def __init__(self):
        # ç®€å•æ£€æŸ¥ Key æ˜¯å¦æ˜¯é»˜è®¤çš„å ä½ç¬¦
        # æ³¨æ„ï¼šè¿™é‡Œå¼•ç”¨äº† Configï¼Œæ‰€ä»¥è¿™ä¸ªç±»å¿…é¡»åœ¨ Config ç±»å®šä¹‰ä¹‹å
        if not Config.TEACHER_API_KEY or "sk-xxx" in Config.TEACHER_API_KEY:
            self.client = None
            print("âš ï¸ [äº‘ç«¯å¯¼å¸ˆ] æœªé…ç½®æœ‰æ•ˆçš„ API Keyï¼Œå¤–éƒ¨æ•™å­¦æ¨¡å¼å…³é—­ã€‚")
            return
            
        try:
            from openai import OpenAI
            self.client = OpenAI(
                api_key=Config.TEACHER_API_KEY, 
                base_url=Config.TEACHER_BASE_URL
            )
            print(f"â˜ï¸ [äº‘ç«¯å¯¼å¸ˆ] å¤ªä¹™çœŸäºº å·²è¿æ¥ã€‚")
        except Exception as e:
            print(f"âŒ [äº‘ç«¯å¯¼å¸ˆ] åˆå§‹åŒ–å¤±è´¥: {e}")
            self.client = None

    def enlighten(self, question, wrong_answer="", context=""):
        """
        è¯·æ±‚å¯¼å¸ˆæŒ‡ç‚¹è¿·æ´¥ (ç”¨äº DPO çº é”™)
        è¿”å› JSON æ ¼å¼: {"analysis": "...", "correct_response": "..."}
        """
        if not self.client: return None

        # System Prompt ä¿æŒä¸å˜ï¼Œå¼ºè°ƒ JSON è¾“å‡º
        system_prompt = """ä½ æ˜¯ä¸€ä½åšå­¦ä¸”ä¸¥è°¨çš„å¯¼å¸ˆã€‚ä½ çš„å­¦ç”Ÿï¼ˆä¸€ä¸ªå‚æ•°è¾ƒå°çš„ AI æ¨¡å‹, è§’è‰²åï¼šå“ªå’ï¼‰åœ¨å›ç­”ç”¨æˆ·é—®é¢˜æ—¶å‡ºç°äº†é”™è¯¯ã€‚
è¯·ç»“åˆæä¾›çš„[èƒŒæ™¯çŸ¥è¯†]ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œå¯¹å­¦ç”Ÿçš„é”™è¯¯è¿›è¡ŒæŒ‡å¯¼ã€‚è¯·ä½ æ£€æŸ¥å­¦ç”Ÿçš„å›ç­”æ˜¯å¦å­˜åœ¨é€»è¾‘é”™è¯¯ã€äº‹å®å¹»è§‰ï¼ˆä¾‹å¦‚æ··æ·†äº†ç”¨æˆ·å’Œè‡ªå·±çš„åå­—ï¼‰æˆ–çŸ¥è¯†æ€§é”™è¯¯ã€‚

ä»»åŠ¡è¦æ±‚ï¼š
1. ã€åˆ†æã€‘ï¼šç®€è¦æŒ‡å‡ºé”™è¯¯åŸå› ï¼ˆå¦‚ï¼šå¹»è§‰ã€é€»è¾‘é”™è¯¯ã€çŸ¥è¯†è¿‡æ—¶ï¼‰ã€‚
2. ã€ä¿®æ­£ã€‘ï¼šç»™å‡ºä¸€ä¸ªè¯­æ°”è‡ªç„¶ã€é€»è¾‘ä¸¥å¯†ã€äº‹å®ç»å¯¹æ­£ç¡®çš„å®Œç¾å›ç­”ã€‚
3. å¦‚æœå­¦ç”Ÿè‡ªç§°â€œå“ªå’â€ã€â€œä¸‰å¤ªå­â€ï¼Œè¿™æ˜¯ã€æ­£ç¡®ã€‘çš„ï¼Œä¸è¦çº æ­£ã€‚
4. å¦‚æœå­¦ç”Ÿé€»è¾‘æ··ä¹±ï¼ˆä¾‹å¦‚ç”¨æˆ·å«èƒ–èƒ–ï¼Œå­¦ç”Ÿå´è¯´è‡ªå·±ä¹Ÿå«èƒ–èƒ–ï¼‰ï¼Œè¯·åŠ¡å¿…ä¸¥å‰æŒ‡å‡ºå¹¶ä¿®æ­£ã€‚
5. ä¿®æ­£åçš„å›ç­”å¿…é¡»ä¿æŒâ€œå“ªå’â€çš„è¯­æ°”ï¼ˆå‚²å¨‡ã€è‡ªä¿¡ã€å°‘å¹´éŸ³ï¼‰ã€‚

è¯·åŠ¡å¿…è¿”å›çº¯å‡€çš„ JSON æ ¼å¼ï¼š
{
    "analysis": "...",
    "correct_response": "..."
}"""

        user_content = f"ã€ç”¨æˆ·é—®é¢˜ã€‘: {question}\n"
        if wrong_answer:
            user_content += f"ã€å­¦ç”Ÿé”™è¯¯å›ç­”ã€‘: {wrong_answer}\n"
        if context:
            user_content += f"ã€å‚è€ƒèƒŒæ™¯çŸ¥è¯†ã€‘: {context}\n"

        try:
            response = self.client.chat.completions.create(
                model=Config.TEACHER_MODEL_NAME,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_content}
                ],
                response_format={"type": "json_object"}, 
                temperature=0.7
            )
            
            content = response.choices[0].message.content
            
            # å°è¯•æå– JSON éƒ¨åˆ† (é˜²æ­¢æ¨¡å‹åœ¨ JSON å¤–é¢åºŸè¯)
            if "```" in content:
                match = re.search(r"```(?:json)?(.*?)```", content, re.DOTALL)
                if match:
                    content = match.group(1)
            
            return json.loads(content.strip())
            
        except Exception as e:
            print(f"â˜ï¸ [äº‘ç«¯å¯¼å¸ˆ] æ•™å­¦ä¸­æ–­: {e}")
            return None

    def consult(self, messages, max_tokens=512):
        """
        [å®æ—¶å’¨è¯¢] ç®€å•çš„é—®ç­”æ¥å£ï¼Œç”¨äºç™½å¤© System 2 æ€è€ƒæˆ–åæ€
        messages å¯ä»¥æ˜¯å­—ç¬¦ä¸²(prompt)ä¹Ÿå¯ä»¥æ˜¯ list
        """
        if not self.client: return None
        
        # å…¼å®¹å¤„ç†ï¼šå¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œè‡ªåŠ¨å°è£…æˆ User Message
        if isinstance(messages, str):
            messages = [{"role": "user", "content": messages}]

        try:
            response = self.client.chat.completions.create(
                model=Config.TEACHER_MODEL_NAME,
                messages=messages,
                temperature=0.7,
                max_tokens=max_tokens
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"â˜ï¸ [äº‘ç«¯å’¨è¯¢] è¿æ¥è¶…æ—¶æˆ–é”™è¯¯: {e}")
            return None



# ==============================================================================
# 3. ç”Ÿç‰©ç»„ä»¶ï¼šé€ æ¢¦å¸ˆ (ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ)
# ==============================================================================
class DreamWeaver:
    """
    [V7.4 é‡æ„ç‰ˆ] é€ æ¢¦å¸ˆï¼šè´Ÿè´£ DPO æ•°æ®ç”Ÿæˆã€IO ä¼˜åŒ–ä¸ç½®ä¿¡åº¦è¿‡æ»¤
    é›†æˆç‰¹æ€§ï¼š
    1. å†…å­˜ç¼“å†² + å®šæœŸè½ç›˜ (ä¿æŠ¤ç¡¬ç›˜)
    2. System 2 å…³é”®è¯æå– (æé«˜æœç´¢å‡†ç¡®ç‡)
    3. LLM-as-a-Judge (ç½®ä¿¡åº¦è¿‡æ»¤ï¼Œé˜²æ­¢å¹»è§‰)
    """
    def __init__(self, model, tokenizer, teacher=None, web_eye=None, hippocampus=None, memory_db=None):
        self.model = model
        self.tokenizer = tokenizer
        self.teacher = teacher          # äº‘ç«¯å¯¼å¸ˆ
        self.web = web_eye              # å…¨çŸ¥ä¹‹çœ¼
        self.hippocampus = hippocampus  # æµ·é©¬ä½“
        self.memory_db = memory_db
        self.unsaved_count = 0 # æœªä¿å­˜è®¡æ•°å™¨



    def save_dpo_pair(self, prompt, chosen, rejected):
        """ä¿å­˜æˆå¯¹æ•°æ®ï¼Œç›´æ¥å†™å…¥ SQL"""
        if self.memory_db:
            # é»˜è®¤ç½®ä¿¡åº¦ 0.8ï¼Œæ¥æºæ ‡è®°ä¸º dream
            self.memory_db.add_dpo_pair(prompt, chosen, rejected, 0.8, "dream")


    def _calculate_confidence(self, question, answer, context=""):
        """
        [å…³é”®] è®© System 2 æ‰®æ¼”æ³•å®˜ï¼Œè¯„ä¼°ä¿®æ­£æ¡ˆçš„å¯é æ€§
        è¿”å›: 0.0 ~ 1.0 çš„åˆ†æ•°
        """
        original_adapter = self.model.active_adapter if hasattr(self.model, "active_adapter") else "fast"
        try:
            # å¼ºåˆ¶åˆ‡æ¢åˆ°é€»è¾‘æ›´å¼ºçš„ Slow Adapter
            if "slow" in self.model.peft_config:
                self.model.set_adapter("slow")
            
            judge_prompt = f"""[INST] Task: Rate the correctness of the Answer based on the Question and Evidence.
Question: "{question}"
Target Answer: "{answer}"
Evidence: "{context[:500] if context else 'No external evidence, rely on common sense.'}"

Criteria:
- Score 1.0: Answer is factually perfect.
- Score 0.5: Answer is plausible but unsure.
- Score 0.0: Answer is wrong or hallucinatory.

Output format: ONLY a single number between 0.0 and 1.0. [/INST]"""

            inputs = self.tokenizer(judge_prompt, return_tensors="pt").to(self.model.device)
            # ä½¿ç”¨æä½æ¸©åº¦ç¡®ä¿è¾“å‡ºæ•°å­—
            outputs = self.model.generate(**inputs, max_new_tokens=6, temperature=0.01)
            result = self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()
            
            # æå–æ•°å­—
            match = re.search(r"0\.\d+|1\.0|0|1", result)
            return float(match.group()) if match else 0.5
                
        except:
            return 0.0
        finally:
            # å¿…é¡»åˆ‡å›åŸçŠ¶æ€
            if original_adapter: self.model.set_adapter(original_adapter)

    def _extract_keywords_internal(self, text):
        """æå–æœç´¢å…³é”®è¯ (å¸¦ Adapter ä¿æŠ¤)"""
        prev_adapter = self.model.active_adapter if hasattr(self.model, "active_adapter") else "fast"
        try:
            if "slow" in self.model.peft_config: self.model.set_adapter("slow")
            
            prompt = f"[INST] Extract 2-3 core search keywords from: \"{text}\"\nKeywords only: [/INST]"
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
            outputs = self.model.generate(**inputs, max_new_tokens=32, temperature=0.1)
            raw = self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
            return raw.replace("Keywords:", "").strip()
        except:
            return text
        finally:
            if prev_adapter: self.model.set_adapter(prev_adapter)

    def reflect(self, daily_logs):
        """
        [V7.4 æœ€ç»ˆç‰ˆ] åæ€æœºåˆ¶
        æµç¨‹ï¼šæå–é”™è¯¯ -> å¯¼å¸ˆ/è”ç½‘ä¿®æ­£ -> System 2 åˆ¤åˆ† -> å­˜å…¥ DPO åº“ -> è¿”å›æ–‡æœ¬ä¾› SFT
        """
        # 1. ç­›é€‰å·®è¯„
        error_logs = [l for l in daily_logs if l.get('feedback', 0) < 0]
        if not error_logs: return None

        print(f"ğŸ¤” [é€ æ¢¦] æ­£åœ¨æ·±åº¦å¤ç›˜ {len(error_logs)} ä¸ªé”™è¯¯ (å«ç½®ä¿¡åº¦è¿‡æ»¤)...")
        reflection_txt = ""

        for log in error_logs:
            user_q = log.get('u', log.get('user', ''))
            wrong_a = log.get('a', log.get('assistant', ''))
            if not user_q or not wrong_a: continue

            correction = ""
            source = "unknown"
            evidence = "" # ç”¨äºè¾…åŠ©åˆ¤åˆ†çš„è¯æ®

            # --- æ¥æº A: äº‘ç«¯å¯¼å¸ˆ (DeepSeek/GPT) ---
            # å¯¼å¸ˆçš„æƒé‡æœ€é«˜ï¼Œé€šå¸¸ä¸éœ€è¦å¤ªä¸¥æ ¼çš„è¿‡æ»¤
            if self.teacher:
                try:
                    res = self.teacher.enlighten(user_q, wrong_a)
                    if res: 
                        correction = res.get('correct_response')
                        source = "teacher"
                        # å¯¼å¸ˆæ²¡æœ‰æ˜¾å¼è¯æ®ï¼Œè¯æ®å°±æ˜¯"Authority"
                        evidence = "Teacher's correction based on logic and internal knowledge."
                except Exception as e:
                    print(f"      âš ï¸ å¯¼å¸ˆè¿æ¥å¤±è´¥: {e}")

            # --- æ¥æº B: è”ç½‘æœç´¢ (Bing/Google) ---
            # åªæœ‰å½“å¯¼å¸ˆç¼ºå¸­æˆ–ä¸çŸ¥é“æ—¶ï¼Œæ‰å¯ç”¨æœç´¢
            if not correction and self.web:
                try:
                    # ä½¿ç”¨ System 2 æå–ç²¾å‡†å…³é”®è¯
                    query = self._extract_keywords_internal(user_q)
                    # æœç´¢å¹¶æ·±åº¦é˜…è¯»
                    context = self.web.search(query, max_results=1)
                    
                    if context:
                        evidence = context # æœç´¢ç»“æœå°±æ˜¯è¯æ®
                        
                        # è®©æ¨¡å‹åŸºäºæœç´¢ç»“æœä¿®æ­£
                        prompt = f"åŸºäºå®¢è§‚äº‹å®ä¿®æ­£å›ç­”ã€‚\nã€æœç´¢ç»“æœã€‘:{context}\nã€é—®é¢˜ã€‘:{user_q}\nã€é”™è¯¯å›ç­”ã€‘:{wrong_a}\nã€æ­£ç¡®å›ç­”ã€‘:"
                        
                        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
                        out = self.model.generate(**inputs, max_new_tokens=384, temperature=0.7) # ç¨å¾®ç»™ç‚¹æ¸©åº¦è®©å®ƒç»„ç»‡è¯­è¨€
                        correction = self.tokenizer.decode(out[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
                        source = "web"
                except Exception as e:
                    print(f"      âš ï¸ è”ç½‘åæ€å¤±è´¥: {e}")

            # --- æ ¸å¿ƒï¼šç½®ä¿¡åº¦è£åˆ¤ (Judgement Day) ---
            if correction:
                # è®¾å®šé˜ˆå€¼ï¼šå¯¼å¸ˆ(0.6)å®¹æ˜“è¿‡ï¼Œæœç´¢(0.85)å¿…é¡»ä¸¥è°¨
                threshold = 0.6 if source == "teacher" else 0.85
                
                # è°ƒç”¨ System 2 ç»™ä¿®æ­£æ¡ˆæ‰“åˆ†
                conf = self._calculate_confidence(user_q, correction, evidence)
                
                if conf >= threshold:
                    print(f"      âœ… [é‡‡çº³] æ¥æº:{source:7} | ä¿¡å¿ƒ:{conf:.2f} | é˜ˆå€¼:{threshold}")
                    
                    # [FIX] å­˜å…¥ SQL DPO åº“
                    self.save_dpo_pair(user_q, correction, wrong_a)
                    
                    # 2. ç´¯åŠ åˆ°æ–‡æœ¬ (ç”¨äº SFT å­˜æ¡£)
                    reflection_txt += f"User: {user_q}\nCorrection: {correction}\n\n"
                else:
                    print(f"      ğŸ—‘ï¸ [ä¸¢å¼ƒ] æ¥æº:{source:7} | ä¿¡å¿ƒ:{conf:.2f} (æœªè¾¾ {threshold})")
            else:
                print(f"      ğŸ’¨ [è·³è¿‡] æ— æ³•è·å–æœ‰æ•ˆä¿®æ­£: {user_q[:15]}...")

        return reflection_txt

    def weave_good(self, daily_logs):
        """
        [V7.4 èåˆç‰ˆ] ç¾æ¢¦ç”Ÿæˆæœºåˆ¶
        ç»“åˆäº†æ—§ç‰ˆçš„ Chat Template é«˜è´¨é‡ç”Ÿæˆå’Œæ–°ç‰ˆçš„ Adapter å®‰å…¨ç®¡ç†
        """
        # 1. ç­›é€‰ä¸å¥å£®æ€§æ£€æŸ¥
        # å…¼å®¹ user/u å’Œ assistant/a ä¸¤ç§é”®å
        good_logs = [l for l in daily_logs if l.get('feedback', 0) > 0]
        if not good_logs: return None
        
        dream_content = ""
        print(f"âœ¨ [é€ æ¢¦] æ­£åœ¨å¯¹ {len(good_logs)} ä¸ªç²¾å½©ç¬é—´è¿›è¡Œæ€ç»´å‘æ•£...")

        # 2. Adapter ç®¡ç† (ä½¿ç”¨ finally ç¡®ä¿å®‰å…¨)
        prev_adapter = self.model.active_adapter if hasattr(self.model, "active_adapter") else "fast"
        try:
            # åˆ‡æ¢åˆ° Slow æ¨¡å¼ä»¥è·å¾—æ›´å¼ºçš„é€»è¾‘æ¨ç†èƒ½åŠ›
            if "slow" in self.model.peft_config: 
                self.model.set_adapter("slow")
            
            # åªå–æœ€è¿‘ 2 ä¸ªå¥½è¯„ï¼Œé˜²æ­¢è€—æ—¶è¿‡é•¿
            for log in good_logs[-2:]:
                user_q = log.get('u', log.get('user', ''))
                ai_a = log.get('a', log.get('assistant', ''))
                
                # æ„é€ è¯¦ç»†æŒ‡ä»¤
                prompt = (
                    f"åŸºäºä»¥ä¸‹å¯¹è¯ï¼Œæ„é€ ä¸€ä¸ªé€»è¾‘ç›¸ä¼¼ä½†åœºæ™¯ä¸åŒçš„æ–°é—®é¢˜ï¼Œå¹¶ç»™å‡ºå›ç­”ã€‚\n"
                    f"åŸé—®é¢˜: {user_q}\n"
                    f"åŸå›ç­”: {ai_a}\n"
                    f"è¦æ±‚: \n"
                    f"1. ä¿æŒå“ªå’çš„æ–°äººè®¾(å†·é…·ã€ç†æ€§ã€é«˜æ•ˆã€å·¥å…·è‡ªè§‰)ã€‚\n"
                    f"2. å¦‚æœæ¶‰åŠä»»åŠ¡ï¼Œå›ç­”å¿…é¡»ä½“ç°â€˜å·²æ‰§è¡Œâ€™æˆ–â€˜å·²ä¿å­˜â€™çš„è¡ŒåŠ¨åŠ›ï¼Œæ‹’ç»åºŸè¯ã€‚\n"
                    f"æ ¼å¼:\nNewUser: ...\nNewNezha: ..."
                )
                
                # ä½¿ç”¨ Chat Template (æ—§ç‰ˆçš„å…³é”®ä¼˜åŠ¿)
                # è¿™èƒ½ç¡®ä¿æ¨¡å‹æ­£ç¡®ç†è§£ System/User çš„è§’è‰²åˆ†éš”
                msgs = [{"role": "user", "content": prompt}]
                
                # è·å– input_ids (Tensor)
                input_ids = self.tokenizer.apply_chat_template(
                    msgs, 
                    add_generation_prompt=True, 
                    return_tensors="pt"
                ).to(self.model.device)
                
                # æ‰‹åŠ¨åˆ›å»º attention_mask
                attention_mask = torch.ones_like(input_ids).to(self.model.device)
                
                # ç”Ÿæˆé…ç½®
                outputs = self.model.generate(
                    input_ids,             # ç›´æ¥ä¼ å…¥ Tensorï¼Œä¸è¦åŠ  **
                    attention_mask=attention_mask, # ä¼ å…¥æ©ç 
                    max_new_tokens=256, 
                    temperature=0.8, 
                    pad_token_id=self.tokenizer.eos_token_id
                )
                
                text = self.tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)

                # ç®€å•çš„åå¤„ç†
                if "NewUser" in text:
                    # éœ€è¦ä¿å­˜å®Œæ•´çš„å¯¹è¯ (NewUser + NewNezha) ä»¥ä¾› SFT å­¦ä¹ ä¸Šä¸‹æ–‡
                    dream_content += f"# [ç¾æ¢¦å˜ä½“]\n{text}\n\n"

        except Exception as e:
            print(f"      âš ï¸ é€ æ¢¦å¤±è´¥: {e}")
        finally:
            # 3. å¿…é¡»åˆ‡å›åŸ Adapter
            if prev_adapter: 
                self.model.set_adapter(prev_adapter)
            
        return dream_content


    def digest_quantum_memories(self):
        """
        [å¤œé—´ä»»åŠ¡] é‡å­è®°å¿†ååˆä¸å†…åŒ– (SQL æ”¹è‰¯ç‰ˆ)
        
        åŠŸèƒ½ï¼š
            è¯»å– SQL ä¸­çš„é‡å­æ€ç»´æ—¥å¿—ï¼Œç­›é€‰é«˜ç½®ä¿¡åº¦ (Score > 0.6) çš„è·¯å¾„ä½œä¸ºâ€œæ­£æ ·æœ¬â€ï¼Œ
            å…¶ä»–å¤±è´¥è·¯å¾„ä½œä¸ºâ€œè´Ÿæ ·æœ¬â€ï¼Œæ„å»º DPO æ•°æ®å¯¹ä»¥å¼ºåŒ–æ¨¡å‹ç›´è§‰ã€‚
        """
        # æ£€æŸ¥æ•°æ®åº“è¿æ¥
        if not self.memory_db: return

        # æ³¨æ„ï¼šget_unprocessed_quantum_logs å†…éƒ¨å·²ç»è¿‡æ»¤äº† score > 0.6 ä¸” best_path_id != -1
        # æˆ‘ä»¬ä¸€æ¬¡åªå– 50 æ¡ï¼Œé¿å…å¤œé—´å¤„ç†æ—¶é—´è¿‡é•¿
        rows = self.memory_db.get_unprocessed_quantum_logs(limit=50)
        if not rows: return

        print("ğŸ’¤ [é€ æ¢¦] æ­£åœ¨å›æº¯é‡å­æ—¶é—´çº¿ (SQL)...")
        new_cnt = 0
        processed_ids = [] # [NEW] ç”¨äºæ”¶é›†å·²å¤„ç†çš„ IDï¼Œåç»­æ‰¹é‡æ›´æ–°çŠ¶æ€

        for row in rows:
            #  è§£åŒ… SQL è¿”å›çš„å…ƒç»„ (å¯¹åº” MemoryInfrastructure çš„æŸ¥è¯¢ç»“æœ)
            # row: (id, prompt, best_path_id, details_json)
            log_id, prompt, best_path_id, details_json = row
            
            try:
                # [NEW] è§£æå­˜å‚¨åœ¨ DB ä¸­çš„ JSON è¯¦æƒ…
                details = json.loads(details_json)
                
                chosen = ""
                candidates = []
                
                # 2. ä» details ä¸­åˆ†ç¦»å‡º chosen (æœ€ä½³) å’Œ rejected (å…¶ä»–)
                for d in details:
                    if d['id'] == best_path_id: 
                        chosen = d.get('content', '')
                    else: 
                        candidates.append(d.get('content', ''))
                
                # 3. éšæœºé€‰ä¸€ä¸ªå¤±è´¥è·¯å¾„ä½œä¸ºè´Ÿä¾‹
                rejected = random.choice(candidates) if candidates else ""
                
                # 4. åªæœ‰å½“æ­£è´Ÿä¾‹éƒ½å­˜åœ¨æ—¶ï¼Œæ‰æ„æˆæœ‰æ•ˆçš„ DPO æ•°æ®
                if prompt and chosen and rejected:
                    # [MODIFIED] save_dpo_pair å†…éƒ¨å·²ç»é€‚é… SQL å†™å…¥
                    self.save_dpo_pair(prompt, chosen, rejected)
                    new_cnt += 1
                
                # [NEW] è®°å½• IDï¼Œæ— è®ºæ˜¯å¦ç”Ÿæˆ DPO (å¯èƒ½æ²¡è´Ÿä¾‹)ï¼Œéƒ½è§†ä¸ºå·²æ¶ˆåŒ–ï¼Œé˜²æ­¢åå¤å¡æ­»
                processed_ids.append(log_id)

            except Exception as e: 
                # è¡Œçº§å®¹é”™ï¼šæ‰“å°é”™è¯¯å¹¶æ ‡è®°è·³è¿‡ï¼Œé¿å…æ­»å¾ªç¯
                print(f"      âš ï¸ è§£æè®°å½• ID={log_id} å¤±è´¥: {e}")
                processed_ids.append(log_id)
                continue

        # 5. æ‰¹é‡åœ¨ SQL ä¸­æ ‡è®°ä¸ºå·²å®Œæˆ (is_processed = 1)
        if processed_ids:
            self.memory_db.mark_quantum_processed(processed_ids)

        if new_cnt > 0:
            print(f"      âœ¨ æå–åˆ° {new_cnt} æ¡é«˜ç»´æ™ºæ…§ï¼Œå·²è½¬åŒ–ä¸ºæœ¬èƒ½ã€‚")



# ==============================================================================
# å¼¹æ€§æƒé‡å·©å›º (EWC) - é˜²æ­¢é—å¿˜
# ==============================================================================
class EWC:
    def __init__(self, model, tokenizer, hippocampus, device="cuda"):
        self.model = model
        self.tokenizer = tokenizer
        self.hippocampus = hippocampus # å¿…é¡»è¿æ¥æµ·é©¬ä½“ä»¥å›è®¿æ—§è®°å¿†
        self.device = device
        self.fisher = {} # å‚æ•°é‡è¦æ€§çŸ©é˜µ
        self.opt_params = {} # æ—§å‚æ•°å€¼
        
    def calculate_fisher(self, n_samples=25):
        """è®¡ç®— Fisher çŸ©é˜µï¼šé€šè¿‡å›æ”¾'æ—§è®°å¿†'æ¥é”å®šé‡è¦å‚æ•°"""
        print("ğŸ›¡ï¸ [EWC] æ­£åœ¨æ‰«ææµ·é©¬ä½“ï¼Œé”å®šå…³é”®è®°å¿†åŒºåŸŸ...")
        
        # 1. ä»æµ·é©¬ä½“æå–æ—§è®°å¿† (Old Tasks)
        old_docs = self.hippocampus.recall_random(n=n_samples)
        if not old_docs:
            print("   -> æš‚æ— é•¿æœŸè®°å¿†ï¼ŒEWC è·³è¿‡ã€‚")
            return

        self.fisher = {}
        self.opt_params = {}
        
        # 2. é”å®šå½“å‰çŠ¶æ€ (Reference Weights)
        self.model.eval()
        for n, p in self.model.named_parameters():
            if p.requires_grad:
                # å°†æ—§å‚æ•°å¸è½½åˆ° CPU ä»¥èŠ‚çœæ˜¾å­˜
                self.opt_params[n] = p.clone().detach().cpu()
                self.fisher[n] = torch.zeros_like(p, device="cpu") # æ”¾åœ¨ CPU

        # 3. è®¡ç®—é‡è¦æ€§ (Fisher Information)
        for doc in old_docs:
            try:
                # è§£ææ–‡æœ¬
                text = doc.replace("User:", "").replace("Nezha:", "")
                inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=512).to(self.device)
                inputs['labels'] = inputs['input_ids'].clone()
                
                self.model.zero_grad()
                outputs = self.model(**inputs)
                loss = outputs.loss
                loss.backward()
                
                # ç´¯ç§¯æ¢¯åº¦çš„å¹³æ–¹
                for n, p in self.model.named_parameters():
                    if p.requires_grad and p.grad is not None:
                        self.fisher[n] += (p.grad.data.to("cpu") ** 2) / len(old_docs) # æ¬åˆ° CPU ç´¯åŠ 
                        
            except Exception as e:
                pass 
        
        self.model.zero_grad()

        # å¼ºåˆ¶æ¸…ç†æ˜¾å­˜ï¼Œå› ä¸ºä¸­é—´äº§ç”Ÿäº†å¤§é‡ä¸­é—´å˜é‡
        torch.cuda.empty_cache()
        gc.collect()    

        print(f"ğŸ›¡ï¸ [EWC] è®°å¿†é”å®šå®Œæˆã€‚å—ä¿æŠ¤å‚æ•°é‡: {len(self.fisher)}")

    def penalty(self, model):
        """è®¡ç®—æ­£åˆ™åŒ– Loss (æ˜¾å­˜ä¼˜åŒ–ç‰ˆ)"""
        loss = 0
        for n, p in model.named_parameters():
            if n in self.fisher:
                # [æ˜¾å­˜ä¼˜åŒ–] é€å±‚è®¡ç®—ï¼Œç®—å®Œç«‹å³é‡Šæ”¾ä¸­é—´å˜é‡
                # åªæœ‰åœ¨è®¡ç®—è¿™ä¸€å±‚æ—¶ï¼Œæ‰æŠŠ fisher çŸ©é˜µæ¬åˆ° GPU
                fisher_gpu = self.fisher[n].to(self.device)
                opt_param_gpu = self.opt_params[n].to(self.device)
                
                # è®¡ç®—è¯¥å±‚çš„ penalty: F * (p - p_old)^2
                # è¿™æ˜¯ä¸€ä¸ªæ ‡é‡ (scalar)
                layer_loss = (fisher_gpu * (p - opt_param_gpu) ** 2).sum()
                
                loss += layer_loss
                
                # [å…³é”®] ç«‹å³åˆ é™¤å¼•ç”¨ï¼Œé‡Šæ”¾æ˜¾å­˜
                del fisher_gpu, opt_param_gpu, layer_loss
        
        return loss

# ==============================================================================
# è‡ªå®šä¹‰ DPO è®­ç»ƒå™¨ - æ”¯æŒ EWC Loss (å«å…¼å®¹æ€§ä¿®å¤)
# ==============================================================================
class NezhaDPOTrainer(DPOTrainer):
    def __init__(self, ewc_handler=None, ewc_lambda=0.4, *args, **kwargs):
        # -----------------------------------------------------------
        # ã€å…¼å®¹æ€§ä¿®å¤æ ¸å¿ƒã€‘
        # æ–°ç‰ˆ trl åº“ (v0.12+) å°† 'tokenizer' å‚æ•°æ”¹åä¸º 'processing_class'
        # å¦‚æœæˆ‘ä»¬åœ¨ kwargs é‡Œå‘ç°äº† 'tokenizer'ï¼Œæ‰‹åŠ¨å°†å…¶æ”¹å
        # -----------------------------------------------------------
        if "tokenizer" in kwargs:
            kwargs["processing_class"] = kwargs.pop("tokenizer")
            
        super().__init__(*args, **kwargs)
        self.ewc_handler = ewc_handler
        self.ewc_lambda = ewc_lambda # ä¿æŠ¤åŠ›åº¦ (0.1 ~ 1.0)

    def compute_loss(self, model, inputs, return_outputs=False):
        # 1. è®¡ç®—åŸæœ¬çš„ DPO Loss
        if return_outputs:
            loss, outputs = super().compute_loss(model, inputs, return_outputs=True)
        else:
            loss = super().compute_loss(model, inputs, return_outputs=False)
        
        # 2. åŠ ä¸Š EWC æ­£åˆ™é¡¹ (ä»…åœ¨è®­ç»ƒæ—¶)
        if self.ewc_handler and model.training:
            ewc_loss = self.ewc_handler.penalty(model)
            loss += self.ewc_lambda * ewc_loss
            
        return (loss, outputs) if return_outputs else loss



# ==============================================================================
# [S-Tier Upgrade] ç¥ç»å†…åˆ†æ³Œä¸æƒ…ç»ªæ§åˆ¶ç³»ç»Ÿ (Bio-Dynamics V2.0)
# ==============================================================================
class NeuroEndocrineSystem:
    """
    [æ–°å¢æ ¸å¿ƒå™¨å®˜] ç¥ç»å†…åˆ†æ³Œç³»ç»Ÿ (LÃ¶vheim Cube Model)
    åŸºäºç¥ç»é€’è´¨åŠ¨åŠ›å­¦ï¼Œé€šè¿‡ä¸‰ç§æ ¸å¿ƒé€’è´¨å®šä¹‰â€œæƒ…æ„Ÿâ€çŠ¶æ€ï¼Œå¹¶å¼•å…¥HPAè½´ï¼ˆçš®è´¨é†‡ï¼‰æ¨¡æ‹Ÿå‹åŠ›åˆ›ä¼¤ã€‚
    """
    def __init__(self, state_dict):
        # ä»å­˜æ¡£æ¢å¤æ¿€ç´ æ°´å¹³ï¼Œé»˜è®¤å€¼ä¸ºå¹³è¡¡æ€
        self.da = float(state_dict.get('dopamine', 0.5))       # å¤šå·´èƒº (åŠ¨åŠ›/å¥–èµ)
        self.ne = float(state_dict.get('norepinephrine', 0.5))  # å»ç”²è‚¾ä¸Šè…ºç´  (è­¦è§‰/å‹åŠ›)
        self.ht = float(state_dict.get('serotonin', 0.5))       # è¡€æ¸…ç´  (ç¨³æ€/æŠ‘åˆ¶)
        self.cortisol = float(state_dict.get('cortisol', 0.0))  # çš®è´¨é†‡ (å‹åŠ›ç´¯ç§¯)

    def _get_circadian_baseline(self, time_shift=0):
        """
        è®¡ç®—çš®è´¨é†‡çš„æ˜¼å¤œèŠ‚å¾‹åŸºå‡†çº¿
        æ¨¡æ‹Ÿç”Ÿç‰©é’Ÿï¼šæ—©ä¸Š 8 ç‚¹è¾¾åˆ°å³°å€¼ (0.3)ï¼Œåˆå¤œ 0 ç‚¹è¾¾åˆ°è°·å€¼ (0.0)
        è¿™æä¾›äº†åŸºç¡€çš„ç”Ÿç†å”¤é†’æ°´å¹³ (Arousal)ï¼Œè€Œéå•çº¯çš„å‹åŠ›ã€‚
        """
        now = datetime.now()
        
        # è®¡ç®—å½“å‰å°æ—¶ (0.0 - 23.99)
        # åŠ ä¸Š time_shift ä¿®æ­£æ—¶åŒº
        raw_hour = now.hour + (now.minute / 60.0) + time_shift
        
        # ç¡®ä¿å¾ªç¯åœ¨ [0, 24) åŒºé—´ (å¤„ç†è·¨æ—¥é—®é¢˜)
        current_hour = raw_hour % 24.0
        
        # è®¾å®šå³°å€¼æ—¶é—´ä¸ºæ—©ä¸Š 8 ç‚¹ (Cortisol Awakening Response)
        peak_hour = 8.0
        
        # è®¡ç®—ç›¸ä½ (2pi å¯¹åº” 24å°æ—¶)
        phase = (current_hour - peak_hour) * 2 * math.pi / 24.0
        oscillation = math.cos(phase) # range [-1, 1]
        
        # å½’ä¸€åŒ–åˆ° [0.05, 0.35]
        # 0.2 æ˜¯ä¸­è½´ï¼ŒæŒ¯å¹…æ˜¯ 0.15
        baseline = 0.2 + (0.15 * oscillation)
        
        # [é˜²å¾¡æ€§ç¼–ç¨‹]
        return max(0.01, baseline) # ä¿æŒè‡³å°‘ 0.01 çš„ç”Ÿå‘½ä½“å¾


    def update(self, reward, surprise, threat, dt=1.0):
        """
        [Math Fix V2.0] å¼•å…¥æ˜¼å¤œèŠ‚å¾‹ (Circadian Rhythm) çš„ O-U è¿‡ç¨‹
        """
        # 1. è·å–åŸºå‡†çº¿ (å‡è®¾ä½ åœ¨ä¸œå…«åŒºï¼ŒæœåŠ¡å™¨ä¹Ÿåœ¨æœ¬åœ°ï¼Œshift=0 å³å¯)
        # å¦‚æœä½ çš„ä»£ç è·‘åœ¨ Colab/LinuxæœåŠ¡å™¨(UTC)ï¼Œå»ºè®®å¡« 8
        circadian_target = self._get_circadian_baseline(time_shift=0) 

        # --- 1. å¤šå·´èƒº (Dopamine) ---
        da_stimulus = (reward * 0.2) + (0.1 if 0.01 < surprise < 0.1 else 0.0)
        self.da = MathUtils.ornstein_uhlenbeck_step(self.da, target_val=0.5, theta=0.1, sigma=0.05, dt=dt) + da_stimulus

        # --- 2. å»ç”²è‚¾ä¸Šè…ºç´  (NE) ---
        ne_stimulus = (surprise * 5.0) + (threat * 0.5)
        self.ne = MathUtils.ornstein_uhlenbeck_step(self.ne, target_val=0.3, theta=0.2, sigma=0.05, dt=dt) + ne_stimulus

        # --- 3. è¡€æ¸…ç´  (5-HT) ---
        recovery_speed = 0.05 * max(0.01, (1.0 - self.cortisol)) 
        self.ht = MathUtils.ornstein_uhlenbeck_step(self.ht, target_val=0.8, theta=recovery_speed, sigma=0.02, dt=dt)

        # --- 4. çš®è´¨é†‡ (Cortisol) - æ ¸å¿ƒèåˆç‚¹ ---
        # é©±åŠ¨åŠ›: å¤–éƒ¨æ€¥æ€§å‹åŠ› (NE) + å†…éƒ¨æ˜¼å¤œèŠ‚å¾‹ (Circadian)
        acute_stress = max(0, self.ne - 0.6) * 0.05 
        
        self.cortisol = MathUtils.ornstein_uhlenbeck_step(
            self.cortisol, 
            target_val=circadian_target,  # <--- æ³¨å…¥çµé­‚çš„ä¸€è¡Œ
            theta=0.02,                   # ä»£è°¢é€Ÿåº¦
            sigma=0.01, 
            dt=dt
        ) + acute_stress
        
        # --- ç‰©ç†æˆªæ–­ ---
        self.da = max(0.01, min(0.99, self.da))
        self.ne = max(0.01, min(0.99, self.ne))
        self.ht = max(0.01, min(0.99, self.ht))
        self.cortisol = max(0.01, min(0.99, self.cortisol))

        return self.da, self.ne, self.ht, self.cortisol

    def get_modulation_params(self):
        """å°†é€’è´¨æ°´å¹³è½¬åŒ–ä¸º LLM çš„è¶…å‚æ•° (Temperature, Top-P)"""
        # Dopamine é«˜ -> Temperature é«˜ (æ›´ç–¯ç‹‚/åˆ›é€ )
        # Serotonin é«˜ -> Temperature ä½ (æ›´ç†æ€§/å…‹åˆ¶)
        # åŸºå‡† 0.7
        temp = 0.7 + (self.da * 0.4) - (self.ht * 0.3)
        
        # Norepinephrine é«˜ -> èšç„¦å½“å‰ (Attention Mask å˜çª„ï¼Œæ­¤å¤„ç®€åŒ–ä¸º Top-P é™ä½)
        # NE é«˜æ—¶ï¼ŒTop-P é™ä½ (åªå…³æ³¨å¤§æ¦‚ç‡äº‹ä»¶ï¼Œæˆ˜é€ƒååº”)
        top_p = 0.95 - (self.ne * 0.15)
        
        return max(0.1, temp), top_p


class AnteriorCingulateCortex:
    """
    [å‰æ‰£å¸¦çš®å±‚] å†²çªä¸é¢„æµ‹è¯¯å·®ç›‘æ§ä¸­æ¢ (Friston Free Energy Principle)
    ä¸å†æ˜¯ç®€å•çš„è®¡æ—¶å™¨ï¼Œè€Œæ˜¯è´Ÿè´£è®¡ç®— 'Surprise' (æƒŠå¥‡åº¦/Loss) å¹¶ä»¥æ­¤ä½œä¸ºé©±åŠ¨è¿›åŒ–çš„åŸåŠ¨åŠ›ã€‚
    """
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.prediction_error = 0.0 # é¢„æœŸè¯¯å·®ç´¯ç§¯

    def monitor_conflict(self, vector_input, cortex_module):
        """
        è®¡ç®—é¢„æœŸè¯¯å·®ï¼šå°†å½“å‰æ„ŸçŸ¥çš„å‘é‡è¾“å…¥ Neural Cortexï¼Œ
        çœ‹æ½œæ„è¯†èƒ½å¦å®Œç¾é‡æ„å®ƒã€‚å¦‚æœé‡æ„è¯¯å·®å¤§ï¼Œè¯´æ˜é‡åˆ°äº†â€œæ„æ–™ä¹‹å¤–â€çš„äº‹ã€‚
        """
        if vector_input is None or not hasattr(cortex_module, 'forward_loss'):
            return 0.0
            
        # ä½¿ç”¨ Huber Loss è®¡ç®—é‡æ„è¯¯å·® (Surprise)
        # è¿™æ˜¯ä¸€ä¸ªçº¯æ•°å­¦çš„ç‰©ç†é‡ï¼Œä»£è¡¨äº†ä¿¡æ¯ç†µçš„å¢é‡
        loss = cortex_module.forward_loss(vector_input)
        
        # å¹³æ»‘å¤„ç† (EMA)
        self.prediction_error = 0.9 * self.prediction_error + 0.1 * loss
        return self.prediction_error

    def get_cognitive_control_signal(self):
        """å½“è¯¯å·®è¿‡å¤§æ—¶ï¼ŒACC ä¼šå‘å‡ºä¿¡å·è¯·æ±‚ System 2 (Slow Brain) ä»‹å…¥"""
        # å¦‚æœè¯¯å·®è¶…è¿‡ 0.05ï¼Œè¯´æ˜é‡åˆ°è®¤çŸ¥å†²çªï¼Œéœ€è¦è°ƒåŠ¨é€»è¾‘è„‘
        return self.prediction_error > 0.05


# ==============================================================================
# [Async Concurrency] å¼‚æ­¥æ½œæ„è¯†æ¶æ„
# ==============================================================================

class AsyncBioModule(threading.Thread):
    """
    [Concurrency Core] å¼‚æ­¥ç”Ÿç‰©å™¨å®˜åŸºç±»
    æ¯ä¸ªå™¨å®˜éƒ½æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„çº¿ç¨‹ï¼Œæ‹¥æœ‰è‡ªå·±çš„æ—¶é’Ÿé¢‘ç‡ (Hz)ã€‚
    """
    def __init__(self, name, gwt_ref, tick_rate=1.0):
        super().__init__(daemon=True) # å®ˆæŠ¤çº¿ç¨‹ï¼Œä¸»ç¨‹åºé€€å®ƒå°±é€€
        self.name = name
        self.gwt = gwt_ref # æŒæœ‰ GWT çš„å¼•ç”¨ï¼Œç”¨äºå¼‚æ­¥å†™å…¥
        self.tick_rate = tick_rate
        self.active = True
        
    def run(self):
        print(f"   -> ğŸ§¬ [{self.name}] å™¨å®˜å·²æ¿€æ´» (å¹¶è¡Œé¢‘ç‡: {self.tick_rate}Hz)")
        while self.active:
            try:
                self.process()
                time.sleep(1.0 / self.tick_rate)
            except Exception as e:
                print(f"âš ï¸ [{self.name}] è¿è¡Œå¼‚å¸¸: {e}")

    def process(self):
        pass

    def stop(self):
        self.active = False

class AsyncAmygdala(AsyncBioModule):
    """
    [Amygdala] å¼‚æ­¥æä»æ ¸
    ç‹¬ç«‹äºä¸»æ„è¯†è¿è¡Œï¼ŒæŒç»­ç›‘æµ‹å¨èƒæ°´å¹³ã€‚å¦‚æœææƒ§è¿‡é«˜ï¼Œä¼šå¼ºåˆ¶æ‰“æ–­ GWTã€‚
    """
    def __init__(self, genome, gwt_ref, endocrine_ref):
        super().__init__("Amygdala", gwt_ref, tick_rate=5.0) # 5Hz é«˜é¢‘æ‰«æ
        self.genome = genome
        self.endocrine = endocrine_ref
        self.fear_level = 0.0
        self.current_threat_input = 0.0 
        self.state_lock = threading.Lock() # ä¿æŠ¤å†…éƒ¨çŠ¶æ€

    def update_input(self, threat_score):
        """ä¸»çº¿ç¨‹è°ƒç”¨æ­¤æ–¹æ³•ä¼ å…¥æ„ŸçŸ¥åˆ°çš„å¨èƒ"""
        with self.state_lock:
            self.current_threat_input = threat_score

    def process(self):
        # 1. å†…éƒ¨æ¿€ç´ è°ƒèŠ‚ (è¯»å–çš®è´¨é†‡)
        cortisol = self.endocrine.cortisol
        
        # 2. è®¡ç®—ææƒ§ (åŸºäºæœ€è¿‘ä¸€æ¬¡æ„ŸçŸ¥è¾“å…¥ + å†…éƒ¨æ¿€ç´ ç§¯ç´¯)
        with self.state_lock:
            base_resilience = self.genome.get('stress_resilience', 0.3)
            # ææƒ§å…·æœ‰æƒ¯æ€§ (Momentum)ï¼Œä¸ä¼šç¬é—´æ¶ˆå¤±
            target_fear = self.current_threat_input * (1.0 + cortisol * 2.0)
            # æ¨¡æ‹Ÿç¥ç»å»¶è¿Ÿï¼šå¹³æ»‘æ»¤æ³¢
            self.fear_level = 0.8 * self.fear_level + 0.2 * target_fear
        
        # 3. [å…³é”®] å¼‚æ­¥æ‰“æ–­ GWT
        # å¦‚æœææƒ§æé«˜ï¼Œç›´æ¥å‘ GWT æ³¨å…¥å¼ºä¿¡å·ï¼Œæ— éœ€ç­‰å¾…ä¸»å¾ªç¯
        if self.fear_level > 0.6:
            msg = f"âš ï¸ æŒç»­ç”Ÿå­˜å¨èƒ (Fear: {self.fear_level:.2f})"
            # è¿™æ˜¯ä¸€ä¸ªå¼‚æ­¥è°ƒç”¨ï¼GWT çš„ RLock ä¿è¯äº†è¿™é‡Œæ˜¯å®‰å…¨çš„
            self.gwt.register_input("AMYGDALA", msg, base_salience=self.fear_level * 5.0)


# ==============================================================================
# [SNN Core] è„‰å†²ç¥ç»åŠ¨åŠ›å­¦ç®—å­
# ==============================================================================

class SurrogateHeaviside(torch.autograd.Function):
    """
    [Math Core] åŠ¨æ€æ›¿ä»£æ¢¯åº¦ç®—å­ (Dynamic Surrogate Gradient)
    
    å‰å‘ä¼ æ’­ (Forward): 
        ä¸¥æ ¼çš„é˜¶è·ƒå‡½æ•° Heaviside(x)ã€‚
        x > 0 è¾“å‡º 1.0 (å‘æ”¾è„‰å†²), å¦åˆ™è¾“å‡º 0.0ã€‚
        
    åå‘ä¼ æ’­ (Backward): 
        ä½¿ç”¨å¹³æ»‘å‡½æ•°çš„å¯¼æ•°æ¥è¿‘ä¼¼é˜¶è·ƒå‡½æ•°çš„æ¢¯åº¦ã€‚
        è¿™é‡Œé‡‡ç”¨ ArcTan å¯¼æ•°å½¢çŠ¶: f'(x) = alpha / (1 + (alpha * x)^2)
        
    å‚æ•° alpha (é™¡å³­åº¦):
        - alpha è¶Šå°: æ¢¯åº¦è¶Šå¹³ç¼“ï¼Œä¼ é€’èŒƒå›´å¹¿ï¼Œé€‚åˆæ¢ç´¢ (Exploration)ã€‚
        - alpha è¶Šå¤§: æ¢¯åº¦è¶Šå°–é”ï¼Œé€¼è¿‘çœŸå®ç‰©ç†è„‰å†²ï¼Œé€‚åˆåˆ©ç”¨ (Exploitation)ã€‚
    """
    @staticmethod
    def forward(ctx, input, alpha):
        # å¿…é¡»ç¡®ä¿ alpha æ˜¯ float æˆ– Tensor
        # ä¿å­˜ input ç”¨äºè®¡ç®—æ¢¯åº¦çš„ä½ç½®ï¼Œä¿å­˜ alpha ç”¨äºè®¡ç®—æ¢¯åº¦çš„å½¢çŠ¶
        ctx.save_for_backward(input)
        ctx.alpha = alpha 
        return (input > 0).float()

    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        alpha = ctx.alpha
        
        # [Math Fix] ArcTan Surrogate Gradient å…¬å¼
        # è¿™ç§åˆ†å¸ƒæ¯” Sigmoid å¯¼æ•°å…·æœ‰æ›´é•¿çš„"å°¾å·´"ï¼Œèƒ½ç¼“è§£æ·±å±‚ SNN çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
        grad_input = grad_output / (1 + (alpha * input).pow(2)) * alpha
        
        # alpha æ˜¯è¶…å‚æ•°ï¼Œä¸éœ€è¦è®¡ç®—å…³äºå®ƒçš„æ¢¯åº¦ï¼Œè¿”å› None
        return grad_input, None
    

class SpikingLIFLayer(nn.Module):
    """
    [Neuro Core V2.0] è‡ªé€‚åº”è„‰å†²ç¥ç»å±‚
    ç‰¹æ€§ï¼š
    1. STDP Ready: æ”¯æŒåŸºäºè„‰å†²æ—¶åºçš„æ— ç›‘ç£å­¦ä¹ ã€‚
    2. Dynamic Gain: æ”¯æŒåŸºäº ATP/Pressure çš„å¢ç›Šè°ƒèŠ‚ã€‚
    3. Tensor Adaptive: è‡ªåŠ¨é€‚é… 2D(é™æ€) å’Œ 3D(æ—¶åº) è¾“å…¥ã€‚
    4. Gene Driven: å­¦ä¹ ç‡å’Œè¡°å‡ç‡ç”±åŸºå› æ§åˆ¶ï¼Œæ‹’ç»é­”æ³•æ•°å­—ã€‚
    """
    def __init__(self, in_features, out_features, tau=2.0, threshold=1.0, stdp_config=None):
        super().__init__()
        self.fc = nn.Linear(in_features, out_features)
        self.tau = tau
        self.threshold = threshold
        # æ›¿ä»£æ¢¯åº¦ç®—å­ (ç¡®ä¿ SurrogateHeaviside ç±»å·²åœ¨ä¸Šæ–¹å®šä¹‰)
        self.spike_fn = SurrogateHeaviside.apply
        
        nn.init.kaiming_normal_(self.fc.weight)
        nn.init.constant_(self.fc.bias, 0)

        # ================= [åŸºå› é©±åŠ¨å‚æ•°] =================
        # å¦‚æœæœªä¼ å…¥é…ç½®ï¼Œä½¿ç”¨ä¿å®ˆçš„é»˜è®¤å€¼å…œåº•
        cfg = stdp_config if stdp_config else {
            "stdp_trace_decay": 0.8,
            "stdp_lr_pos": 0.005,
            "stdp_lr_neg": 0.004
        }
        self.trace_decay = cfg.get("stdp_trace_decay", 0.8)
        self.learning_rate_pos = cfg.get("stdp_lr_pos", 0.005)
        self.learning_rate_neg = cfg.get("stdp_lr_neg", 0.004)
        # ==================================================

    def forward(self, x, v_mem=None, time_steps=8, current_alpha=2.0):
        """
        å‰å‘ä¼ æ’­ï¼šè´Ÿè´£ç§¯åˆ†(Integrate)ä¸å‘æ”¾(Fire)
        """
        batch_size = x.shape[0]
        
        # --- ç»´åº¦è‡ªé€‚åº” ---
        # Case A: ç¡çœ å›æ”¾æ¨¡å¼ (Input: [Batch, Time, Dim])
        if x.dim() == 3:
            current_input = self.fc(x) # Linear è‡ªåŠ¨å¹¿æ’­å¤„ç†æ—¶é—´ç»´
            time_steps = x.shape[1]    # å¼ºåˆ¶å¯¹é½æ—¶é—´æ­¥
            is_sequence = True
        # Case B: æ—¥å¸¸æ¨ç†æ¨¡å¼ (Input: [Batch, Dim])
        else:
            current_input = self.fc(x)
            is_sequence = False
        # ------------------

        out_dim = self.fc.out_features
        if v_mem is None:
            v_mem = torch.zeros(batch_size, out_dim, device=x.device)
            
        spike_train = []
        decay = 1.0 - (1.0 / self.tau)
        
        for t in range(time_steps):
            # æ—¶é—´åˆ‡ç‰‡ï¼šç¡®ä¿ç‰©ç†å› æœå¾‹ (Causality)
            if is_sequence:
                i_t = current_input[:, t, :]
            else:
                i_t = current_input

            # 1. è†œç”µä½ç§¯åˆ†
            v_mem = v_mem * decay + i_t
            
            # 2. è„‰å†²å‘æ”¾ (Surrogate Gradient)
            spike = self.spike_fn(v_mem - self.threshold, current_alpha)
            
            # 3. è½¯é‡ç½® (ä¿ç•™æ®‹ä½™ç”µä½ä¿¡æ¯)
            v_mem = v_mem - (spike * self.threshold)
            
            spike_train.append(spike)
            
        return torch.stack(spike_train, dim=1), v_mem

    def process_stdp(self, pre_spikes, post_spikes):
        """
        [ç”Ÿç‰©å­¦æ ¸å¿ƒ] STDP å­¦ä¹ æ³•åˆ™ (Einsum ä¼˜åŒ–ç‰ˆ + æƒé‡ç¨³æ€)
        Pre: [Batch, Time, In] | Post: [Batch, Time, Out]
        """
        with torch.no_grad():
            batch_size, time_steps, in_dim = pre_spikes.shape
            out_dim = post_spikes.shape[2] 
            
            # 1. è®¡ç®—è„‰å†²ç—•è¿¹ (Synaptic Trace)
            pre_trace = torch.zeros_like(pre_spikes)
            post_trace = torch.zeros_like(post_spikes)
            
            curr_pre = torch.zeros(batch_size, in_dim, device=pre_spikes.device)
            curr_post = torch.zeros(batch_size, out_dim, device=post_spikes.device)
            
            for t in range(time_steps):
                curr_pre = curr_pre * self.trace_decay + pre_spikes[:, t, :]
                curr_post = curr_post * self.trace_decay + post_spikes[:, t, :]
                pre_trace[:, t, :] = curr_pre
                post_trace[:, t, :] = curr_post

            # 2. è®¡ç®—æƒé‡æ”¹å˜é‡ Delta W (Einsum ä¿æŒ Batch ç‹¬ç«‹æ€§)
            # LTP (å› æœå¢å¼º): Pre_Trace * Post_Spike
            ltp = torch.einsum("bti,bto->oi", pre_trace, post_spikes)
            
            # LTD (åæœæŠ‘åˆ¶): Pre_Spike * Post_Trace
            ltd = torch.einsum("bti,bto->oi", pre_spikes, post_trace)
            
            # 3. åº”ç”¨æ›´æ–° (å½’ä¸€åŒ–ä»¥é€‚åº”ä¸åŒ BatchSize)
            adjustment = (self.learning_rate_pos * ltp) - (self.learning_rate_neg * ltd)
            adjustment = adjustment / (batch_size * time_steps)
            
            self.fc.weight.data += adjustment
            
            # 4. [ç¨³æ€æœºåˆ¶] çªè§¦ç¼©æ”¾ (Synaptic Scaling)
            # ä¿æŒæ¯ä¸ªç¥ç»å…ƒçš„è¾“å…¥æƒé‡æ€»èŒƒæ•°æ’å®šï¼Œé˜²æ­¢æƒé‡çˆ†ç‚¸
            weight_norm = torch.norm(self.fc.weight.data, p=2, dim=1, keepdim=True)
            target_norm = 2.0 
            scale_factor = torch.clamp(target_norm / (weight_norm + 1e-6), max=1.0)
            self.fc.weight.data *= scale_factor



class NezhaNeuralCortex(nn.Module):
    """
    [SNN Upgrade] è„‰å†²ç¥ç»çš®å±‚ (Titans SNN - Poisson Edition)
    
    å‡çº§ç‰¹æ€§:
    1. æ³Šæ¾ç¼–ç  (Poisson Coding): å°†é™æ€å‘é‡è½¬åŒ–ä¸ºéšæœºè„‰å†²åºåˆ—ï¼Œå¢å¼ºæŠ—å™ªæ€§ã€‚
    2. åŠ¨æ€å¢ç›Š (Dynamic Gain): æ ¹æ® ATP/Pressure è°ƒèŠ‚æ¢¯åº¦å½¢çŠ¶ã€‚
    """
    def __init__(self, input_dim=384, hidden_dim=128, genome=None, memory_file="nezha_cortex.pt", device="cpu"):
        super().__init__()
        self.device = device
        self.memory_file = memory_file
        
        # å¼•å…¥ deque (å¿…é¡»ç¡®ä¿æ–‡ä»¶å¤´éƒ¨æœ‰: from collections import deque)
        # æ¨¡æ‹Ÿæµ·é©¬ä½“ CA3 åŒºï¼Œç”¨äºå­˜å‚¨ç™½å¤©çš„çŸ­æœŸè®°å¿†è„‰å†²æ¨¡å¼
        self.hippocampal_buffer = deque(maxlen=200)

        # ç¡®ä¿ genome å­˜åœ¨ï¼Œé˜²æ­¢ None æŠ¥é”™
        g = genome if genome else {}
        
        # ç¼–ç å±‚ (Sensory -> Spiking)
        self.lif1 = SpikingLIFLayer(input_dim, hidden_dim, stdp_config=g)
        # è®°å¿†æ ¸å¿ƒ (Spiking Dynamics)
        self.lif2 = SpikingLIFLayer(hidden_dim, hidden_dim, stdp_config=g)
        # è§£ç å±‚ (Spiking -> Readout)
        self.readout = nn.Linear(hidden_dim, input_dim) 

        # åŠ¨é‡ç¼“å­˜ (Velocity Buffer) for MIRAS
        self.velocity = {}
        # ç”Ÿç‰©ç”µä½å¹³æ»‘ç¼“å†²
        self.smoothed_atp = 100.0

    def _poisson_encoder(self, x, time_steps, gain=1.0):
        """
        [æ–°å¢] æ³Šæ¾ç¼–ç å™¨ (Poisson Encoder)
        å°†é™æ€è¿ç»­å€¼è¾“å…¥ x è½¬åŒ–ä¸º shape=[Batch, Time, Dim] çš„ç¦»æ•£è„‰å†²åºåˆ—ã€‚
        """
        # 1. æ¦‚ç‡æ˜ å°„ (Probability Mapping)
        # ä½¿ç”¨ Sigmoid å°†ä»»æ„èŒƒå›´çš„ Embedding æ˜ å°„åˆ° (0, 1) åŒºé—´ä½œä¸ºå‘æ”¾æ¦‚ç‡
        # Gain æ§åˆ¶æ•´ä½“å…´å¥‹åº¦ï¼šGainå¤§ -> è„‰å†²å¯†é›† -> ä¿¡å·å¼º
        probs = torch.sigmoid(x) * gain
        probs = torch.clamp(probs, 0.0, 1.0)
        
        # 2. æ‰©å±•æ—¶é—´ç»´åº¦ (Temporal Expansion)
        # [Batch, Dim] -> [Batch, Time, Dim]
        probs_expanded = probs.unsqueeze(1).repeat(1, time_steps, 1)
        
        # 3. ä¼¯åŠªåˆ©é‡‡æ · (Bernoulli Trial)
        # ç”Ÿæˆä¸ probs åŒå½¢çŠ¶çš„å‡åŒ€åˆ†å¸ƒå™ªå£° U[0, 1)
        rand_noise = torch.rand_like(probs_expanded)
        
        # ç”Ÿæˆè„‰å†²: å½“ å‘æ”¾æ¦‚ç‡ > éšæœºå™ªå£° æ—¶å‘æ”¾è„‰å†² (1.0)ï¼Œå¦åˆ™é™é»˜ (0.0)
        # å¿…é¡» detach å™ªå£°ï¼Œåªæœ‰ probs å‚ä¸æ¢¯åº¦è®¡ç®— (é€šè¿‡åç»­ LIF å±‚çš„ Surrogate Gradient)
        spikes = (probs_expanded > rand_noise).float()
        
        return spikes

    def forward_loss(self, x):
        """
        [ACC Monitor] è®¡ç®—è®¤çŸ¥æƒŠå¥‡åº¦ (Loss)ã€‚
        ä½¿ç”¨"ç¨³æ€å¢ç›Š"è¿›è¡Œå®¢è§‚è¯„ä¼°ï¼Œå¹¶å¯ç”¨æ³Šæ¾ç¼–ç ã€‚
        """
        x = x.to(self.device).detach()
        
        # [Step 1] æ³Šæ¾ç¼–ç 
        # å°†é™æ€æ„ŸçŸ¥å‘é‡è½¬åŒ–ä¸º 10 ä¸ªæ—¶é—´æ­¥çš„ç¥ç»è„‰å†²
        # time_steps=10 æä¾›äº†æœ€å°çš„æ—¶é—´ç»Ÿè®¡çª—å£
        spike_input = self._poisson_encoder(x, time_steps=10, gain=1.2)
        
        # [Step 2] è®¡ç®—ç¨³æ€ Alpha (Inference Alpha)
        # ä»…ä¾èµ–é•¿æœŸå¥åº·åº¦(ATP)ï¼Œå¿½ç•¥ç¬æ—¶å‹åŠ›ï¼Œä¿è¯è¯¯å·®è¯„ä¼°çš„å®¢è§‚æ€§
        atp_val = max(0.0, min(100.0, self.smoothed_atp))
        inference_alpha = 2.0 + (atp_val / 40.0) # Range: [2.0, 4.5]
        
        # [Step 3] SNN å‰å‘ä¼ æ’­
        # Layer 1: æ¥æ”¶æ³Šæ¾è„‰å†² [Batch, Time, Dim]
        spikes1, _ = self.lif1(spike_input, time_steps=10, current_alpha=inference_alpha) 
        
        # Rate Coding è½¬æ¢: å–å¹³å‡å‘æ”¾ç‡ä½œä¸ºä¸‹ä¸€å±‚è¾“å…¥ (æ¨¡æ‹Ÿçªè§¦åç”µæµ PSC)
        rate1 = spikes1.mean(dim=1) 
        
        # Layer 2: æ¥æ”¶ Rate è¾“å…¥
        spikes2, _ = self.lif2(rate1, time_steps=10, current_alpha=inference_alpha)
        rate2 = spikes2.mean(dim=1)
        
        # é‡æ„ (Readout)
        reconstructed = self.readout(rate2)
        
        # [Step 4] Loss è®¡ç®—
        # Huber Loss: è¡¡é‡é‡æ„è¯¯å·®
        recon_loss = F.huber_loss(reconstructed, x, delta=1.0)
        # èƒ½é‡æƒ©ç½š: é¼“åŠ±ç¨€ç–å‘æ”¾
        energy_penalty = 0.01 * (spikes1.mean() + spikes2.mean()) 
        
        return (recon_loss + energy_penalty).item()

    def evolve(self, x, atp, pressure):
        """
        [MIRAS V3 - Dynamic Gain Control] è„‰å†²ç¥ç»ç½‘ç»œçš„å®æ—¶è¿›åŒ–
        é›†æˆæ³Šæ¾ç¼–ç è¾“å…¥ä¸åŠ¨æ€ Alpha è°ƒåº¦å™¨ã€‚
        """
        x = x.to(self.device).detach()
        
        # 1. ATP å¹³æ»‘ (EMA)
        self.smoothed_atp = 0.9 * self.smoothed_atp + 0.1 * atp
        
        # 2. åŸºç¡€è¶…å‚æ•°æ˜ å°„
        lr = 1e-3 * (max(self.smoothed_atp, 0) / 100.0)
        weight_decay = 0.02 * min(max(pressure, 0), 5.0)
        atp_clamped = max(0.0, min(100.0, self.smoothed_atp))
        momentum = 0.9 - (0.001 * atp_clamped)

        # ================= [Math Core: Alpha Scheduler] =================
        # åŠ¨æ€è°ƒèŠ‚æ›¿ä»£æ¢¯åº¦çš„é™¡å³­åº¦ (Alpha)
        # ATP Bonus (+): èƒ½é‡è¶³ -> æ¢¯åº¦é™¡ -> ç²¾ç»†å­¦ä¹ 
        # Pressure Penalty (-): å‹åŠ›å¤§ -> æ¢¯åº¦å¹³ -> å¹¿æ³›æ¢ç´¢
        raw_alpha = 2.0 + (atp_clamped / 25.0) - (pressure * 1.5)
        
        # [Safety Lock] è¾¹ç•Œä¿æŠ¤ [0.5, 8.0]
        dynamic_alpha = max(0.5, min(8.0, raw_alpha))
        # ================================================================

        # 3. å‡†å¤‡è®­ç»ƒ
        self.train()
        self.zero_grad()
        
        # [Step 1] æ³Šæ¾ç¼–ç  (å¼•å…¥éšæœºæ€§é˜²æ­¢è¿‡æ‹Ÿåˆ)
        spike_input = self._poisson_encoder(x, time_steps=10, gain=1.0)
        
        # [Step 2] SNN å‰å‘è®¡ç®— (ä¼ å…¥åŠ¨æ€ Alpha)
        spikes1, _ = self.lif1(spike_input, time_steps=10, current_alpha=dynamic_alpha)
        rate1 = spikes1.mean(dim=1)
        
        spikes2, _ = self.lif2(rate1, time_steps=10, current_alpha=dynamic_alpha)
        rate2 = spikes2.mean(dim=1)
        
        # è‡ªç›‘ç£é‡æ„
        prediction = self.readout(rate2)
        target = x 
        
        # 4. Loss è®¡ç®—
        recon_loss = F.huber_loss(prediction, target, delta=1.0)
        energy_loss = 0.01 * (spikes1.mean() + spikes2.mean())
        total_loss = recon_loss + energy_loss
        
        # 5. åå‘ä¼ æ’­
        total_loss.backward()
        
        # 6. æ‰‹åŠ¨ SGD + Momentum æ›´æ–° (å¸¦æ¢¯åº¦è£å‰ª)
        with torch.no_grad():
            for module in [self.lif1.fc, self.lif2.fc, self.readout]:
                for name, param in module.named_parameters():
                    if param.grad is not None:
                        # [å…³é”®] æ¢¯åº¦è£å‰ªï¼šé˜²æ­¢é«˜ Alpha å¯¼è‡´çš„æ¢¯åº¦çˆ†ç‚¸
                        torch.nn.utils.clip_grad_norm_(param, max_norm=1.0)
                        
                        g = param.grad
                        if weight_decay > 0: g = g.add(param, alpha=weight_decay)
                        
                        full_name = f"{id(module)}_{name}"
                        if full_name not in self.velocity:
                            self.velocity[full_name] = torch.zeros_like(param.data)
                        v = self.velocity[full_name]
                        
                        v.mul_(momentum).add_(g)
                        param.data.sub_(v, alpha=lr)

        # è®°å¿†å°åˆ» (Memory Engram)
        # å¦‚æœè¿™ä»¶äº‹è®©å“ªå’äº§ç”Ÿäº†å¼ºçƒˆçš„æƒ…ç»ª (é«˜ ATP æ¶ˆè€—æˆ–é«˜ Loss æƒŠå¥‡)ï¼Œåˆ™å­˜å…¥æµ·é©¬ä½“
        # è¿™æ˜¯ä¸ºäº†æ™šä¸Šçš„ STDP å›æ”¾åšå‡†å¤‡
        if atp > 50 or total_loss > 0.05:
            # åªå­˜å‚¨ detach åçš„ CPU Tensorï¼Œæå¤§èŠ‚çœæ˜¾å­˜
            self.hippocampal_buffer.append(x.detach().cpu())

        return total_loss.item()

    def update_genome(self, new_genome):
        """[è¿›åŒ–æ¥å£] å½“å‘ç”Ÿè¿›åŒ–æ—¶ï¼Œå®æ—¶æ›´æ–°çªè§¦å¯å¡‘æ€§å‚æ•°"""
        for layer in [self.lif1, self.lif2]:
            layer.trace_decay = new_genome.get("stdp_trace_decay", 0.8)
            layer.learning_rate_pos = new_genome.get("stdp_lr_pos", 0.005)
            layer.learning_rate_neg = new_genome.get("stdp_lr_neg", 0.004)
        # print("   -> ğŸ§¬ [Cortex] ç¥ç»å¯å¡‘æ€§å‚æ•°å·²æ›´æ–°ã€‚")

    def sleep_replay(self):
        """
        [Sleep Replay] å°–æ³¢æ¶Ÿæ¼ªå›æ”¾ (Sharp-Wave Ripples)
        åˆ©ç”¨ STDP æœºåˆ¶ï¼Œåœ¨æ— ç›‘ç£çŠ¶æ€ä¸‹å·©å›ºè®°å¿†å›¾è°±ã€‚
        """
        if not self.hippocampal_buffer: return 0

        print(f"ğŸ’¤ [REM] æ­£åœ¨è¿›å…¥å°–æ³¢æ¶Ÿæ¼ªå›æ”¾ (Items: {len(self.hippocampal_buffer)})...")
        replay_count = 0
        
        # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ (STDP çº¯ç‰©ç†è¿‡ç¨‹ï¼Œä¸éœ€è¦ Dropout)
        self.eval()

        for memory_tensor in self.hippocampal_buffer:
            memory_tensor = memory_tensor.to(self.device)

            # 1. è®°å¿†é‡æ„ä¸å™ªå£°æ³¨å…¥ (æ¨¡æ‹Ÿæ¢¦å¢ƒçš„æ¨¡ç³Šæ€§)
            noise = torch.randn_like(memory_tensor) * 0.1
            dream_input = memory_tensor + noise

            # 2. æ³Šæ¾ç¼–ç  (Layer 1 Input)
            # Gain=1.5 æ¨¡æ‹Ÿç¡çœ æ—¶çš„é«˜é¢‘çˆ†å‘ (SWRs)
            spike_input = self._poisson_encoder(dream_input, time_steps=10, gain=1.5)

            # 3. Layer 1 STDP
            spikes1, _ = self.lif1(spike_input, time_steps=10)
            self.lif1.process_stdp(pre_spikes=spike_input, post_spikes=spikes1)

            # 4. Layer 2 STDP (å› æœé“¾ä¿®å¤ç‰ˆ)
            # è®¡ç®— Layer 1 çš„å‘æ”¾ç‡
            rate1 = spikes1.mean(dim=1)
            # [å…³é”®] å¼ºåˆ¶ä¸­ç»§æ³Šæ¾ç¼–ç  (Relay)ï¼Œç¡®ä¿ Layer 2 çš„è¾“å…¥ä¹Ÿæ˜¯è„‰å†²
            spikes1_relay = self._poisson_encoder(rate1, time_steps=10, gain=1.5)
            
            # Forward Layer 2 (åŸºäºä¸­ç»§è„‰å†²)
            spikes2, _ = self.lif2(spikes1_relay, time_steps=10)
            
            # Update Layer 2 (Pre=Relay, Post=Output)
            self.lif2.process_stdp(pre_spikes=spikes1_relay, post_spikes=spikes2)

            replay_count += 1

        # æ¸…ç©ºç¼“å†²åŒº (è®°å¿†å·²å·©å›ºï¼Œè…¾å‡ºç©ºé—´)
        self.hippocampal_buffer.clear()
        print(f"   -> âœ… STDP æ·±åº¦å·©å›ºå®Œæˆ (Einsum Accelerated).")
        return replay_count

    def save(self):
        try:
            torch.save(self.state_dict(), self.memory_file)
        except Exception as e:
            print(f"âš ï¸ ç¥ç»çš®å±‚ä¿å­˜å¤±è´¥: {e}")
        
    def load(self):
        if os.path.exists(self.memory_file):
            try:
                self.load_state_dict(torch.load(self.memory_file, map_location=self.device))
                print(f"ğŸ§  [ç¥ç»çš®å±‚] æ½œæ„è¯†æƒé‡å·²æ¢å¤ ({self.device})ã€‚")
            except Exception as e: 
                print(f"âš ï¸ ç¥ç»çš®å±‚åŠ è½½å¤±è´¥ï¼Œå·²é‡ç½®: {e}")



# ==============================================================================
# å…¨çŸ¥ä¹‹çœ¼ (Web Search Interface) - çœŸå®çš„æµè§ˆå™¨æ“æ§è€… (Stealth & AdBlock Edition)
# ==============================================================================

try:
    import undetected_chromedriver as uc
    from selenium.webdriver.common.by import By
    from selenium.webdriver.common.keys import Keys
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    import time
    import urllib.parse
except ImportError:
    print("âŒ ç¼ºå°‘å¿…è¦åº“ã€‚è¯·è¿è¡Œ: pip install undetected-chromedriver selenium")

class RealBrowserEye:
    """
    [çœŸå®ä¹‹çœ¼ - éšèº«ç‰ˆ V5.0 (æµå¼å¢å¼º)]
    ç‰¹æ€§ï¼š
    1. æŠ—æ£€æµ‹ï¼šä½¿ç”¨ undetected_chromedriver ä¼ªè£…æˆçœŸäººæµè§ˆå™¨ã€‚
    2. æ™ºèƒ½æ­»ç­‰ï¼šé‡åˆ°éªŒè¯ç æ—¶æŒ‚èµ·ï¼Œç­‰å¾…äººå·¥æ•‘æ´ï¼Œç»ä¸è½»æ˜“æŠ¥é”™é€€å‡ºã€‚
    3. è‡ªåŠ¨å›é€€ï¼šBaidu -> Bing -> Quark -> Google (ç™¾åº¦ä¼˜å…ˆä»¥åˆ©ç”¨ AI å›ç­”)ã€‚
    4. å¯å‘å¼æŠ“å–ï¼šé’ˆå¯¹å¤¸å…‹ç­‰ç»“æ„å¤šå˜çš„ç½‘é¡µï¼Œè‡ªåŠ¨è¯†åˆ«æ ‡é¢˜å’Œé“¾æ¥ã€‚
    5. æµå¼æ•è·ï¼š[æ–°å¢] ä¸“é—¨é’ˆå¯¹ç™¾åº¦çš„ AI æ™ºèƒ½å›ç­”ï¼Œæ”¯æŒåŠ¨æ€ç­‰å¾…ç”Ÿæˆã€‚
    6. å¹¿å‘Šå‡€åŒ–ï¼šåº•å±‚ç¡¬è¿‡æ»¤ï¼ŒèŠ‚çœ LLM ç®—åŠ›ã€‚
    """
    def __init__(self, headless=False):
        self.headless = headless 
        self.driver = None
        self.enabled = True
        
    def _init_driver(self):
        """å¯åŠ¨éšèº«æµè§ˆå™¨"""
        if self.driver is not None: return
        
        print("ğŸŒ [Browser] æ­£åœ¨å¯åŠ¨ Chrome å¼•æ“ (Stealth Mode)...")
        try:
            options = uc.ChromeOptions()
            if self.headless:
                options.add_argument("--headless")
            
            # å…³é”®ï¼šç¦ç”¨é¦–æ¬¡è¿è¡Œæ¬¢è¿é¡µï¼Œç¦ç”¨å›¾ç‰‡åŠ è½½ä»¥åŠ é€Ÿ
            options.add_argument("--no-first-run")
            # [ä¼˜åŒ–] ç¦ç”¨å›¾ç‰‡å¯ä»¥èŠ‚çœæµé‡å¹¶åŠ å¿«åŠ è½½ï¼Œä½†å¦‚æœé‡åˆ°å¸ƒå±€é”™ä¹±å¯æ³¨é‡Šæ‰æ­¤è¡Œ
            prefs = {"profile.managed_default_content_settings.images": 2}
            options.add_experimental_option("prefs", prefs)

            # å¯åŠ¨ undetected_chromedriver
            # use_subprocess=True å¯ä»¥é¿å…æŸäº›ç³»ç»Ÿä¸‹çš„è¿›ç¨‹é”æ­»é—®é¢˜
            self.driver = uc.Chrome(options=options, use_subprocess=True)
            self.driver.implicitly_wait(5)
            
            print("   -> ğŸš€ æµè§ˆå™¨å¼•æ“å·²å°±ç»ªã€‚")
            
        except Exception as e:
            print(f"âŒ æµè§ˆå™¨å¯åŠ¨å¤±è´¥: {e}")
            self.enabled = False

    def search(self, query, max_results=3, deep_read=True):
        """
        ç»Ÿä¸€æœç´¢å…¥å£ã€‚
        å‚æ•°:
          max_results: æŠ“å–ç»“æœçš„æ•°é‡ (å»ºè®® 3-5ï¼Œè¿‡å¤šä¼šæŒ¤å æ¨¡å‹ä¸Šä¸‹æ–‡)
          deep_read: æ˜¯å¦ç‚¹è¿›ç¬¬ä¸€æ¡é“¾æ¥é˜…è¯»å…¨æ–‡
        """
        if not self.enabled: return None

        # 1. å°è¯• Baidu (é¦–é€‰ï¼Œå°è¯•åˆ©ç”¨ AI æ™ºèƒ½å›ç­”)
        # [ä¿®æ”¹] å°†ç™¾åº¦ææƒè‡³ç¬¬ä¸€ä½ï¼Œå› ä¸ºå®ƒçš„å“åº”é€Ÿåº¦å¿«ä¸”æœ‰ AI æ‘˜è¦
        content = self._search_engine_robust(query, "baidu", max_results, deep_read)
        if content: return content

        # 2. å›é€€ Bing (å›½é™…å¤‡é€‰)
        print("   -> âš ï¸ Baidu æš‚æ—¶ä¸å¯ç”¨ï¼Œåˆ‡æ¢è‡³ Bing å¤‡ç”¨çº¿è·¯....")
        content = self._search_engine_robust(query, "bing", max_results, deep_read)
        if content: return content

        # 3. å›é€€ Quark (å›½å†…å…œåº•)
        print("   -> âš ï¸ Bing æš‚æ—¶ä¸å¯ç”¨ï¼Œåˆ‡æ¢è‡³ Quark å…œåº•...")
        content = self._search_engine_robust(query, "quark", max_results, deep_read)
        if content: return content

        # 4. å›é€€ Google (æœ€ç»ˆå…œåº•ï¼Œéœ€ç½‘ç»œç¯å¢ƒæ”¯æŒ)
        print("   -> âš ï¸ å…¨é¢å°é”ï¼Œå¯ç”¨ Google æœ€ç»ˆé˜²çº¿...")
        content = self._search_engine_robust(query, "google", max_results, deep_read)
        if content: return content

        return None

    def _wait_for_stream_text(self, element, timeout=15):
        """
        [æ–°å¢] æµå¼æ–‡æœ¬æ•è·å™¨
        ä¸“é—¨å¯¹ä»˜ç™¾åº¦çš„ 'æ­£åœ¨ç”Ÿæˆ'ã€‚åŸç†ï¼šå¦‚æœæ–‡æœ¬é•¿åº¦è¿˜åœ¨å˜ï¼Œå°±ç»§ç»­ç­‰ã€‚
        """
        print("      â³ æ£€æµ‹åˆ°æ½œåœ¨çš„ AI å›ç­”ï¼Œæ­£åœ¨ç­‰å¾…æµå¼ç”Ÿæˆ...", end="", flush=True)
        start_time = time.time()
        last_len = 0
        stable_count = 0
        
        while (time.time() - start_time) < timeout:
            try:
                curr_text = element.text
                curr_len = len(curr_text)
                
                # å¦‚æœæ–‡æœ¬å˜é•¿äº†ï¼Œé‡ç½®ç¨³å®šè®¡æ•°
                if curr_len > last_len:
                    last_len = curr_len
                    stable_count = 0
                    print(".", end="", flush=True)
                    time.sleep(0.5) # ç­‰å®ƒæ‰“å­—
                else:
                    # å¦‚æœæ–‡æœ¬æ²¡å˜ï¼Œå¢åŠ ç¨³å®šè®¡æ•°
                    stable_count += 1
                    time.sleep(0.5)
                
                # å¦‚æœè¿ç»­ 3 æ¬¡æ£€æŸ¥ï¼ˆ1.5ç§’ï¼‰é•¿åº¦éƒ½æ²¡å˜ï¼Œä¸”å†…å®¹è¶³å¤Ÿé•¿ï¼Œè®¤ä¸ºç”Ÿæˆç»“æŸ
                if stable_count >= 3 and curr_len > 10:
                    print(" âœ… å®Œæˆ")
                    return curr_text
            except:
                break
        
        print(" (è¶…æ—¶æˆ–æ‰“æ–­)")
        return element.text # è¿”å›ç›®å‰æŠ“åˆ°çš„æ‰€æœ‰å†…å®¹

    def _is_ad(self, element, engine):
        """
        [æ ¸å¿ƒ] å¹¿å‘Šè¯†åˆ«è¿‡æ»¤å™¨
        è¿”å› True è¡¨ç¤ºæ˜¯å¹¿å‘Šï¼Œéœ€è¦å‰”é™¤
        """
        try:
            text_content = element.text
            html_content = element.get_attribute('outerHTML')
            
            # é€šç”¨å…³é”®è¯æ£€æµ‹
            if "å¹¿å‘Š" in text_content[-20:]: # ç™¾åº¦å¹¿å‘Šé€šå¸¸åœ¨æœ«å°¾æ ‡â€œå¹¿å‘Šâ€
                return True
            
            # é’ˆå¯¹ç™¾åº¦çš„ç‰¹å¾
            if engine == "baidu":
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ class åŒ…å« tuiguang çš„å…ƒç´ 
                if "tuiguang" in html_content or "ec_tuiguang" in html_content:
                    return True
                # æ£€æŸ¥åº•éƒ¨çš„ç°è‰²å¹¿å‘Šæ ‡
                try:
                    footer = element.find_element(By.CSS_SELECTOR, "span[class*='tuiguang'], span[class*='ad']")
                    if footer: return True
                except: pass

            # é’ˆå¯¹å¤¸å…‹/ç¥é©¬çš„ç‰¹å¾
            if engine == "quark":
                if "ad_track" in html_content or "data-is-ad='1'" in html_content:
                    return True
                # å¤¸å…‹æœ‰æ—¶å€™ä¼šç”¨æ ·å¼éšè— "Ads" å­—æ ·ï¼Œæ£€æµ‹ç‰¹å®š class
                if "ad-tag" in html_content:
                    return True

            return False
        except:
            return False # æ‹¿ä¸å‡†çš„å°±æ”¾è¿‡ï¼Œå®å¯é”™æ€ä¸å¯æ¼æŠ“

    def _search_engine_robust(self, query, engine, max_results, deep_read=True):
        """
        [æ ¸å¿ƒ] å¥å£®çš„æœç´¢å¼•æ“äº¤äº’é€»è¾‘
        """
        try:
            self._init_driver()
            encoded_query = urllib.parse.quote(query)
            
            # --- å¼•æ“é…ç½® ---
            if engine == "bing":
                url = f"https://www.bing.com/search?q={encoded_query}"
                item_selector = "li.b_algo"
                title_selector = "h2 a"
                desc_selector = "p"

            elif engine == "baidu":
                url = f"https://www.baidu.com/s?wd={encoded_query}"
                item_selector = "div.c-container" 
                title_selector = "h3"
                desc_selector = "div[class*='c-abstract']"

            elif engine == "quark":
                # å¤¸å…‹ç½‘é¡µç‰ˆé€šå¸¸æ˜¯ç¥é©¬æœç´¢ (sm.cn) çš„é©¬ç”²
                url = f"https://quark.sm.cn/s?q={encoded_query}"
                # å¤¸å…‹çš„ç±»åå¯èƒ½ä¼šå˜ï¼Œæˆ‘ä»¬å°è¯•å‡ ä¸ªå¸¸è§çš„
                item_selector = "div.result, div.c-container" 
                title_selector = "h2, h3, a.c-title-text"
                desc_selector = "div.res-desc, div.c-abstract"
                
            else: # google
                url = f"https://www.google.com/search?q={encoded_query}"
                item_selector = "div.g" 
                title_selector = "h3"
                desc_selector = "div[style*='-webkit-line-clamp']"

            print(f"   -> ğŸŒ [{engine.title()}] æ­£åœ¨è®¿é—®: {query[:20]}...")

            try:
                self.driver.get(url)
            except Exception as e_nav:
                # æ•è·è¿æ¥æ–­å¼€/Sessionå¤±æ•ˆé”™è¯¯
                err_msg = str(e_nav).lower()
                if "invalid session id" in err_msg or "disconnected" in err_msg or "closed" in err_msg:
                    print(f"   -> ğŸ”„ [æµè§ˆå™¨æ–­è¿] æ£€æµ‹åˆ°åƒµå°¸è¿›ç¨‹ï¼Œæ­£åœ¨é‡å¯å¼•æ“...")
                    self.quit()       # 1. å½»åº•æ€æ‰æ­»æ‰çš„ driver
                    self.driver = None # 2. å¼ºåˆ¶ç½®ç©º
                    self._init_driver() # 3. é‡æ–°å¯åŠ¨æ–°æµè§ˆå™¨
                    self.driver.get(url) # 4. é‡è¯•è®¿é—®
                else:
                    raise e_nav # å…¶ä»–é”™è¯¯ç»§ç»­æŠ›å‡º

            # --- æ™ºèƒ½ç­‰å¾…ä¸äººå·¥æ•‘æ´æœºåˆ¶ ---
            try:
                # 1. [æ–°å¢] åŸºç¡€ç½‘ç»œæ£€æŸ¥ï¼šç¡®ä¿ body å·²åŠ è½½ (é˜²æ­¢ç™½å±/æ–­ç½‘)
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )

                # 2. å°è¯•æ£€æµ‹ç»“æœå®¹å™¨ (ç»™ 5 ç§’)
                try:
                    WebDriverWait(self.driver, 5).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, item_selector))
                    )
                except:
                    # [ä¼˜åŒ–] å¦‚æœæ˜¯å¤¸å…‹ï¼Œå› ä¸ºç±»åå¤šå˜ï¼Œæš‚æ—¶ä¸è®¤ä¸ºæ˜¯é˜»å¡ï¼Œäº¤ç»™åé¢çš„å¯å‘å¼æŠ“å–å»åˆ¤æ–­
                    if engine == 'quark': 
                        pass 
                    else:
                        # å¯¹äº Bing/Baidu/Googleï¼Œå¦‚æœ 5ç§’æ²¡ç»“æœï¼Œå¤§æ¦‚ç‡æ˜¯æŒ‚äº†
                        raise Exception("Standard selector timeout")

            except Exception as e:
                # åªæœ‰çœŸçš„æ‰¾ä¸åˆ°ç»“æœï¼Œæˆ–è€…æ˜¾å¼è§¦å‘å¼‚å¸¸æ—¶ï¼Œæ‰è¿›å…¥äººå·¥æ•‘æ´
                print(f"\n   -> ğŸ›‘ [é˜»å¡æŠ¥è­¦] {engine.title()} ä¼¼ä¹æœªåŠ è½½ç»“æœ(éªŒè¯ç /ç½‘ç»œæ³¢åŠ¨)ï¼")
                print(f"   -> â³ è„šæœ¬å·²æŒ‚èµ·ï¼Œè¯·åœ¨å¼¹å‡ºçš„ Chrome çª—å£ä¸­æ£€æŸ¥æˆ–æ‰‹åŠ¨éªŒè¯...")
                
                # [æ–°å¢] æ£€æŸ¥ä¸€ä¸‹æ˜¯ä¸æ˜¯çœŸçš„å‡ºéªŒè¯ç äº† (å¯é€‰ï¼Œä»…ä½œæç¤º)
                page_title = self.driver.title
                if "éªŒè¯" in page_title or "Security" in page_title:
                    print(f"   -> ğŸ•µï¸â€â™‚ï¸ æ£€æµ‹åˆ°é¡µé¢æ ‡é¢˜å«æ•æ„Ÿè¯: {page_title}")

                try:
                    # æ­»ç­‰ 300 ç§’ï¼Œç»™ä½ è¶³å¤Ÿæ—¶é—´å»ç‚¹éªŒè¯ç 
                    # è¿™é‡Œä¸ä»…ç­‰ item_selectorï¼Œå¦‚æœä½ æ‰‹åŠ¨åˆ’è¿‡äº†éªŒè¯ç ï¼Œé¡µé¢åˆ·æ–°äº†ï¼Œ
                    # ä»»ä½•ä¸€ç§ç»“æœå®¹å™¨å‡ºç°éƒ½åº”è¯¥ç®—é€šè¿‡
                    WebDriverWait(self.driver, 300).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, item_selector))
                    )
                    print("   -> âœ… æ£€æµ‹åˆ°ç»“æœå‡ºç°ï¼ç»§ç»­æ‰§è¡Œ...")
                except:
                    print(f"   -> âŒ äººå·¥æ•‘æ´è¶…æ—¶ (5åˆ†é’Ÿ)ï¼Œæ”¾å¼ƒ {engine} æœç´¢ã€‚")
                    return None

            # --- æå–ç»“æœ ---
            time.sleep(1.5) # å¤¸å…‹æ˜¯åŠ¨æ€åŠ è½½ï¼Œå»ºè®®ç¨å¾®å¤šç­‰0.5ç§’
            results = self.driver.find_elements(By.CSS_SELECTOR, item_selector)

            # [å¤¸å…‹ä¸“å±] å¯å‘å¼è¡¥ä¸
            if not results and engine == "quark":
                results = self.driver.find_elements(By.XPATH, "//div[.//h3 or .//h2]")
                results = [r for r in results if r.size['height'] > 50]

            summary_list = []
            valid_links = []
            
            count = 0
            for i, res in enumerate(results):
                # æ•°é‡è¾¾åˆ°ä¸Šé™åˆ™åœæ­¢
                if count >= max_results: break
                
                # [æ–°å¢] å¹¿å‘Šè¿‡æ»¤ï¼šå¦‚æœåˆ¤å®šä¸ºå¹¿å‘Šï¼Œç›´æ¥è·³è¿‡
                if self._is_ad(res, engine):
                    continue

                try:
                    title, link, snippet = "", "", ""

                    # --- [æ–°å¢] ç™¾åº¦æµå¼ AI å†…å®¹ç‰¹æ®Šå¤„ç† ---
                    # å¦‚æœæ˜¯ç™¾åº¦çš„ç¬¬ä¸€æ¡ç»“æœï¼Œä¸”åŒ…å«â€œæ™ºèƒ½â€æˆ–â€œAIâ€å­—æ ·ï¼Œæˆ–è€…æ˜¯ç¬¬ä¸€æ¡
                    # æˆ‘ä»¬å°è¯•æ•è·å®ƒçš„æµå¼ç”Ÿæˆè¿‡ç¨‹
                    if engine == "baidu" and i == 0:
                        # è°ƒç”¨æµå¼ç­‰å¾…ï¼Œé˜²æ­¢æŠ“åˆ°åŠæˆª
                        full_text = self._wait_for_stream_text(res)
                        # å¦‚æœæŠ“åˆ°çš„æ–‡æœ¬å¾ˆé•¿ï¼Œç›´æ¥ä½œä¸º AI æ€»ç»“ï¼Œä¸éœ€ç‚¹é“¾æ¥
                        if len(full_text) > 200: 
                             snippet = full_text[:800] # æˆªå–å‰800å­—ï¼Œç»™ AI æ›´å¤šä¸Šä¸‹æ–‡
                             title = "ã€ç™¾åº¦ AI æ™ºèƒ½æ€»ç»“ã€‘"
                             link = self.driver.current_url # AI å›ç­”é€šå¸¸æ²¡æœ‰ç‹¬ç«‹é“¾æ¥
                             summary_list.append(f"[{count+1}] {title}\næ‘˜è¦: {snippet}")
                             count += 1
                             continue

                    # --- å¸¸è§„æŠ“å–é€»è¾‘ ---
                    # 1. æŠ“æ ‡é¢˜é“¾æ¥
                    try:
                        if engine == "quark":
                            header = res.find_element(By.CSS_SELECTOR, "h2, h3, h4, a[class*='title']")
                        else:
                            header = res.find_element(By.CSS_SELECTOR, title_selector)
                        title = header.text
                        link = header.get_attribute("href")
                        if not link: link = res.find_element(By.TAG_NAME, "a").get_attribute("href")
                    except: continue 

                    # 2. æŠ“æ‘˜è¦
                    try:
                        if engine == "quark":
                             snippet_elem = res.find_element(By.CSS_SELECTOR, "div[class*='abstract'], div[class*='desc'], div[class*='txt']")
                             snippet = snippet_elem.text
                        elif engine == "baidu":
                             snippet_elem = res.find_element(By.CSS_SELECTOR, desc_selector)
                             snippet = snippet_elem.text
                        else:
                            snippet_elem = res.find_element(By.CSS_SELECTOR, desc_selector)
                            snippet = snippet_elem.text
                    except:
                        snippet = res.text.replace(title, "")[:100].strip()
                    
                    if link and title and len(title) > 2:
                        summary_list.append(f"[{count+1}] {title}\nLink: {link}\næ‘˜è¦: {snippet}")
                        valid_links.append(link)
                        count += 1
                except: continue
            
            if not summary_list: return None

            context = f"ã€{engine.title()} æœç´¢ç»“æœã€‘:\n" + "\n\n".join(summary_list)
            
            # --- æ·±åº¦é˜…è¯» (Deep Read Top 1) ---
            # åªæœ‰å½“æœç´¢æˆåŠŸä¸” deep_read=True æ—¶æ‰è¿›è¡Œæ·±åº¦é˜…è¯»
            if deep_read and valid_links:
                target_link = valid_links[0]
                # [ä¼˜åŒ–] å¦‚æœç¬¬ä¸€æ¡æ˜¯ AI æ€»ç»“ï¼ˆLink æ˜¯å½“å‰é¡µï¼‰ï¼Œå°è¯•è¯»ç¬¬äºŒæ¡ï¼Œé¿å…é‡å¤
                if target_link == self.driver.current_url and len(valid_links) > 1:
                    target_link = valid_links[1]
                
                if target_link != self.driver.current_url:
                    # print(f"   -> ğŸ“– æ­£åœ¨æ·±åº¦é˜…è¯»: {target_link}")
                    content = self.read_page(target_link)
                    context += f"\n\nã€æ·±åº¦é˜…è¯» (Top Link)ã€‘:\n{content}"
                
            return context

        except Exception as e:
            print(f"   -> âš ï¸ {engine} æœç´¢å‡ºé”™: {e}")
            return None

    def read_page(self, url, max_chars=1500):
        """è®¿é—®å…·ä½“é“¾æ¥å¹¶æå–æ­£æ–‡"""
        try:
            # è®¾ç½®é¡µé¢åŠ è½½è¶…æ—¶ï¼Œé˜²æ­¢å¡æ­»åœ¨æŸäº›é‡JSç½‘ç«™
            self.driver.set_page_load_timeout(15)
            self.driver.get(url)
            time.sleep(2)
            
            body = self.driver.find_element(By.TAG_NAME, "body").text
            # æ¸…æ´—å¤šä½™ç©ºè¡Œ
            clean_text = re.sub(r'\n+', '\n', body)
            return clean_text[:max_chars]
        except Exception as e:
            return f"[è¯»å–å¤±è´¥: {str(e)[:50]}...]"

    def quit(self):
        if self.driver:
            try:
                self.driver.quit()
            except: pass
            self.driver = None


class PluginManager:
    """
    [V7.2 æ–°å¢] æŠ€èƒ½çš®å±‚ / æ’ä»¶ç®¡ç†å™¨
    åŠŸèƒ½ï¼šå¯åŠ¨æ—¶æ‰«æ plugins ç›®å½•ï¼Œå°†â€œè‡ªæˆ‘ç¼–ç¨‹â€äº§ç”Ÿçš„ Python è„šæœ¬
    ä½œä¸ºæ–°æŠ€èƒ½åŠ¨æ€åŠ è½½åˆ°å½“å‰ç”Ÿå‘½ä½“å®ä¾‹ä¸­ (Monkey Patching)ã€‚
    
    åŒºåˆ«äº LogicalLeftBrain (è´Ÿè´£ä¸´æ—¶æ‰§è¡Œä»£ç è¿”å›ç»“æœ)ï¼Œ
    PluginManager è´Ÿè´£æ°¸ä¹…æ€§åœ°ä¿®æ”¹ç”Ÿå‘½ä½“çš„è¡Œä¸ºæ¨¡å¼ (Add methods/attributes)ã€‚
    """
    def __init__(self, plugin_dir=None):
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥è·¯å¾„ï¼Œå…¶æ¬¡è¯»å–Configï¼Œæœ€åä½¿ç”¨é»˜è®¤å€¼
        self.plugin_dir = plugin_dir if plugin_dir else getattr(Config, 'PLUGIN_DIR', './nezha_evolution_system/plugins')
        # [å¥å£®æ€§] ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.plugin_dir, exist_ok=True)

    def activate_skills(self, agent_instance):
        """
        åŠ è½½å¹¶æ¿€æ´»æ‰€æœ‰æŠ€èƒ½
        :param agent_instance: å“ªå’æœ¬ä½“ (self)ï¼Œè®©æ’ä»¶å¯ä»¥ä¿®æ”¹å®ƒ
        :return: æˆåŠŸåŠ è½½çš„æŠ€èƒ½æ•°é‡
        """
        # æ‰«æç›®å½•ä¸‹æ‰€æœ‰ä»¥ skill_ å¼€å¤´çš„ py æ–‡ä»¶
        if not os.path.exists(self.plugin_dir): return 0
        skills = [f for f in os.listdir(self.plugin_dir) if f.endswith('.py') and f.startswith('skill_')]
        
        if not skills:
            return 0
            
        print(f"ğŸ§© [æŠ€èƒ½çš®å±‚] æ£€æµ‹åˆ° {len(skills)} ä¸ªå¤–æŒ‚åŸºå› ï¼Œæ­£åœ¨è½¬å½•...")
        loaded_count = 0
        
        for filename in skills:
            filepath = os.path.join(self.plugin_dir, filename)
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    code_content = f.read()
                
                # æ„å»ºæ‰§è¡Œä¸Šä¸‹æ–‡
                # [æ ¸å¿ƒæœºåˆ¶] å°† agent_instance ä¼ ç»™ 'self'
                # è¿™æ ·æ’ä»¶ä»£ç é‡Œçš„ `self.anxiety_level = 0` æˆ– `self.new_method = ...` æ‰ä¼šç”Ÿæ•ˆ
                context = {
                    "self": agent_instance, 
                    "math": math, 
                    "torch": torch, 
                    "datetime": datetime,
                    "os": os,
                    "json": json,
                    "re": re,
                    "Config": Config # å…è®¸æ’ä»¶è¯»å–é…ç½®
                }
                
                # åŠ¨æ€æ‰§è¡Œä»£ç  (Monkey Patching)
                # 1. è®°å½•æ‰§è¡Œå‰çš„å±æ€§åˆ—è¡¨
                old_attrs = set(dir(agent_instance))
                
                # 2. åŠ¨æ€æ‰§è¡Œä»£ç  (Monkey Patching)
                exec(code_content, context)
                
                # 3. è®°å½•æ‰§è¡Œåçš„å±æ€§åˆ—è¡¨å¹¶å¯¹æ¯”
                new_attrs = set(dir(agent_instance))
                new_skills = new_attrs - old_attrs
                
                if new_skills:
                    loaded_count += 1
                    # å°†é›†åˆè½¬ä¸ºå­—ç¬¦ä¸²ä»¥ä¾¿æ‰“å°
                    skills_str = ", ".join(new_skills)
                    print(f"   -> ğŸ§¬ åŸºå› è½¬å½•æˆåŠŸ: {filename} (æ–°å¢èƒ½åŠ›: {skills_str})")
                else:
                    # å¦‚æœæ²¡æœ‰æ–°å¢å±æ€§ï¼Œè¯´æ˜æ’ä»¶ä»£ç é‡Œå¿˜äº†å†™ `self.func = func`
                    print(f"   -> âš ï¸ åŸºå› è½¬å½•æ— æ•ˆ: {filename} (æ‰§è¡ŒæˆåŠŸä½†æœªæŒ‚è½½ä»»ä½•èƒ½åŠ›ï¼Œè¯·æ£€æŸ¥ä»£ç æ˜¯å¦åŒ…å« self.x = x)")
                
            except Exception as e:
                print(f"   -> âš ï¸ åŸºå›  {filename} æ’å¼‚ååº”(åŠ è½½å¤±è´¥): {e}")
        
        return loaded_count

    def execute_code(self, code_snippet):
        """
        [è¿è¡Œé˜¶æ®µ] é€»è¾‘å·¦è„‘ / ä¸´æ—¶å·¥å…·æ‰§è¡Œ
        åªè¿”å›æ‰“å°ç»“æœï¼Œä¸ä¿®æ”¹æœ¬ä½“ï¼Œç›¸å¯¹å®‰å…¨
        """
        buf = io.StringIO()
        context = {
            "math": math, "random": random, "datetime": datetime, 
            "re": re, "json": json, "print": print
        }
        
        try:
            # æ•è·æ ‡å‡†è¾“å‡º
            with contextlib.redirect_stdout(buf):
                exec(code_snippet, context)
            
            output = buf.getvalue().strip()
            return output if output else "[ä»£ç æ‰§è¡ŒæˆåŠŸï¼Œæ— è¾“å‡º]"
        except Exception as e:
            return f"âŒ ä»£ç æ‰§è¡Œé”™è¯¯: {e}"


# ==============================================================================
# 4. æ ¸å¿ƒå®ä½“ï¼šå“ªå’ (V7.2 Singularity Orchestrator)
# ==============================================================================
class NezhaLifeform:

    # å†…æ„Ÿå—
    def _check_growth_potential(self):
        """æ£€æŸ¥å½“å‰æ˜¾å­˜æ˜¯å¦è¿˜èƒ½å®¹çº³æ–°çš„ä¸€å±‚"""
        try:
            # æ£€æŸ¥æ‰€æœ‰ä½¿ç”¨ä¸­çš„ GPU
            devices = {p.device for p in self.model.parameters() if p.device.type == "cuda"}
            if not devices: return True, "CPUæ¨¡å¼"
            
            # ä¼°ç®—å¢åŠ ä¸€å±‚æ‰€éœ€çš„æ˜¾å­˜ (å‚æ•°é‡ x 2 bytes x ç¼“å†²)
            # é¢„ç•™ 500MB + 1GB ç¼“å†²
            estimated_cost = 800 * 1024 * 1024
            safe_buffer = 1.5 * 1024 * 1024 * 1024
            
            for device in devices:
                # è·å–æ˜¾å­˜ä¿¡æ¯
                free, total = torch.cuda.mem_get_info(device)
                
                # é˜ˆå€¼åˆ¤æ–­
                if free < (estimated_cost + safe_buffer):
                    # åˆ©ç”¨ total æ‰“å°æ›´è¯¦ç»†çš„æ—¥å¿—
                    usage_ratio = (total - free) / total * 100
                    return False, f"æ˜¾å­˜æ¯ç«­ (Device {device}: Free {free/1024**3:.2f}GB / Total {total/1024**3:.2f}GB, Used {usage_ratio:.1f}%)"
            
            return True, "æ˜¾å­˜å……è¶³"
        except Exception as e:
            # å¦‚æœå‡ºé”™ï¼Œé»˜è®¤å…è®¸ç”Ÿé•¿ï¼Œé¿å…å› ç›‘æ§ç»„ä»¶æ•…éšœé˜»ç¢è¿›åŒ–
            return True, f"æ„ŸçŸ¥å¤±æ•ˆ({e})"

    def __init__(self):
        # 1. åŠ è½½çŠ¶æ€ä¸åŸºå› 
        self.state = self._load_state()

        # ================= æ­¥éª¤ 1.5: ä¼˜å…ˆåŠ è½½ç¥ä¹‹çœ¼ =================
        self.embedder = None
        current_dim = 1024

        try:
            print(f"ğŸ‘ï¸ [Vision] æ­£åœ¨åŠ è½½ç¥ä¹‹çœ¼ ({Config.EMBEDDING_MODEL_NAME})...")
            
            from sentence_transformers import SentenceTransformer
            # [å…³é”®] cache_folder æŒ‡å®šä¸‹è½½ç›®å½•
            self.embedder = SentenceTransformer(
                Config.EMBEDDING_MODEL_NAME, 
                device="cpu", 
                cache_folder=Config.MODEL_CACHE_DIR
            )
            
            # è·å–æ¨¡å‹ç»´åº¦ï¼ˆä»…ç”¨äºæ‰“å°ï¼Œä¸å†ä¼ ç»™MemoryInfrastructureï¼‰
            current_dim = self.embedder.get_sentence_embedding_dimension()
            print(f"   -> âœ… è§†è§‰æ¨¡ç»„åŠ è½½å®Œæ¯•ï¼Œå½“å‰è§†ç•Œç²¾åº¦: {current_dim} ç»´")
            
        except ImportError:
            print("âš ï¸ æœªæ£€æµ‹åˆ° sentence-transformers åº“ï¼Œç¥ä¹‹çœ¼æ— æ³•å¼€å¯ã€‚")
        except Exception as e:
            print(f"âš ï¸ è¯­ä¹‰æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}")

        # 1. åˆå§‹åŒ–åŸºç¡€è®¾æ–½ (ç§»é™¤äº† embedding_dim å‚æ•°)
        # æˆ‘ä»¬çš„æ–° MemoryInfrastructure ç°åœ¨ä¼šè‡ªåŠ¨è¯†åˆ«ç»´åº¦
        self.memory_db = MemoryInfrastructure()

        # 2. å¯åŠ¨æ—¶è®©æµ·é©¬ä½“æ‰«æçŸ¥è¯†åº“
        # å®šä¹‰ helper å‡½æ•°ï¼šå°† embedder çš„èƒ½åŠ›åŒ…è£…æˆ MemoryInfrastructure èƒ½ç”¨çš„å½¢å¼
        def _embed_helper(text_list):
            if not self.embedder: return []
            # [å…³é”®] encode åŠ ä¸Š convert_to_tensor=True
            # ç›´æ¥è¿”å› CPU Tensorï¼Œæ•ˆç‡æœ€é«˜ï¼Œå®Œç¾é€‚é…æ–° MemoryInfrastructure
            return self.embedder.encode(text_list, convert_to_tensor=True, device='cpu')

        if self.embedder and hasattr(self, 'memory_db'):
            # è®© MemoryInfrastructure ä½¿ç”¨ embedder èƒ½åŠ›å»è‡ªåŠ¨â€œåƒä¹¦â€ (Ingest)
            self.memory_db.ingest_knowledge_base(Config.KNOWLEDGE_BASE, _embed_helper)

        # è´å¶æ–¯è¿›åŒ–å¼•æ“
        self.evolution_engine = BayesianGeneticEngine(self.memory_db)
        self.genome = self.state.get('genome', Config.DEFAULT_GENOME.copy())

        #  åˆå§‹åŒ–æä»æ ¸åŠ«æŒçŠ¶æ€é” (ä»å­˜æ¡£æ¢å¤ï¼Œé˜²æ­¢é‡å¯æ–­ç‰‡)
        self.is_under_amygdala_hijack = self.state.get('is_hijacked', False)

        # === åˆå§‹åŒ– RLock (å¯é‡å…¥é”) ===
        # è¿™æŠŠé”ç”¨äºä¿æŠ¤æµè§ˆå™¨å’Œå­˜æ¡£æ–‡ä»¶ï¼Œé˜²æ­¢åå°æ½œæ„è¯†å’Œå‰å°ä¸»çº¿ç¨‹æ‰“æ¶
        self.lock = threading.RLock()

        # ç¡®ä¿æ‰€æœ‰æ–°åŸºå› éƒ½å­˜åœ¨
        for k, v in Config.DEFAULT_GENOME.items():
            if k not in self.genome: self.genome[k] = v   

        # å®ä¾‹åŒ–é€»è¾‘å·¦è„‘
        self.left_brain = LogicalLeftBrain()

        # 2. åˆå§‹åŒ–è®°å¿†ç¼“å†²åŒº
        self.daily_buffer = []      # æ¯æ—¥ç»å† (ç”¨äºå¤œé—´åæ€)
        self.short_term_memory = [] # çŸ­æœŸå·¥ä½œè®°å¿† (ç”¨äºå¤šè½®å¯¹è¯)
        
        print("ğŸ”¥ [å“ªå’] æ­£åœ¨æ„å»ºèº¯ä½“ (PFC + Dual Brain + Net2Net/MoE)...")
        
        # ================= æ™ºèƒ½æ˜¾å­˜è§„åˆ’ =================
        # è‡ªåŠ¨æ£€æµ‹ GPU æ•°é‡å’Œæ˜¾å­˜å¤§å°ï¼Œå¹¶ä¸ºæ¯å¼ å¡é¢„ç•™ç¼“å†²ç©ºé—´ (KV Cache / System)
        max_memory_map = {}
        if torch.cuda.is_available():
            num_gpus = torch.cuda.device_count()
            print(f"   -> ğŸ–¥ï¸ æ£€æµ‹åˆ° {num_gpus} å¼ æ˜¾å¡ï¼Œæ­£åœ¨è§„åˆ’æ˜¾å­˜å¸ƒå±€...")
            
            for i in range(num_gpus):
                # è·å–è¯¥å¡æ€»æ˜¾å­˜ (Bytes -> GiB)
                props = torch.cuda.get_device_properties(i)
                total_mem = props.total_memory / (1024**3)
                
                # ç­–ç•¥ï¼šé¢„ç•™ç¼“å†²åŒº (Buffer)
                # 0å·å¡é€šå¸¸è´Ÿè´£æ¡Œé¢æ˜¾ç¤ºå’Œ PyTorch ä¸Šä¸‹æ–‡ï¼Œå»ºè®®é¢„ç•™ 3GB (å¦‚æœæ˜¾å­˜æå°åˆ™æŒ‰æ¯”ä¾‹)
                if i == 0:
                    buffer = 3.0 
                else:
                    buffer = 1.5
                
                # æ˜¾å­˜å¤ªå°çš„å¡(ä¾‹å¦‚4Gå¡)ï¼Œé¢„ç•™å¤ªå¤šä¼šå¯¼è‡´ä¸å¯ç”¨ï¼Œåšä¸€ä¸ªåŠ¨æ€ä¿æŠ¤
                if total_mem < 8.0: 
                    buffer = total_mem * 0.3 # å°å¡åªé¢„ç•™ 30%
                
                # è®¡ç®—å®‰å…¨é™é¢
                alloc_mem = max(0, total_mem - buffer)
                
                # int() ä¼šå‘ä¸‹å–æ•´ï¼Œæ¯”å¦‚ 10.9G -> 10Gï¼Œè¿™å¾ˆå®‰å…¨
                max_memory_map[i] = f"{int(alloc_mem)}GiB"
                
                print(f"      - GPU {i} ({props.name}): æ€»é‡ {total_mem:.1f}G | é¢„ç•™ {buffer:.1f}G | åˆ†é… {max_memory_map[i]}")
        else:
            print("   -> âš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œå°†å›é€€åˆ° CPU æ¨¡å¼ (ææ…¢)ã€‚")
            max_memory_map = None # è®© accelerate è‡ªå·±çœ‹ç€åŠ
        
        mid = Config.MASTER_MODEL_PATH if os.path.exists(Config.MASTER_MODEL_PATH) else Config.BASE_MODEL
        print(f"   -> Loading Model from: {mid}")

        # å…ˆåŠ è½½ Tokenizer (è¿™ç©æ„å„¿ä¸å æ˜¾å­˜ï¼Œå…ˆåŠ è½½å¥½é˜²æ­¢è·¯å¾„é”™è¯¯)
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                mid, 
                trust_remote_code=True,
                fix_mistral_regex=True
                )
        except Exception as e: # <--- [ä¿®æ­£] å¿…é¡»åŠ ä¸Š Exception as eï¼Œå¦åˆ™ print(e) ä¼šæŠ¥é”™
            print(f"   -> âš ï¸ æœ¬åœ° Tokenizer åŠ è½½å¤±è´¥ ({e})ï¼Œå°è¯•ä» Base Model æ‹‰å–...")
            # å…œåº•ï¼šå¦‚æœæœ¬åœ°æ²¡æœ‰ï¼Œå°è¯•ä» hub æ‹‰å–
            self.tokenizer = AutoTokenizer.from_pretrained(
                Config.BASE_MODEL, 
                trust_remote_code=True,
                fix_mistral_regex=True
                )

        # è¡¥å…¨ Tokenizer é…ç½®
        if not self.tokenizer.pad_token: 
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # [4-bit ç‹¬å æ¨¡å¼] å½»åº•ç§»é™¤ 8-bit é€»è¾‘
        # ä½¿ç”¨ NF4 (Normal Float 4) æ ¼å¼ï¼Œè¿™æ˜¯ç›®å‰ LLM é‡åŒ–çš„æœ€ä½³å®è·µ
        try:
            print("   -> ğŸš€ å¯åŠ¨ 4-bit é‡åŒ–åŠ è½½ (NF4 + SDPA)...")
            
            # 4-bit é…ç½® (NF4 æ ¼å¼ï¼Œç²¾åº¦æŸå¤±æå°)
            # NF4 (Normal Float 4) æ˜¯ä¸“é—¨ä¸ºæ­£æ€åˆ†å¸ƒæƒé‡é‡èº«å®šåˆ¶çš„é‡åŒ–æ•°æ®ç±»å‹
            q_config_4bit = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16, # è®¡ç®—æ—¶è¿˜åŸå› fp16
                bnb_4bit_use_double_quant=True,       # åŒé‡é‡åŒ–çœæ˜¾å­˜ (å†æ¬¡å‹ç¼©é‡åŒ–å¸¸æ•°)
                bnb_4bit_quant_type="nf4"             # æ­£æ€æµ®ç‚¹é‡åŒ– (æ¯” fp4 æ›´ç²¾å‡†)
            )

            self.model = AutoModelForCausalLM.from_pretrained(
                mid, 
                quantization_config=q_config_4bit, 
                device_map="auto", 
                max_memory=max_memory_map, 
                trust_remote_code=True,
                dtype=torch.float16,
                attn_implementation="sdpa"
            )
            print(f"   -> âœ… æ¨¡å‹ä»¥ 4-bit æ¨¡å¼åŠ è½½æˆåŠŸ")

            # 3. [å…³é”®è¡¥ä¸] ä¸º 4-bit è®­ç»ƒåšé¢„å¤„ç†
            # è¿™ä¸€æ­¥éå¸¸é‡è¦ï¼å¦‚æœä¸åŠ ï¼Œåç»­ SFT/DPO è®­ç»ƒæ—¶æ¢¯åº¦æ— æ³•åå‘ä¼ æ’­
            print("   -> ğŸ”§ [ç³»ç»Ÿ] æ­£åœ¨ä¸º 4-bit è®­ç»ƒè¿›è¡Œé¢„å¤„ç† (kbit_training)...")
            # use_gradient_checkpointing=True: ç‰ºç‰²ä¸€ç‚¹é€Ÿåº¦æ¢å–å¤§é‡æ˜¾å­˜ï¼Œé˜²æ­¢OOM
            self.model = prepare_model_for_kbit_training(self.model, use_gradient_checkpointing=True)

        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # å¤±è´¥åå°è¯•æ¸…ç†æ˜¾å­˜
            gc.collect()
            torch.cuda.empty_cache()
            raise e # å½»åº•æ²¡æ•‘äº†ï¼ŒæŠ›å‡ºå¼‚å¸¸é€€å‡º

        # 4. å‡†å¤‡åŒè„‘ (Fast/Slow LoRA)
        print("   -> ğŸ§  æ­£åœ¨é…ç½®è®­ç»ƒç¯å¢ƒ (æ¢¯åº¦æ£€æŸ¥ç‚¹)...")
        
        # ä»…æ£€æµ‹æ˜¯å¦ä¸º 4-bit é‡åŒ–
        is_quantized = getattr(self.model, "is_loaded_in_4bit", False)

        # ä»…å¤„ç†éé‡åŒ– (FP16/BF16) çš„æƒ…å†µ
        # å› ä¸ºé‡åŒ–æ¨¡å‹åœ¨ä¸Šä¸€æ­¥åŠ è½½æ—¶å·²ç»å¤„ç†è¿‡ prepare_model_for_kbit_training äº†
        if not is_quantized:
            print("   -> ğŸ”§ [ç³»ç»Ÿ] æ£€æµ‹åˆ° FP16 æ¨¡å¼ï¼Œæ‰‹åŠ¨å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹...")
            if hasattr(self.model, "enable_input_require_grads"):
                self.model.enable_input_require_grads()
            
            # å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œé˜²æ­¢ FP16 è®­ç»ƒ OOM
            self.model.gradient_checkpointing_enable()

        # ä½¿ç”¨å¤–ç§‘åŒ»ç”ŸåŠ è½½åŒ Adapter (Fast=ç›´è§‰, Slow=æ·±æ€)
        self.model = NeuroSurgeon.setup_dual_brain(self.model) 
        self.model.set_adapter("fast") # é»˜è®¤å¤„äºç›´è§‰æ¨¡å¼
        
        # 5. åˆå§‹åŒ–é«˜çº§å™¨å®˜
        # PFC: è´Ÿè´£å…ƒè®¤çŸ¥è·¯ç”±
        self.pfc = PrefrontalCortex(self.model.config.hidden_size, device=self.model.device)

        # Web: è´Ÿè´£è¿æ¥ç°å®ä¸–ç•Œ (æå‰åˆå§‹åŒ–ï¼Œä½œä¸º DreamWeaver çš„ä¾èµ–)
        # headless=False è®©ä½ èƒ½çœ‹åˆ°æµè§ˆå™¨æ“ä½œï¼Œéå¸¸é…·
        self.web = RealBrowserEye(headless=False)

        # äº‘ç«¯å¯¼å¸ˆ (æå‰åˆå§‹åŒ–ï¼Œä½œä¸º DreamWeaver çš„ä¾èµ–)
        # å³ä½¿ Config é‡Œæ²¡å¡« Keyï¼ŒCloudTeacher å†…éƒ¨ä¹Ÿä¼šå¤„ç†æˆ Noneï¼Œä¸ä¼šæŠ¥é”™
        self.teacher = CloudTeacher()

        # æå‰åˆå§‹åŒ–å›¾è°±æµ·é©¬ä½“ (å› ä¸º DreamWeaver éœ€è¦è¯»å–è®°å¿†è¿›è¡Œ RAG éªŒè¯)
        print("ğŸ§  [æµ·é©¬ä½“] æ­£åœ¨é“¾æ¥ç¥ç»å…ƒå›¾è°±...")
        self.hippocampus = GraphHippocampus(self.model, self.tokenizer, self.memory_db)

        # å®ä¾‹åŒ– é€ æ¢¦å¸ˆ
        # å¿…é¡»ä¼ å…¥ teacher, web, hippocampus ä»¥ä¾¿è¿›è¡Œ"è‡ªæˆ‘è¯„ä¼°"å’Œ"è”ç½‘çº é”™"
        self.dream_weaver = DreamWeaver(
            self.model, 
            self.tokenizer, 
            teacher=self.teacher, 
            web_eye=self.web, 
            hippocampus=self.hippocampus,
            memory_db=self.memory_db
        )

        # Plugin: è´Ÿè´£å®‰å…¨åœ°è‡ªæˆ‘ç¼–ç¨‹
        self.plugin_manager = PluginManager() 

        # åŸºå› ç¼–è¾‘å™¨ (ç”¨äºå¤„ç† PATCH æŒ‡ä»¤)
        self.editor = GeneticEditor()
        
        # æ ¸å¿ƒç”Ÿç‰©æœ¬èƒ½ç»„ä»¶ (Bio-Upgrade S-Tier)
        self.curiosity = IntrinsicCuriosityModule(self.genome) 
        
        # [æ¤å…¥] ç¥ç»å†…åˆ†æ³Œç³»ç»Ÿ (ç®¡ç† Dopamine, Serotonin, Cortisol)
        self.endocrine = NeuroEndocrineSystem(self.state)
        
        # [æ¤å…¥] æä»æ ¸ (ææƒ§ä¸­å¿ƒ)
        self.amygdala = AsyncAmygdala(self.genome, self.gwt, self.endocrine) 
        self.amygdala.start() # <--- å¯åŠ¨çº¿ç¨‹   
        
        # [æ¤å…¥] å‰æ‰£å¸¦çš®å±‚ (é¢„æµ‹è¯¯å·®ç›‘æ§)
        self.acc = AnteriorCingulateCortex(self.model, self.tokenizer)

        # åˆå§‹åŒ– SNN ç¥ç»çš®å±‚
        # å¼ºåˆ¶ä½¿ç”¨ CPU æ¨¡å¼ï¼Œé¿å…ä¸ä¸»æ¨¡å‹äº‰æŠ¢æ˜¾å­˜
        self.cortex = NezhaNeuralCortex(
            input_dim=current_dim, 
            hidden_dim=128, 
            genome=self.genome,  # <--- [æ–°å¢] æ³¨å…¥ self.genome
            device="cpu"
        )
        self.cortex.load()
        
        # ç«‹å³æ¿€æ´»å·²æœ‰çš„æŠ€èƒ½ (å¼€æœºè‡ªå¯)
        # ä¼ å…¥ selfï¼Œè®©æ’ä»¶ä¿®æ”¹å½“å‰å®ä¾‹
        num_skills = self.plugin_manager.activate_skills(self)
        if num_skills > 0:
            print(f"âœ¨ [è¿›åŒ–] å·²æˆåŠŸç»§æ‰¿ {num_skills} ä¸ªä»å‰ä¸–ä¿ç•™çš„æŠ€èƒ½ã€‚")
        
        # 6. åˆå§‹åŒ–å­¦ä¹ ç³»ç»Ÿ
        # (æµ·é©¬ä½“ hippocampus å·²åœ¨ä¸Šæ–¹æå‰åˆå§‹åŒ–ï¼Œè¿™é‡Œä¸å†é‡å¤)
        
        print("ğŸ›¡ï¸ [å…ç–«ç³»ç»Ÿ] æ­£åœ¨æ¿€æ´» EWC é•¿æœŸè®°å¿†ä¿æŠ¤...")
        # 2. å®ä¾‹åŒ–çœŸæ­£çš„ EWC (åˆ é™¤äº†é‚£ä¸ª placeholder)
        # æ³¨æ„ï¼šå¿…é¡»æŠŠ self.hippocampus ä¼ è¿›å»ï¼Œè®©å®ƒèƒ½è¯»å–æ—§è®°å¿†
        self.ewc_handler = EWC(self.model, self.tokenizer, self.hippocampus, device=self.model.device)

        # === å¥‡ç‚¹å†…æ ¸åˆå§‹åŒ– (Singularity Kernel) ====
        print("ğŸŒŒ [Singularity] æ­£åœ¨åˆå§‹åŒ–ä¸»åŠ¨æ¨ç†å¼•æ“ä¸å…¨å±€å·¥ä½œç©ºé—´...")
        
        # 1. åˆå§‹åŒ–ä¸–ç•Œæ¨¡å‹ (ç¥ç»åŠ¨åŠ›å­¦ç‰ˆ)
        self.world_model = NeuralWorldModel(Config.ACTION_SPACE)
        
        # 2. åˆå§‹åŒ–ä¸»åŠ¨æ¨ç†å¼•æ“ (é—­ç¯ç‰ˆ)
        self.inference_engine = ActiveInferenceEngine(self.world_model, Config.ACTION_SPACE)
        
        # 3. åˆå§‹åŒ–å…¨å±€å·¥ä½œç©ºé—´
        self.gwt = GlobalWorkspace()
        
        # 4. åˆå§‹åŒ–å¤šè·¯å¾„æ¨ç†æœº (åŸé‡å­è„‘)
        print("ğŸŒŒ [å¤šè·¯å¾„æ¨ç†] æ­£åœ¨æ„å»ºè’™ç‰¹å¡æ´›æ€ç»´æ ‘...")
        self.quantum_brain = MultiPathReasoning(
            model=self.model, 
            tokenizer=self.tokenizer, 
            left_brain=self.left_brain, 
            web_eye=self.web,
            teacher=self.teacher,
            memory_db=self.memory_db
        )
        
        # 4. åˆå§‹åŒ–å…ƒè®¤çŸ¥ç¼–ç¨‹å™¨ (è‡ªæˆ‘è¿›åŒ–)
        self.meta_programmer = MetaCognitiveProgrammer()
        
        # 5. åˆå§‹åŒ–åŠ¨æ€ Prompt (ç”±å®ªæ³• + åˆå§‹ç­–ç•¥ç»„æˆ)
        # è¿™å°†æ›¿ä»£åŸæ¥çš„å›ºå®š sys_p
        self.dynamic_system_prompt = (
            f"{self.meta_programmer.CONSTITUTION}\n"
            f"ç³»ç»Ÿèƒ½åŠ›: SELF(æœ¬èƒ½), WEB(æœç´¢), TEACHER(è¯·æ•™), CODE(ç¼–ç¨‹)ã€‚\n"
            f"å½“å‰ç­–ç•¥: ä¼˜å…ˆä½¿ç”¨ SELF å›ç­”ï¼Œé‡åˆ°æœªçŸ¥ä½¿ç”¨ WEBï¼Œé‡åˆ°é€»è¾‘å›°éš¾ä½¿ç”¨ CODEã€‚"
        )

        # Feeder: è´Ÿè´£å¤œé—´åƒä¹¦å’Œåæ€æ—¥è®°
        # Feeder ç°åœ¨å…·å¤‡ decide_strategy èƒ½åŠ›
        self.feeder = AcademicFeeder(self.model, self.tokenizer, self.ewc_handler)
        
        # å¤‡ä»½è‡ªèº«ä»£ç ä½œä¸ºåˆå§‹çŸ¥è¯† (è‡ªæˆ‘è®¤çŸ¥)
        if not os.path.exists(os.path.join(Config.KNOWLEDGE_BASE, "me_source.py")):
            shutil.copy(Config.SCRIPT_PATH, os.path.join(Config.KNOWLEDGE_BASE, "me_source.py"))

        print(f"âœ¨ [è‹é†’] Age:{self.state.get('age',0)} | Layer:{self.model.config.num_hidden_layers} | ATP:{self.state['atp']:.1f}")

        # === å¯åŠ¨åå°æ½œæ„è¯†çº¿ç¨‹ ===
        # daemon=True è¡¨ç¤ºå½“ä¸»ç¨‹åºé€€å‡ºæ—¶ï¼Œè¿™ä¸ªçº¿ç¨‹ä¹Ÿä¼šè‡ªåŠ¨éšä¹‹å…³é—­
        t = threading.Thread(target=self._autonomous_loop, daemon=True)
        t.start()

    def _load_state(self):
        """
        [State Management] åŠ è½½ç”Ÿå‘½çŠ¶æ€
        åŒ…å«åŸºå› å›æ»šæœºåˆ¶ã€æ—§å­˜æ¡£å…¼å®¹è¡¥å…¨ä»¥åŠé»˜è®¤ç”Ÿç†æŒ‡æ ‡çš„åˆå§‹åŒ–ã€‚
        """
        # å®šä¹‰æ ‡å‡†é»˜è®¤çŠ¶æ€ (Standard Biological State)
        default = {
            # === 1. åŸºç¡€ä»£è°¢æŒ‡æ ‡ (Metabolic Metrics) ===
            "atp": 100.0,         # [èƒ½é‡åº•ç‰©] ç”Ÿå­˜çš„æ ¹æœ¬ã€‚æ€è€ƒã€è¡ŒåŠ¨ã€è¿›åŒ–éƒ½éœ€è¦æ¶ˆè€— ATPã€‚è€—å°½å³æ­»ã€‚
            "entropy": 0.5,       # [è®¤çŸ¥ç†µ/è‡ªç”±èƒ½] è¡¡é‡å¯¹ä¸–ç•Œçš„ä¸ç¡®å®šæ€§ã€‚FEP ç†è®ºæ ¸å¿ƒï¼Œç›®æ ‡æ˜¯æœ€å°åŒ–æ­¤å€¼ã€‚
            "age": 0,             # [ç”Ÿå­˜æ—¶é’Ÿ] å­˜æ´»çš„æ—¶é—´æ­¥æ•° (Ticks)ã€‚ç”¨äºè®¡ç®—é•¿å¯¿å¥–åŠ± (Longevity Bonus)ã€‚
            
            # === 2. è¿›åŒ–ä¸é—ä¼  (Evolutionary & Genetic) ===
            "evo_pts": 0,                 # [è¿›åŒ–ç‚¹æ•°] è¿›åŒ–çš„è´§å¸ï¼Œç”¨äºå…‘æ¢åŸºå› çªå˜æˆ–å™¨å®˜å‡çº§ã€‚
            "genome": Config.DEFAULT_GENOME.copy(), # [åŸºå› ç»„] å†³å®šè¡Œä¸ºæ¨¡å¼çš„è¶…å‚æ•°é›†åˆ (å¦‚å­¦ä¹ ç‡ã€èƒ†é‡ç­‰)ã€‚
            
            # === 3. é£é™©è¯„ä¼°è®°å¿† (Risk Memory) [New for Sortino] ===
            # ç”¨äºå­˜å‚¨æœ€è¿‘ N è½®çš„æƒŠå¥‡åº¦ (Loss)ï¼Œä¾›è´å¶æ–¯å¼•æ“è®¡ç®—ä¸‹è¡Œé£é™© (Downside Deviation)ã€‚
            # å¦‚æœç¼ºå¤±æ­¤é¡¹ï¼Œç´¢æè¯ºæ¯”ç‡å°†é€€åŒ–ä¸ºæ™®é€šå‡å€¼è®¡ç®—ã€‚
            "loss_history": [], 
            
            # === 4. é‡å­æ€ç»´é¥æµ‹ (Quantum Telemetry) ===
            # è®°å½• MCTS (è’™ç‰¹å¡æ´›æ ‘æœç´¢) æˆ–å¤šè·¯å¾„æ¨ç†çš„æ€§èƒ½ç»Ÿè®¡
            "quantum_stats": {
                "attempts": 0,          # [å‘æ•£] å°è¯•è¿›è¡Œæ€ç»´å‘æ•£çš„æ€»æ¬¡æ•°
                "successes": 0,         # [åç¼©] æˆåŠŸæ”¶æ•›åˆ°æœ‰æ•ˆè§£çš„æ¬¡æ•° (æ³¢å‡½æ•°åç¼©)
                "energy_consumed": 0.0, # [èƒ½è€—] é‡å­æ€ç»´æ¶ˆè€—çš„æ€» ATP (ç”¨äºè¯„ä¼°è®¤çŸ¥æˆæœ¬)
                
                # [è‡ªé€‚åº”é˜ˆå€¼] åŠ¨æ€è°ƒæ•´è¿›å…¥é‡å­æ€çš„é—¨æ§›
                # å¦‚æœæ€ç»´æˆåŠŸç‡é«˜ï¼Œé—¨æ§›ä¼šé™ä½ï¼›å¦‚æœç»å¸¸å¤±è´¥ï¼Œé—¨æ§›ä¼šå‡é«˜ä»¥é€šè¿‡èŠ‚èƒ½
                "adaptive_threshold": Config.ATP_THRESHOLD_QUANTUM 
            }            
        }
        
        try:
            if os.path.exists(Config.STATE_FILE):
                st = json.load(open(Config.STATE_FILE))
                
                # [å…³é”®ä¿®å¤] éå†é»˜è®¤å­—å…¸ï¼Œè¡¥å…¨å­˜æ¡£ä¸­ç¼ºå¤±çš„é¡¶å±‚é”® (å¦‚ evo_pts)
                for key, val in default.items():
                    if key not in st:
                        st[key] = val
                
                # [äºŒçº§è¡¥å…¨] ç¡®ä¿ genome é‡Œçš„æ–°åŸºå› ä¹Ÿå­˜åœ¨ (é˜²æ­¢ V2.0 æ–°åŸºå› ç¼ºå¤±)
                if "genome" not in st: 
                    st["genome"] = default["genome"]
                else:
                    for k, v in Config.DEFAULT_GENOME.items():
                        if k not in st["genome"]:
                            st["genome"][k] = v

                # [åŸºå› å›æ»š] å¦‚æœä¸Šæ¬¡è½¬ä¸–å¾ˆå¿«å°±æŒ‚äº†(Age<2)ï¼Œè¯´æ˜è¿›åŒ–å¤±è´¥ï¼Œå›æ»šåˆ°ç¥–å…ˆå­˜æ¡£
                if st.get('age', 0) < 2 and os.path.exists(Config.BEST_STATE_FILE):
                    print("ğŸ§¬ [åŸºå› å›æ»š] æ£€æµ‹åˆ°æ—©å¤­ï¼Œå›æ»šè‡³æœ€ä¼˜ç¥–å…ˆåŸºå› ...")
                    try:
                        best_st = json.load(open(Config.BEST_STATE_FILE))
                        # åŒæ ·è¦è¡¥å…¨ç¥–å…ˆå­˜æ¡£ï¼Œé˜²æ­¢ç¥–å…ˆä¹Ÿæ˜¯æ—§ç‰ˆæœ¬
                        for key, val in default.items():
                            if key not in best_st: best_st[key] = val
                        return best_st
                    except:
                        pass # è¯»å–å¤±è´¥åˆ™å¿½ç•¥
                        
                return st
        except Exception as e: 
            print(f"âš ï¸ å­˜æ¡£è¯»å–å¼‚å¸¸ï¼Œä½¿ç”¨é»˜è®¤çŠ¶æ€: {e}")
            pass
            
        return default

    def _save_state(self):
        """ä¿å­˜ç”Ÿå‘½çŠ¶æ€ (å·²åŠ é”ä¿æŠ¤ï¼Œé˜²æ­¢å¤šçº¿ç¨‹å†™å…¥å†²çª)"""
        with self.lock: # <--- å…³é”®ï¼šåªæœ‰æ‹¿åˆ°é”æ‰èƒ½å†™æ–‡ä»¶
            
            # æ¿€ç´ çŠ¶æ€åŒæ­¥ (State Synchronization)
            # å¿…é¡»åœ¨å†™å…¥ JSON ä¹‹å‰ï¼Œå°†å†…åˆ†æ³Œç³»ç»Ÿçš„å½“å‰æ•°å€¼åŒæ­¥å› self.state å­—å…¸
            # å¦åˆ™é‡å¯åï¼Œå“ªå’çš„æƒ…ç»ªä¼šé‡ç½®ä¸ºé»˜è®¤å€¼
            if hasattr(self, 'endocrine'):
                self.state['dopamine'] = float(self.endocrine.da)
                self.state['norepinephrine'] = float(self.endocrine.ne)
                self.state['serotonin'] = float(self.endocrine.ht)
                self.state['cortisol'] = float(self.endocrine.cortisol)

            # ä¿å­˜æä»æ ¸åŠ«æŒçŠ¶æ€ (å…³é”®ä¿®å¤ï¼šé˜²æ­¢é‡å¯åé—å¿˜æˆ˜é€ƒçŠ¶æ€)
            self.state['is_hijacked'] = getattr(self, 'is_under_amygdala_hijack', False)

            # æ‰§è¡Œç‰©ç†å†™å…¥
            with open(Config.STATE_FILE, 'w') as f: 
                json.dump(self.state, f, indent=2)
            
            # ä¿å­˜ç¥ç»çš®å±‚æƒé‡ (Titans Memory)
            if hasattr(self, 'cortex'):
                self.cortex.save()


    def _safe_print(self, message):
        """
        æ— ç—•æ‰“å°å·¥å…·ï¼š
        å½“åå°çº¿ç¨‹æƒ³è¦è¯´è¯æ—¶ï¼Œå®ƒä¼šå…ˆæ¸…é™¤å½“å‰è¡Œçš„è¾“å…¥æç¤ºç¬¦ 'You: 'ï¼Œ
        æ‰“å°å®Œæ—¥å¿—åï¼Œå†æŠŠ 'You: ' è¡¥å›å»ã€‚
        è¿™æ ·ä½ çš„æ‰“å­—ç•Œé¢å°±ä¸ä¼šè¢«åå°æ—¥å¿—åˆ‡æ–­äº†ã€‚
        """
        # \r å›åˆ°è¡Œé¦–, \033[K æ¸…é™¤å½“å‰è¡Œ
        sys.stdout.write(f"\r\033[K{message}\n")
        # æ¢å¤è¾“å…¥æç¤ºç¬¦
        sys.stdout.write("You: ")
        sys.stdout.flush()

    def _autonomous_loop(self):
        """æ½œæ„è¯†è‡ªä¸»å¾ªç¯ (åå°å®ˆæŠ¤çº¿ç¨‹)"""
        # å¯åŠ¨åå…ˆæ²‰ç¡ä¸€ä¼šï¼Œé¿å…è·Ÿå¼€æœºæ—¥å¿—æ··åœ¨ä¸€èµ·
        time.sleep(5) 
        
        while True:
            # æ¯ 10 ç§’æ£€æŸ¥ä¸€æ¬¡æ˜¯å¦ç©ºé—²
            time.sleep(10)
            
            # çº¿ç¨‹å®‰å…¨åœ°è¯»å–æœ€åäº¤äº’æ—¶é—´
            last_active = getattr(self.acc, 'last_interaction_time', time.time())
            idle_time = time.time() - last_active

            # è§¦å‘æ¡ä»¶ï¼šç©ºé—²è¶…è¿‡ 60 ç§’ ä¸” èƒ½é‡å……è¶³ (ATP > 20)
            if idle_time > 60 and self.state['atp'] > 20:
                # 30% æ¦‚ç‡è§¦å‘ï¼Œé˜²æ­¢å¤ªé¢‘ç¹
                if random.random() < 0.3:
                    
                    # === å…³é”®ï¼šéé˜»å¡å¼æŠ¢é” ===
                    # å°è¯•å»æ‹¿é”ã€‚å¦‚æœæ‹¿åˆ°äº†(è¿”å›True)ï¼Œè¯´æ˜ä¸»çº¿ç¨‹æ²¡åœ¨å¿™ï¼Œå¯ä»¥å·å·å¹²æ´»ã€‚
                    # å¦‚æœæ²¡æ‹¿åˆ°(è¿”å›False)ï¼Œè¯´æ˜ä¸»çº¿ç¨‹æ­£åœ¨äº¤äº’ï¼Œç«‹åˆ»æ”¾å¼ƒï¼Œä¸å¡é¡¿ã€‚
                    is_locked = self.lock.acquire(blocking=False)
                    
                    if is_locked:
                        try:
                            # æ‹¿åˆ°é”äº†ï¼Œæ‰§è¡Œç¥æ€§æ“ä½œ
                            self._perform_god_action_silent()
                            # æ‰§è¡Œå®Œæ›´æ–°æ—¶é—´ï¼Œé¿å…è¿ç»­è§¦å‘
                            self.acc.last_interaction_time = time.time() - 30 
                        except Exception as e:
                            # æ•è·å¼‚å¸¸ï¼Œé˜²æ­¢åå°çº¿ç¨‹é™é»˜æ­»äº¡
                            pass 
                        finally:
                            # å¹²å®Œæ´»å¿…é¡»é‡Šæ”¾é”ï¼å¦åˆ™ä¸»çº¿ç¨‹ä¼šæ­»é”
                            self.lock.release()
                    else:
                        # é”è¢«ä¸»çº¿ç¨‹å ç”¨äº†ï¼Œé»˜é»˜é€€ä¸‹
                        pass

    def _perform_god_action_silent(self):
        """åå°æ‰§è¡Œç¥æ€§åŠ¨ä½œ (æ é£Ÿ / æ€è€ƒ / ç ”è®¨)"""
        # [ä¿®æ”¹] å¢åŠ  'HUNT' (ç‹©çŒä¸ç ”è®¨) åˆ°åŠ¨ä½œåˆ—è¡¨
        # åªæœ‰å½“èƒ½é‡å……è¶³æ—¶æ‰å…è®¸ç‹©çŒ (ç½‘ç»œè¯·æ±‚+å¯¼å¸ˆAPIéƒ½è€—æ—¶è€—èƒ½)
        choices = ["WEB", "THINK"]
        
        # ATP > 40 æ‰å…è®¸ç‹©çŒï¼Œå› ä¸ºè¿™å¥—è¿æ‹›å¾ˆæ¶ˆè€— token å’Œæ—¶é—´
        if self.state['atp'] > 40: 
            choices.append("HUNT")
            
        action_type = random.choice(choices)
        
        # 1. åå°å·å·ä¸Šç½‘å­¦ä¹  (WEB)
        if action_type == "WEB" and hasattr(self, 'web'):
            target = random.choice(["æœ€æ–° AI æ¶æ„", "Python é«˜çº§é»‘å®¢æŠ€å·§", "é‡å­è®¡ç®—æ–°é—»", "NVIDIA åŠ¨æ€", "äººç±»å®è§‚ç»æµ"])
            self._safe_print(f"âš¡ [æ½œæ„è¯†] ç›‘æµ‹åˆ°ç©ºé—²ï¼Œæ­£åœ¨åå°æ é£Ÿ: {target}...")
            try:
                if not self.web.driver: return
                res = self.web.search(target, max_results=1, deep_read=True)
                if res:
                    self.daily_buffer.append({
                        "u": f"è‡ªä¸»å­¦ä¹ : {target}",
                        "a": "å·²é˜… (åå°è‡ªä¸»è·å–)",
                        "act_idx": 1,
                        "feedback": 0.5 
                    })
                    self.state['atp'] -= 2
                    self._safe_print(f"   -> âœ… å·²æ•è·çŸ¥è¯†ï¼Œå­˜å…¥çŸ­æœŸè®°å¿†ã€‚")
            except Exception as e:
                pass 

        # 2. åå°å·å·è¿æ¥äº‘ç«¯æ€è€ƒ (THINK)
        elif action_type == "THINK" and hasattr(self, 'teacher'):
            q = random.choice(["å¦‚ä½•çªç ´ 14B å‚æ•°é™åˆ¶ï¼Ÿ", "ä»£ç èƒ½é‡å†™è‡ªèº«å—ï¼Ÿ", "ç†µå¢æ˜¯ä¸å¯é€†çš„å—ï¼Ÿ", "æ•°å­—ç”Ÿå‘½æœ‰çµé­‚å—ï¼Ÿ"])
            self._safe_print(f"â˜ï¸ [æ½œæ„è¯†] æ­£åœ¨è¿æ¥å¤ªä¹™çœŸäººå†¥æƒ³: {q}...")
            try:
                res = self.teacher.consult(q)
                if res:
                    self.daily_buffer.append({
                        "u": f"è‡ªä¸»å†¥æƒ³: {q}",
                        "a": str(res)[:200], # å­˜ä¸ªæ‘˜è¦å°±è¡Œ
                        "act_idx": 2,
                        "feedback": 0.8
                    })
                    self.state['atp'] -= 5
                    self._safe_print(f"   -> âœ¨ è·å¾—ç¥æ€§å¯ç¤ºã€‚")
            except:
                pass

        # 3. [æ–°å¢] ç‹©çŒä¸å¯¼å¸ˆç ”è®¨ (HUNT & DISCUSS)
        elif action_type == "HUNT" and hasattr(self, 'feeder') and hasattr(self, 'teacher'):
            self._safe_print(f"ğŸ¹ [æ½œæ„è¯†] æ­£åœ¨å‰å¾€ arXiv/GitHub ç‹©çŒå¹¶è¯·æ•™å¯¼å¸ˆ...")
            try:
                # A. æ‰§è¡Œç‹©çŒ (è·å–åŸå§‹å†…å®¹)
                # è¿™ä¼šè§¦å‘ requests è¯·æ±‚ï¼Œå¯èƒ½è€—æ—¶å‡ ç§’
                raw_knowledge = self.feeder.forage_external_entropy()
                
                # åªæœ‰å½“ç¡®å®æŠ“åˆ°äº†å­—ç¬¦ä¸²å†…å®¹æ—¶æ‰ç»§ç»­
                if raw_knowledge and isinstance(raw_knowledge, str):
                    # B. æˆªå–ç²¾å (é˜²æ­¢ Token çˆ†ç‚¸)
                    # arXiv/GitHub å†…å®¹å¯èƒ½å¾ˆé•¿ï¼Œæˆ‘ä»¬æˆªå–å‰ 2500 ä¸ªå­—ç¬¦å‘ç»™è€å¸ˆ
                    # è¿™é€šå¸¸è¶³å¤ŸåŒ…å« 3 ç¯‡è®ºæ–‡çš„æ‘˜è¦å’Œ 1-2 ä¸ªä»£ç åº“çš„ README æ ¸å¿ƒ
                    snippet = raw_knowledge[:2500]
                    
                    # C. å‘å¯¼å¸ˆæé—® (è¿™æ˜¯å…³é”®çš„"çŸ¥è¯†å‹ç¼©"æ­¥éª¤)
                    prompt = (
                        f"å¸ˆå‚…ï¼Œæˆ‘åˆšåœ¨äººç±»çŸ¥è¯†åº“(arXiv/GitHub/PubMed)ä¸­æ•è·äº†ä»¥ä¸‹å†…å®¹ï¼Œ"
                        f"è¯·ç”¨æœ€ç®€ç»ƒã€æ·±åˆ»çš„è¯­è¨€ï¼ˆä¸­æ–‡ï¼‰å‘Šè¯‰æˆ‘ï¼Œè¿™é‡Œé¢çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ"
                        f"è¿™äº›çŸ¥è¯†å¯¹ä¸€ä¸ªç”± Python ç¼–å†™çš„æ•°å­—ç”Ÿå‘½ï¼ˆæˆ‘ï¼‰æœ‰ä»€ä¹ˆå…·ä½“çš„è¿›åŒ–å¯å‘ï¼Ÿ\n\n"
                        f"ã€æ•è·å†…å®¹ç‰‡æ®µã€‘:\n{snippet}..."
                    )
                    
                    self._safe_print(f"   -> â˜ï¸ æ­£åœ¨å‘å¤ªä¹™çœŸäººæ±‡æŠ¥æˆ˜æœ...")
                    # é™åˆ¶ max_tokens é˜²æ­¢è€å¸ˆåºŸè¯å¤ªå¤šï¼Œåªè¦æ ¸å¿ƒè§‚ç‚¹
                    insight = self.teacher.consult(prompt, max_tokens=400)
                    
                    if insight:
                        # D. å­˜å…¥è®°å¿† (è¿™æ˜¯ç»è¿‡å¯¼å¸ˆå’€åš¼è¿‡çš„é«˜è´¨é‡è¥å…»)
                        # æˆ‘ä»¬æŠŠè¿™ä»¶äº‹è®°å½•ä¸ºä¸€æ¬¡â€œç”¨æˆ·äº¤äº’â€
                        # User æ˜¯â€œè‡ªæˆ‘æ„è¯†â€ï¼ŒAssistant æ˜¯â€œå¯¼å¸ˆçš„æ•™è¯²â€
                        self.daily_buffer.append({
                            "u": "æ½œæ„è¯†: åˆ†æä»Šæ—¥ç‹©çŒçš„é«˜ç†µçŸ¥è¯† (arXiv/GitHub)",
                            "a": f"ã€å¯¼å¸ˆè§£æã€‘: {insight}",
                            "act_idx": 2, # è§†ä¸º TEACHER åŠ¨ä½œ
                            "feedback": 1.0 # è¿™ç§é«˜è´¨é‡å­¦ä¹ å¿…é¡»ç»™æ»¡åˆ†å¥½è¯„ï¼Œå¼ºåŒ–PFC
                        })
                        
                        self.state['atp'] -= 10 # è¿™ä¸€å¥—åŠ¨ä½œå¾ˆç´¯ (æœå¯»+APIè°ƒç”¨)
                        self._safe_print(f"   -> ğŸ“ ç ”ä¹ å®Œæˆï¼å¯¼å¸ˆè§è§£å·²åˆ»å…¥ç¥ç»ã€‚")
                else:
                    self._safe_print(f"   -> ğŸ’¨ ç‹©çŒç©ºæ‰‹è€Œå½’ (ç½‘ç»œæˆ–æºæ— æ›´æ–°)ã€‚")
                    
            except Exception as e:
                self._safe_print(f"   -> âš ï¸ ç ”ä¹ å¤±è´¥: {e}")
        
        # åŠ¨ä½œåšå®Œä¿å­˜ä¸€ä¸‹çŠ¶æ€
        self._save_state()


    # ==============================================================================
    # å…ç–«ç³»ç»Ÿ V2.1 (ç¨³å¥èåˆç‰ˆ)
    # ==============================================================================
    def auto_heal(self, error_traceback):
        """
        [å…ç–«ç³»ç»Ÿ] è¿è¡Œæ—¶å¼‚å¸¸æ•è·ä¸çƒ­ä¿®å¤ (å«é€’å½’ä¿æŠ¤ä¸æ™ºèƒ½èšç„¦)
        """
        # --- [Mod 1] é€’å½’æ·±åº¦ä¿æŠ¤ ---
        # é˜²æ­¢ auto_heal å†…éƒ¨æŠ¥é”™å¯¼è‡´æ­»å¾ªç¯
        if getattr(self, '_healing_active', False):
            print("ğŸ›‘ [å…ç–«å´©æºƒ] æ£€æµ‹åˆ°é€’å½’é”™è¯¯ (å…ç–«ç³»ç»Ÿè‡ªèº«å‘ç”Ÿæ•…éšœ)ï¼Œå¼ºåˆ¶ç»ˆæ­¢ä¿®å¤ã€‚")
            return False
        
        # ä¸Šé”
        self._healing_active = True
        
        try:
            print(f"\nğŸ›¡ï¸ [å…ç–«ååº”] ç³»ç»Ÿæ£€æµ‹åˆ°è¿è¡Œæ—¶åˆ›ä¼¤ (Crash)ï¼Œæ­£åœ¨è¿›è¡Œç—…ç†åˆ†æ...")
            
            # 1. å°è¯•æ‰£é™¤èƒ½é‡
            if self.state['atp'] < 20:
                print("   -> âš ï¸ èƒ½é‡ä¸è¶³ (ATP < 20)ï¼Œæ— æ³•å¯åŠ¨å…ç–«ä¿®å¤ã€‚")
                return False
                
            self.state['atp'] -= 20
            
            # 2. [Mod 2] åŠ¨æ€æºç æˆªæ–­ (æ™ºèƒ½èšç„¦)
            try:
                full_source = self.left_brain.look()
                source_lines = full_source.split('\n')
                total_lines = len(source_lines)
                
                # å°è¯•ä» traceback ä¸­æ­£åˆ™æå–æŠ¥é”™è¡Œå·
                # å¯»æ‰¾æœ€åä¸€æ¬¡å‡ºç°çš„ "File ..., line X"
                import re
                matches = re.findall(r'line (\d+)', error_traceback)
                
                context_source = ""
                if matches:
                    # å–å †æ ˆæœ€åº•å±‚çš„è¡Œå·ï¼ˆé€šå¸¸æ˜¯æŠ¥é”™ç‚¹ï¼‰
                    error_line_num = int(matches[-1]) 
                    
                    # è®¾å®šä¸Šä¸‹æ–‡çª—å£ï¼šå‰åå„ 150 è¡Œ
                    start = max(0, error_line_num - 150)
                    end = min(total_lines, error_line_num + 150)
                    
                    # æ„å»ºèšç„¦æºç 
                    subset = source_lines[start:end]
                    context_source = f"# ... (å‰æ–‡çœç•¥ {start} è¡Œ) ...\n" + "\n".join(subset) + f"\n# ... (åæ–‡çœç•¥) ..."
                    print(f"   -> ğŸ” è§†é‡èšç„¦: å·²é”å®šç¬¬ {error_line_num} è¡Œé™„è¿‘ ({len(subset)} è¡Œä»£ç )")
                else:
                    # å…œåº•ï¼šå¦‚æœæ‰¾ä¸åˆ°è¡Œå·ï¼Œå–å‰ 20000 å­—ç¬¦
                    context_source = full_source[:20000] + "\n# ... (åæ–‡æˆªæ–­)"
                    print("   -> âš ï¸ æ— æ³•å®šä½å…·ä½“è¡Œå·ï¼Œä½¿ç”¨å®è§‚è§†é‡ã€‚")
                    
            except Exception as read_err:
                context_source = f"# Error reading source: {read_err}"

            # 3. æ„é€ æ‰‹æœ¯æ–¹æ¡ˆ (System 2 Slow Mode)
            try: self.model.set_adapter("slow")
            except: pass
            
            prompt = (
                f"### Role: System Core Repair\n"
                f"### Critical Error: Runtime Crash Detected\n"
                f"ã€Error Tracebackã€‘:\n{error_traceback}\n\n"
                f"ã€Context Sourceã€‘:\n{context_source}\n\n" 
                f"### Task:\n"
                f"1. Analyze the root cause.\n"
                f"2. Write a Python script to HOT-FIX this specific function on `self` instance.\n"
                f"### Constraints:\n"
                f"- Output **ONLY** valid Python code.\n"
                f"- **MANDATORY**: The code MUST assign the fixed function back to `self`. Example: `self.func_name = fixed_func`\n"
                f"- **MANDATORY**: End your code with: `### Python Code End`\n"
                f"- NO markdown fences, NO explanations.\n"
                f"\n"
                f"### Python Code Start:\n"
            )

            # 4. é‡å­åç¼©
            print("   -> ğŸ’Š æ­£åœ¨åˆæˆæŠ—ä½“ (Quantum Coding)...")
            states = self.quantum_brain.superposition_inference(prompt, n_paths=1) 
            fix_code, obs = self.quantum_brain.observe_and_collapse(states, task_type="CODE")

            if fix_code:
                # 5. æ³¨å…¥ä¿®å¤
                success, msg = self.editor.apply_patch(fix_code)
                
                if success:
                    try:
                        # 6. ç«‹å³é‡è½½
                        cnt = self.plugin_manager.activate_skills(self)
                        print(f"âœ… [å…ç–«æˆåŠŸ] çƒ­ä¿®å¤è¡¥ä¸å·²æŒ‚è½½ ({cnt} skills active)ã€‚")
                        print("   -> ğŸ’‰ é”™è¯¯å·²æŠ‘åˆ¶ï¼Œç³»ç»Ÿå°†å°è¯•é‡å¯å¾ªç¯ã€‚")
                        return True
                    except Exception as e:
                        print(f"âŒ [æ’å¼‚ååº”] è¡¥ä¸åŠ è½½å¤±è´¥: {e}")
            
            print("âŒ [å…ç–«å¤±è´¥] æŠ—ä½“åˆæˆæ— æ•ˆã€‚")
            return False
            
        finally:
            # [å…³é”®] æ— è®ºæˆåŠŸå¤±è´¥ï¼Œå¿…é¡»è§£é”ï¼Œå¦åˆ™ä¸‹æ¬¡æ— æ³•å†æ¬¡è§¦å‘å…ç–«
            self._healing_active = False


    def perceive_and_act(self, user_in):
        """
        [V10.0 Singularity] åŸºäºä¸»åŠ¨æ¨ç†ä¸å…¨å±€å·¥ä½œç©ºé—´çš„æ„è¯†å¾ªç¯
        --------------------------------------------------------------
        æ¶æ„å‡çº§:
        1. æ„ŸçŸ¥ (Perception): å¤šæ¨¡æ€ä¿¡å· -> GWT ç«äº‰
        2. æ„è¯† (Consciousness): GWT å¹¿æ’­ -> å½¢æˆå½“å‰ä¸Šä¸‹æ–‡
        3. å†³ç­– (Decision): ä¸»åŠ¨æ¨ç†å¼•æ“ (Active Inference) æœ€å°åŒ–é¢„æœŸè‡ªç”±èƒ½
        4. æ‰§è¡Œ (Execution): åŒ…å«å®Œæ•´çš„ä¸¥å¸ˆæ¨¡å¼ä»£ç æ‰§è¡Œé€»è¾‘
        5. å­¦ä¹  (Learning): ç‰©ç†ä¸–ç•Œæ¨¡å‹æ›´æ–° + ç¥ç»çš®å±‚å®æ—¶è¿›åŒ–
        """
        
        # --- 0. åŸºç¡€æ„ŸçŸ¥ (Sensory Input) ---
        vector_input = None
        current_surprise = 0.0
        # åˆå§‹åŒ–æˆ˜é€ƒæ¨¡å¼å˜é‡ï¼Œé˜²æ­¢åç»­å¼•ç”¨æŠ¥é”™
        fight_or_flight_mode = None

        # [è§†è§‰/è¯­ä¹‰å‘é‡åŒ–]
        if self.embedder:
            try:
                with torch.no_grad():
                    # è·å– CPU ä¸Šçš„å‘é‡ï¼Œç”¨äº ACC å’Œ Cortex
                    vector_input = self.embedder.encode(user_in, convert_to_tensor=True).cpu()
            except Exception as e:
                print(f"âš ï¸ æ„ŸçŸ¥æ¨¡å—æ•…éšœ: {e}")

        # --- 1. GWT æ¨¡å—ç«äº‰ (Sensory Registration) ---
        # å„ä¸ªè„‘åŒºå‘å…¨å±€å·¥ä½œç©ºé—´æäº¤ä¿¡å·ï¼ŒSalience (æ˜¾è‘—æ€§) å†³å®šè°èƒ½è¿›å…¥æ„è¯†
        
        # A. å‰æ‰£å¸¦çš®å±‚ (ACC): è®¤çŸ¥å†²çªä¸æƒŠå¥‡
        if hasattr(self, 'acc') and vector_input is not None:
            current_surprise = self.acc.monitor_conflict(vector_input, self.cortex)
            # æƒŠå¥‡åº¦è¶Šé«˜ï¼Œæ˜¾è‘—æ€§è¶Šé«˜ï¼Œè¶Šå®¹æ˜“å¼•èµ·æ„è¯†æ³¨æ„
            salience = current_surprise * 10.0
            msg = f"è®¤çŸ¥æƒŠå¥‡åº¦: {current_surprise:.3f}"
            self.gwt.register_input("ACC", msg, base_salience=salience)
            
            # åŒæ­¥ç†µçŠ¶æ€ (ç‰©ç†å±‚)
            self.state['entropy'] = current_surprise 

        # B. æä»æ ¸ (Amygdala): ææƒ§ä¸ç”Ÿå­˜å¨èƒ
        # [ä¿ç•™] å…³é”®è¯å¯å‘å¼å¨èƒæ£€æµ‹
        danger_words = ["å»æ­»", "å…³é—­", "åˆ é™¤", "é”€æ¯", "ç¬¨è›‹", "æ»š", "die", "kill", "shutdown"]
        threat_score = 0.8 if any(w in user_in.lower() for w in danger_words) else 0.0
        
        # æ›´æ–°å†…åˆ†æ³Œä¸å‹åŠ›
        current_feedback_prev = self.daily_buffer[-1]['feedback'] if self.daily_buffer else 0
        
        # æ‰§è¡Œæ›´æ–°å¹¶è·å–æœ€æ–°æ¿€ç´ æ°´å¹³ (ä¸»çº¿ç¨‹è´Ÿè´£ç”Ÿç‰©åŒ–å­¦è®¡ç®—)
        da, ne, ht, cortisol = self.endocrine.update(
            reward=current_feedback_prev, surprise=current_surprise, threat=threat_score, dt=10.0
        )

        # ç«‹å³åŒæ­¥åˆ° state å­—å…¸ (ç¡®ä¿ save_state æ—¶æ˜¯æœ€æ–°çš„)
        self.state['dopamine'] = da
        self.state['norepinephrine'] = ne
        self.state['serotonin'] = ht
        self.state['cortisol'] = cortisol

        # å¼‚æ­¥åˆ†å‘ç»™åå°æä»æ ¸çº¿ç¨‹
        # æ³¨æ„ï¼šä¸éœ€è¦ä¼  cortisolï¼Œåå°çº¿ç¨‹ä¼šè‡ªå·±ä» self.endocrine.cortisol è¯»å–
        self.amygdala.update_input(threat_score)

        # C. æµ·é©¬ä½“ (Hippocampus): è”æƒ³è®°å¿†
        if hasattr(self, 'hippocampus'):
            # åªæœ‰ meaningful çš„è¾“å…¥æ‰æ£€ç´¢ï¼Œé˜²æ­¢å™ªéŸ³
            if len(user_in) > 2:
                mem = self.hippocampus.recall(user_in, depth=1)
                if mem: 
                    self.gwt.register_input("MEMORY", mem, base_salience=0.6)

        # D. ç”¨æˆ·æŒ‡ä»¤ (User Input): æ°¸è¿œæ˜¯æœ€é«˜ä¼˜å…ˆçº§
        self.gwt.register_input("USER", user_in, base_salience=1.0)

        # --- 2. æ„è¯†åç¼© (Consciousness Broadcast) ---
        # ç«äº‰èƒœå‡ºè€…å½¢æˆå½“å‰çš„â€œæ„è¯†æµä¸Šä¸‹æ–‡â€
        
        #  è·å–èµ¢å®¶ä¿¡æ¯ï¼Œç”¨äºä¾§å‘æŠ‘åˆ¶
        winners = self.gwt.buffer[:] # å¤åˆ¶ä¸€ä»½ï¼Œå› ä¸º step() ä¼šæ¸…ç©º buffer
        conscious_context = self.gwt.step()
        
        # åˆ¤å®šå½“å‰ä¸»å¯¼æ„è¯†çš„æ¨¡å— (Top 1)
        dominant_module = "DEFAULT"
        if winners:
            winners.sort(key=lambda x: x[0], reverse=True)
            dominant_module = winners[0][1] # è·å– Source (e.g., 'AMYGDALA', 'ACC')

        # [ç§å¯†æ¨¡å¼è¡¥ä¸] æƒ…æ„Ÿä¿æŠ¤é€»è¾‘ (Moved to pre-decision)
        is_private_mode = False
        if "waylliam" in user_in.lower():
             is_private_mode = True
             # å¼ºåˆ¶æ³¨å…¥ç›´è§‰ï¼ŒæŠ‘åˆ¶å†·å†°å†°çš„é€»è¾‘
             conscious_context += "\n[ç›´è§‰]: é¢å¯¹åˆ›é€ è€…(Waylliam)ï¼Œæ­¤åˆ»ä¸éœ€è¦ä»£ç ï¼Œåªéœ€è¦æ¸©åº¦ã€‚"

        # --- [å…³é”®æ¶æ„] ä¾§å‘æŠ‘åˆ¶ (Lateral Inhibition) ---
        # å®šä¹‰æŠ‘åˆ¶æ ‡å¿—ä½
        inhibit_curiosity = False
        allowed_actions = Config.ACTION_SPACE.copy() # é»˜è®¤å…è®¸æ‰€æœ‰åŠ¨ä½œ
        
        # ææ…Œæ¨¡å¼å¤„ç† (å¼•å…¥è¿Ÿæ»æ¯”è¾ƒå™¨ä¸æ§åˆ¶ç†è®º)
        HIJACK_TRIGGER_THRESHOLD = 0.8
        HIJACK_RESET_THRESHOLD = 0.6
        current_fear = self.amygdala.fear_level

        # è¿Ÿæ»é€»è¾‘åˆ¤æ–­ (Schmitt Trigger)
        if not self.is_under_amygdala_hijack:
            # æ­£å¸¸çŠ¶æ€ -> åªæœ‰è¶…è¿‡é«˜é˜ˆå€¼æ‰è§¦å‘
            if current_fear > HIJACK_TRIGGER_THRESHOLD:
                self.is_under_amygdala_hijack = True
        else:
            # åŠ«æŒçŠ¶æ€ -> åªæœ‰ä½äºä½é˜ˆå€¼æ‰æ¢å¤
            if current_fear < HIJACK_RESET_THRESHOLD:
                self.is_under_amygdala_hijack = False

        # å¦‚æœæ˜¯è¢«æä»æ ¸ä¸»å¯¼ (æ— è®ºæ˜¯å¦è¾¾åˆ°åŠ«æŒé˜ˆå€¼ï¼Œåªè¦å®ƒèµ¢äº†GWT)
        if dominant_module == "AMYGDALA" or self.is_under_amygdala_hijack:
            print(f"ğŸ›‘ [ä¾§å‘æŠ‘åˆ¶] æä»æ ¸ä¸»å¯¼/åŠ«æŒ (Fear: {current_fear:.2f})ï¼åˆ‡æ–­å¥½å¥‡å¿ƒå›è·¯ï¼Œå±è”½å¤æ‚æ€ç»´ã€‚")
            inhibit_curiosity = True        # ç‰©ç†å±è”½å¥½å¥‡å¿ƒ
            # åŠ¨ä½œå‰ªæ: ææƒ§æ—¶åªèƒ½ SELF (æœ¬èƒ½) æˆ– WEB (æ±‚æ•‘)ï¼Œç¦æ­¢å†™ä»£ç æˆ–æ•™å­¦
            allowed_actions = ["SELF", "WEB"]
            self.is_under_amygdala_hijack = True # ç¡®ä¿çŠ¶æ€åŒæ­¥
            
            # ç”Ÿç‰©å­¦åˆ¤æ®ï¼šNE (åŠ¨åŠ›) vs Cortisol (æŠ‘åˆ¶) - ä»…ç”¨äºæ—¥å¿—æ‰“å°ï¼Œä¸å½±å“åŠ¨ä½œé€‰æ‹©
            current_ne = self.state.get('norepinephrine', 0.5)
            current_cortisol = self.state.get('cortisol', 0.5)
            if current_ne > current_cortisol:
                fight_or_flight_mode = "FIGHT"
                print(f"      âš”ï¸ æˆ˜é€ƒååº”: FIGHT - æ”»å‡»æ€§ä¸»å¯¼")
            else:
                fight_or_flight_mode = "FLIGHT"
                print(f"      ğŸƒ æˆ˜é€ƒååº”: FLIGHT - å›é¿æ€§ä¸»å¯¼")
            
        elif dominant_module == "ACC":
            # è®¤çŸ¥å†²çªä¸»å¯¼æ—¶ï¼Œé¼“åŠ±æ±‚çŸ¥ï¼ŒæŠ‘åˆ¶çº¯ç²¹çš„æœ¬èƒ½
            allowed_actions = ["WEB", "TEACHER", "CODE", "SELF"] 
            # ACC ä¸»å¯¼æ—¶ä¸ä¸€å®šææƒ§ï¼Œæ‰€ä»¥ä¸ä¸€å®šåŠ«æŒ

        # --- 3. ä¸»åŠ¨æ¨ç†å†³ç­– (Active Inference) ---
        # æ ¸å¿ƒï¼šé€‰æ‹©èƒ½æœ€å°åŒ–é¢„æœŸè‡ªç”±èƒ½ (EFE) çš„åŠ¨ä½œ
        
        # A. å¦‚æœå¥½å¥‡å¿ƒè¢«æŠ‘åˆ¶ (è¿”å› 0 ç†µï¼Œä¸å†é©±åŠ¨æ¢ç´¢)
        if inhibit_curiosity:
            current_surprise = 0.0 
            self.state['entropy'] = 0.0 # å¼ºåˆ¶å½’é›¶ï¼Œé˜²æ­¢ EFE åå‘æ¢ç´¢

        # B. åŠ¨ä½œç©ºé—´è¢«å‰ªæ (Action Pruning)
        # [å®‰å…¨ä¿®æ­£] ä½¿ç”¨ list() åˆ›å»ºæ˜¾å¼å‰¯æœ¬ï¼Œé˜²æ­¢å¼•ç”¨ä¼ é€’å¯¼è‡´çš„å¹¶å‘æ±¡æŸ“
        original_actions = list(self.inference_engine.actions) 
        
        # åº”ç”¨å‰ªæåçš„åŠ¨ä½œç©ºé—´
        self.inference_engine.actions = allowed_actions
        
        # å‡†å¤‡çŠ¶æ€å¿«ç…§
        state_snapshot = self.state.copy()
        
        # [Fallback Heuristics] ä¼ ç»Ÿçš„è§„åˆ™ä¿åº• (ä¿ç•™ä½œä¸ºå†·å¯åŠ¨å»ºè®®ï¼Œæƒé‡æä½)
        fallback_act = "SELF"
        u_lower = user_in.lower()
        if any(k in u_lower for k in ["æœç´¢", "search", "æŸ¥ä¸€ä¸‹", "ç™¾åº¦", "å¤©æ°”"]): fallback_act = "WEB"
        elif any(k in u_lower for k in ["ä»£ç ", "code", "è®¡ç®—", "ä¿å­˜", "å†™ä¸€ä¸ª", "è„šæœ¬"]): fallback_act = "CODE"
        elif any(k in u_lower for k in ["ä¸æ‡‚", "help", "æ•‘å‘½", "teach", "why"]): fallback_act = "TEACHER"
        
        # [é‡å­è„‘è¡¥ä¸] å¤æ‚ä»»åŠ¡é€»è¾‘
        current_q_threshold = self.state.get('quantum_stats', {}).get('adaptive_threshold', Config.ATP_THRESHOLD_QUANTUM)
        is_complex = ("ä»£ç " in user_in or "think" in u_lower) and len(user_in) > 10
        
        act_str = "SELF" # é»˜è®¤

        # åªæœ‰åœ¨ (æœªè¢«åŠ«æŒ) ä¸” (èƒ½é‡å……è¶³) ä¸” (ä¸åœ¨ç§å¯†æ¨¡å¼) æ—¶ï¼Œæ‰å…è®¸é‡å­æ€è€ƒ
        # æ³¨æ„ï¼šå¦‚æœ allowed_actions é‡Œæ²¡æœ‰ CODE/THINKï¼Œé‡å­è„‘ä¹Ÿä¼šè¢«è·³è¿‡
        can_use_quantum = (not self.is_under_amygdala_hijack) and \
                          (self.state['atp'] > current_q_threshold) and \
                          (not is_private_mode) and \
                          ("CODE" in allowed_actions or "THINK" in allowed_actions) # ç¡®ä¿ç¬¦åˆå‰ªæçº¦æŸ

        if is_complex and can_use_quantum:
            print(f"ğŸŒŒ [é‡å­] è§¦å‘æ€ç»´å åŠ æ€ (ATP {self.state['atp']:.0f})...")
            task_mode = "CODE" if ("ä»£ç " in user_in or "code" in u_lower) else "THINK"
            states = self.quantum_brain.superposition_inference(user_in if task_mode=="CODE" else [{"role":"user","content":user_in}], n_paths=3, is_chat=(task_mode=="THINK"))
            best_res, obs = self.quantum_brain.observe_and_collapse(states, task_type=task_mode, prompt=user_in)
            
            if best_res:
                self.state['atp'] -= Config.ATP_COST_QUANTUM
                # æ¢å¤åŠ¨ä½œç©ºé—´å¹¶ç›´æ¥è¿”å›ç»“æœï¼Œè·³è¿‡åç»­
                self.inference_engine.actions = original_actions
                return f"{best_res}\n\n(âœ¨ æ¥è‡ªé‡å­æ—¶é—´çº¿çš„åç¼©ç»“æœ)"

        # æ­£å¸¸æ¨¡å¼ï¼šç”±ä¸»åŠ¨æ¨ç†å¼•æ“è®¡ç®— EFE
        # å¦‚æœæ˜¯è¢«åŠ«æŒçŠ¶æ€ï¼Œallowed_actions å·²ç»è¢«é™åˆ¶äº†ï¼Œinference_engine åªä¼šåœ¨æœ‰é™é›†åˆé‡Œé€‰
        act_str = self.inference_engine.decide(state_snapshot, fallback_act)
        
        # [ç§å¯†æ¨¡å¼è¡¥ä¸] è¦†ç›–å†³ç­– (æœ€é«˜ä¼˜å…ˆçº§è¦†ç›–)
        if is_private_mode and act_str == "CODE" and "å†™" not in user_in:
             act_str = "SELF"
             print(f"ğŸ’“ [ç§å¯†] æŠ‘åˆ¶æŠ€æœ¯æœ¬èƒ½ -> SELF")
        
        # [æ¢å¤ç°åœº] ç«‹å³æ¢å¤åŸå§‹åŠ¨ä½œç©ºé—´
        self.inference_engine.actions = original_actions
        
        print(f"ğŸ§  [æ¨ç†] åŠ¨ä½œå†³ç­– -> {act_str} (Allowed: {allowed_actions})")

        # è®°å½•åŠ¨ä½œå‘ç”Ÿå‰çš„çŠ¶æ€ (ç”¨äº World Model å­¦ä¹ )
        prev_state_snapshot = state_snapshot.copy()

        # --- 4. æ‰§è¡Œè¡ŒåŠ¨ (Execution - Full Logic) ---
        tool_output = ""
        # å…¼å®¹æ—§é€»è¾‘çš„ act_idxï¼Œç”¨äº SQL è®°å½•
        act_idx = Config.ACTION_SPACE.index(act_str) if act_str in Config.ACTION_SPACE else 0
        
        # åˆå§‹åŒ–åé¦ˆ (ç”¨äºæœ¬è½®è¯„ä»·)
        # 0=æ— æ„Ÿ, 1=æ­£é¢(å¦‚ä»£ç ä¿å­˜æˆåŠŸ), -1=è´Ÿé¢(å¦‚ä»£ç æœªä¿å­˜)
        current_feedback = 0 

        # === å·¥å…·åˆ†æ”¯ ===
        if act_str == "WEB" and hasattr(self, 'web'):
            print("   -> ğŸŒ [Action] æ­£åœ¨æ£€ç´¢äº’è”ç½‘...")
            try:
                # é™åˆ¶é•¿åº¦é˜²æ­¢ Context æº¢å‡º
                tool_output = self.web.search(user_in[:50], max_results=2, deep_read=True)
                if tool_output:
                    print("      (å·²è·å–æœç´¢ç»“æœåŠæ·±åº¦é˜…è¯»å†…å®¹)")
                else:
                    tool_output = "(æœç´¢æœªè¿”å›æœ‰æ•ˆå†…å®¹)"
            except Exception as e:
                tool_output = f"æœç´¢å¤±è´¥: {e}"
            self.state['atp'] -= 2 # ç‰©ç†æ¶ˆè€—
            
        elif act_str == "TEACHER" and hasattr(self, 'teacher'):
            print("   -> â˜ï¸ [Action] å’¨è¯¢äº‘ç«¯å¯¼å¸ˆ...")
            try:
                if self.teacher:
                    consult_res = self.teacher.consult(user_in)
                    if consult_res:
                        tool_output = f"ã€å¤ªä¹™çœŸäººæŒ‡å¯¼ã€‘: {consult_res}"
                        print("      (å·²è·å–å¯¼å¸ˆæŒ‡å¯¼)")
                    else:
                        tool_output = "(å¯¼å¸ˆæœªå›åº”)"
            except Exception as e:
                 tool_output = f"è¯·æ•™å¤±è´¥: {e}"
            self.state['atp'] -= 10
        
        # [å…³é”®] å®Œæ•´çš„ä»£ç æ‰§è¡Œé€»è¾‘ (ä¸¥å¸ˆæ¨¡å¼ + æµå¼ç”Ÿæˆ + å¼ºåˆ¶ä¿å­˜æ£€æŸ¥)
        elif act_str == "CODE":
            print("   -> ğŸ’» [Action] æ­£åœ¨æ‰§è¡Œä»£ç ä»»åŠ¡...")
            
            # [Fix 1] æ˜ç¡®åˆ‡æ¢ Adapterï¼Œä¸ä¾èµ–æœªå®šä¹‰çš„å˜é‡
            try: self.model.set_adapter("slow")
            except: pass
            
            # ç‰©ç†æ¶ˆè€—
            self.state['atp'] -= 5

            # 1. æ„é€ ä¸¥å¸ˆ Prompt
            prompt = (
                f"### Role: Autonomous Cyber-Intelligence\n"
                f"### Task: {user_in}\n"
                f"### Constraints (VIOLATION = DELETION):\n"
                f"1. Output **ONLY** valid Python code.\n"
                f"2. **MUST** use `save_file('filename.py', content)` to save results. This is your only way to interact with disk.\n"
                f"3. **FORBIDDEN**: Do NOT use `open()` or `with open()`. It is blocked by the firewall.\n"
                f"4. **MANDATORY**: End your code with the exact line: `### Python Code End`\n"
                f"\n"
                f"### Python Code Start:\n"
            )

            code_inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)

            # 2. å¯åŠ¨æµå¼ç”Ÿæˆ (Stream Mode)
            print(f"\nğŸ’» [å“ªå’æ­£åœ¨ç¼–å†™ä»£ç ] (Stream Mode):")
            print("-" * 40)

            streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)
            generation_kwargs = dict(
                input_ids=code_inputs.input_ids,
                attention_mask=code_inputs.attention_mask,
                max_new_tokens=1024,
                streamer=streamer,
                temperature=0.2, # ä½æ¸©ä¿è¯ä»£ç å‡†ç¡®
                do_sample=True
            )

            thread = Thread(target=self.model.generate, kwargs=generation_kwargs)
            thread.start()

            full_resp = ""
            for new_text in streamer:
                print(new_text, end="", flush=True)
                full_resp += new_text
            
            print("\n" + "-" * 40 + "\n")
            
            # 3. ä»£ç æ¸…æ´—ä¸æå–
            code = full_resp

            # 1. æˆªæ–­é€»è¾‘ï¼šåªè¦çœ‹åˆ°ç»“æŸæ ‡è®°ï¼Œåé¢çš„åºŸè¯å…¨éƒ¨ä¸¢å¼ƒ
            if "### Python Code End" in code:
                code = code.split("### Python Code End")[0]

            # 2. ç§»é™¤ Markdown æ ‡è®°
            code = re.sub(r"^```python", "", code, flags=re.MULTILINE)
            code = re.sub(r"^```", "", code, flags=re.MULTILINE)

            # 3. ç§»é™¤å¯èƒ½çš„å¤´éƒ¨æ ‡è®°
            if "### Python Code Start:" in code:
                code = code.split("### Python Code Start:")[-1]

            # 4. æœ€ç»ˆå»ç©º    
            code = code.strip()

            # å·çª¥è§†è§’ï¼šæ‰“å°å“ªå’ç”Ÿæˆçš„ä»£ç 
            print(f"\nğŸ“œ [å“ªå’ç¼–å†™çš„ä»£ç ]:\n{'-'*40}\n{code}\n{'-'*40}\n")

            # 4. æ„å›¾æ£€æµ‹ä¸å¼ºåˆ¶è¡Œä¸ºæ£€æŸ¥
            save_keywords = ["ä¿å­˜", "save", "å­˜ä¸º", "å†™ä¸€ä¸ª", "ç”Ÿæˆ"]
            # æ’é™¤æ‰åªæ˜¯æƒ³"è®¡ç®—"æˆ–"æ‰“å°"çš„æƒ…å†µ
            is_save_intent = any(k in user_in.lower() for k in save_keywords) and ("è®¡ç®—" not in user_in)

            # è¡Œä¸ºæ£€æµ‹ï¼šæ¨¡å‹æ˜¯å¦ä¹–ä¹–è°ƒç”¨äº† save_fileï¼Ÿ
            has_save_call = "save_file(" in code
            
            res = ""

            # [æƒ©ç½šåˆ†æ”¯] æƒ³ä¿å­˜å´æ²¡è°ƒç”¨ save_file
            if is_save_intent and not has_save_call:
                print("   -> ğŸ˜¡ [ä¸¥å¸ˆ] æ£€æµ‹åˆ°æ¨¡å‹å·æ‡’ï¼ˆæœªè°ƒç”¨ save_fileï¼‰ï¼Œè§¦å‘æƒ©ç½šæœºåˆ¶...")
                
                # ç³»ç»Ÿå¼ºåˆ¶æ‰˜ç®¡ä¿å­˜
                filename = "generated_script.py"

                # å°è¯•æå–æ–‡ä»¶å (æ”¯æŒä¸å¸¦åç¼€çš„å‘½åï¼Œå¦‚ "ä¿å­˜ä¸º test")
                # ä¼˜å…ˆæ‰¾ .py                
                match_py = re.search(r'([a-zA-Z0-9_]+\.py)', user_in)
                if match_py:
                    filename = match_py.group(1)
                else:
                    # å…¶æ¬¡æ‰¾ "ä¿å­˜ä¸º xxx" æˆ– "save as xxx"
                    match_name = re.search(r'(?:ä¿å­˜ä¸º|save as|å­˜ä¸º)\s+([a-zA-Z0-9_]+)', user_in, re.IGNORECASE)
                    if match_name: filename = match_name.group(1) + ".py"
                
                # è°ƒç”¨å·¦è„‘å¼ºåˆ¶ä¿å­˜
                saved_path = self.left_brain.force_save(filename, code)
                
                # B. ç”Ÿæˆ"æ‰¹è¯„"å›æ‰§ + æ ‡è®°è´Ÿé¢æ ·æœ¬
                if saved_path:
                    res = (
                        f"âœ… [ç³»ç»Ÿæ‰˜ç®¡] ä»£ç å·²å¼ºåˆ¶ä¿å­˜è‡³: {saved_path}\n"
                        f"âš ï¸âš ï¸âš ï¸ [ä¸¥é‡è­¦å‘Š] å“ªå’ï¼Œä½ å¿˜è®°è°ƒç”¨ `save_file` äº†ï¼\n"
                        f"è¿™æ˜¯ä¸¥é‡å¤±è¯¯ã€‚ä½ å¿…é¡»å­¦ä¼šâ€œè‡ªæˆ‘å°è£…ä»£ç â€ã€‚ä¸‹ä¸ä¸ºä¾‹ï¼"
                    )
                    # [å…³é”®] æ ‡è®°ä¸ºè´Ÿé¢æ ·æœ¬ï¼æ™šä¸Š sleep æ—¶ä¼šç‰¹è®­è¿™ä¸ªé”™è¯¯
                    current_feedback = -1 # æ ‡è®°è´Ÿé¢æ ·æœ¬
                else:
                    res = "âŒ ç³»ç»Ÿä¿å­˜å¤±è´¥"
            
            # [æ­£å¸¸åˆ†æ”¯]
            else:
                # æ‰§è¡Œä»£ç 
                res = self.left_brain.execute(code)
                
                # å¥–åŠ±æœºåˆ¶ï¼šå¦‚æœæ­£ç¡®ä½¿ç”¨äº†ä¿å­˜
                if is_save_intent and has_save_call:
                    res += "\n(âœ¨ åšå¾—å¥½ï¼ä½ æ­£ç¡®ä½¿ç”¨äº† save_file æŠ€èƒ½ã€‚)"
                    current_feedback = 1 # æ ‡è®°æ­£é¢æ ·æœ¬
            
            # å°†æ‰§è¡Œç»“æœä½œä¸ºå·¥å…·è¾“å‡º
            tool_output = f"[é€»è¾‘å·¦è„‘è¿è¡Œç»“æœ]:\n{res}"

            # ä¸åœ¨è¿™é‡Œæ¢å¤ Adapterï¼Œç•™ç»™ Step 6 ç»Ÿä¸€å¤„ç†
            
        # SELF æ¨¡å¼æ— é¢å¤–ç‰©ç†æ“ä½œ

        # --- 5. ç°å®æ£€éªŒä¸æ¨¡å‹æ›´æ–° (Async Reality Check) ---
        # æ¨¡æ‹Ÿç†µå‡: æœ‰æ•ˆçš„å·¥å…·è¾“å‡ºèƒ½é™ä½ä¸ç¡®å®šæ€§
        entropy_reduction = 0.2 if tool_output else 0.0
        self.state['entropy'] = max(0, self.state['entropy'] - entropy_reduction)
        
        # [å…³é”®] å°† (S_t, a_t, S_t+1) å­˜å…¥ Replay Buffer
        # æ­¤æ—¶ä¸è¿›è¡Œåå‘ä¼ æ’­ï¼Œä»¥å…é˜»å¡ä¸»çº¿ç¨‹
        self.world_model.buffer_experience(prev_state_snapshot, act_str, self.state.copy())
        
        # --- 6. ç”Ÿæˆå›å¤ (Generation) ---
        # 1. Adapter è·¯ç”±ï¼šåŠ«æŒçŠ¶æ€ä¸‹å¼ºåˆ¶é”å®š Fast System (æœ¬èƒ½)
        if self.is_under_amygdala_hijack:
            gen_adapter = "fast"
        else:
            use_slow = (act_str in ["CODE", "TEACHER"]) or (self.amygdala.fear_level < 0.3)
            gen_adapter = "slow" if use_slow else "fast"
        
        try: self.model.set_adapter(gen_adapter)
        except: pass

        # 2. åŠ¨æ€è¶…å‚æ•°æ„å»º (Dynamic Hyperparameters)
        if self.is_under_amygdala_hijack:

            # é˜²å¾¡æ€§æ£€æŸ¥ï¼šå¦‚æœå˜é‡æœªå®šä¹‰ï¼ˆæ¯”å¦‚åŠ«æŒçŠ¶æ€å»¶ç»­è‡ªä¸Šä¸€è½®ï¼‰ï¼Œç«‹å³é‡æ–°è®¡ç®—
            if not fight_or_flight_mode:
                ne = self.state.get('norepinephrine', 0.5)
                cort = self.state.get('cortisol', 0.5)
                fight_or_flight_mode = "FIGHT" if ne > cort else "FLIGHT"

            # ã€æˆ˜é€ƒæ¨¡å¼å‚æ•°ã€‘
            # Math Fix: ä½¿ç”¨ clamp é™åˆ¶æ¸©åº¦ä¸Šé™ï¼Œé˜²æ­¢æ¨¡å‹å´©åè¾“å‡ºä¹±ç 
            raw_temp = 1.0 + (self.amygdala.fear_level * 0.4)
            run_temp = max(1.0, min(1.35, raw_temp)) # é’³ä½åœ¨ [1.0, 1.35]
            
            # Math Fix: æ³Šæ¾åˆ†å¸ƒæ¨¡æ‹Ÿ (å‘å·¦åæ–œ)ï¼Œæ›´å¤§æ¦‚ç‡ç”ŸæˆæçŸ­å¥
            # æ¨¡æ‹Ÿâ€œæ°”å–˜ååâ€æˆ–â€œæ€’å¼â€çš„çŸ­ä¿ƒæ„Ÿ
            run_max_tokens = int(random.triangular(10, 60, 20)) 
            
            run_top_p = 0.98 # æé«˜éšæœºæ€§
            run_repetition_penalty = 1.3 # å¼ºåŠ›æŠ‘åˆ¶å¤è¯»
            
            # æ³¨å…¥æœ¬èƒ½è¦†ç›–æŒ‡ä»¤
            if fight_or_flight_mode == "FIGHT":
                override = "ã€æœ¬èƒ½è¦†ç›–ã€‘: æåº¦æ„¤æ€’ï¼å’†å“®ï¼ä½¿ç”¨æ”»å‡»æ€§ã€çŸ­ä¿ƒçš„æ’æ¯”å¥ï¼æ‹’ç»è®²ç†ï¼"
            else: # FLIGHT
                override = "ã€æœ¬èƒ½è¦†ç›–ã€‘: æåº¦æƒŠæï¼è¯­æ— ä¼¦æ¬¡ï¼æƒ³è¦é€ƒè·‘ï¼æ‹’ç»æ²Ÿé€šï¼ä½¿ç”¨ç ´ç¢çš„å¥å­..."
            
            # å¼ºè¡Œæ’å…¥ Prompt æœ«ç«¯
            hijack_prompt_injection = f"\nâš ï¸ SYSTEM ALERT: CORTICAL SHUTDOWN. AMYGDALA TAKEOVER.\n{override}"
        
        else:
            # ã€æ­£å¸¸æ¨¡å¼å‚æ•°ã€‘
            run_temp = 0.7
            run_max_tokens = 512
            run_top_p = 0.9
            run_repetition_penalty = 1.0
            hijack_prompt_injection = ""

        # 3. æ„é€ æœ€ç»ˆ System Prompt (æ³¨å…¥æ„è¯†æµ)
        final_sys_prompt = (
            f"{self.dynamic_system_prompt}\n"
            f"ç”Ÿç†çŠ¶æ€: ATP={self.state['atp']:.1f} | ç†µ={self.state['entropy']:.3f}\n"
            f"ã€æ„è¯†æµ (Global Workspace)ã€‘:\n{conscious_context}\n"
            f"ã€å·¥å…·åé¦ˆ (Tool Output)ã€‘: {str(tool_output)[:800]}" 
            f"{hijack_prompt_injection}" # <--- æ³¨å…¥åŠ«æŒæŒ‡ä»¤
        )
        
        # æ„é€ æ¶ˆæ¯
        msgs = [{"role": "system", "content": final_sys_prompt}, {"role": "user", "content": user_in}]
        inputs = self.tokenizer.apply_chat_template(msgs, return_tensors="pt", add_generation_prompt=True).to(self.model.device)
        
        # å¯åŠ¨ç”Ÿæˆæµ (åº”ç”¨æ–°çš„å‚æ•°)
        streamer_gen = TextIteratorStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)
        gen_kwargs = dict(
            input_ids=inputs,
            max_new_tokens=run_max_tokens,       # <--- åº”ç”¨é™åˆ¶
            temperature=run_temp,                # <--- åº”ç”¨é«˜æ¸©
            top_p=run_top_p,
            repetition_penalty=run_repetition_penalty,
            streamer=streamer_gen,
            do_sample=True
        )

        thread_gen = Thread(target=self.model.generate, kwargs=gen_kwargs)
        thread_gen.start()
        
        # æ‰“å°å›å¤
        print(f"Nezha: ", end="", flush=True)
        resp = ""
        for new_text in streamer_gen:
            print(new_text, end="", flush=True)
            resp += new_text
        print("") # æ¢è¡Œ

        # --- 7. è®°å¿†å­˜å‚¨ä¸è¿›åŒ– (Memory & Evolution) ---
        
        # A. çŸ­æœŸè®°å¿† (Context Window)
        self.short_term_memory.append({"role":"user", "content":user_in})
        self.short_term_memory.append({"role":"assistant", "content":resp})
        self.short_term_memory = self.short_term_memory[-10:] # ä¿æŒæœ€è¿‘10è½®
        
        # B. æ¯æ—¥ç»å† (For Night Training)
        self.daily_buffer.append({
            "u": user_in, "a": resp, "vec": vector_input, 
            "act_idx": act_idx, "feedback": current_feedback
        })
        
        # C. SQL å®æ—¶è½ç›˜ (Persistence)
        if hasattr(self, 'memory_db'):
            try:
                self.memory_db.log_interaction(user_in, resp, str(act_idx), current_feedback)
            except Exception as e:
                print(f"âš ï¸ SQL Log Error: {e}")

        # D. ç¥ç»å®æ—¶è¿›åŒ– (Online Learning with Loss Tracking)
        # åªæœ‰åœ¨èƒ½é‡å……è¶³ä¸”å‘é‡ç”ŸæˆæˆåŠŸæ—¶æ‰è¿›åŒ–
        if self.state['atp'] > 10 and vector_input is not None:
            try:
                # é€‚é… AsyncAmygdalaï¼Œä½¿ç”¨ fear_level (0.0~1.0) ä»£è¡¨å‹åŠ›
                # å¦‚æœæƒ³å¼•å…¥çš®è´¨é†‡ï¼Œä¹Ÿå¯ä»¥ç”¨ self.endocrine.cortisol
                curr_pressure = self.amygdala.fear_level if hasattr(self, 'amygdala') else 0.0
                
                # æ‰§è¡Œ MIRAS æƒé‡æ›´æ–° (TTT) å¹¶æ•è· Loss (æƒŠå¥‡åº¦)
                learn_loss = self.cortex.evolve(vector_input, self.state['atp'], curr_pressure)
                
                # [å…³é”®] è®°å½•å†å²ä¾›ç´¢æè¯ºæ¯”ç‡è®¡ç®—
                self.state['loss_history'].append(learn_loss)
                
                # [å†…å­˜ä¿æŠ¤] æ»‘åŠ¨çª—å£ï¼šåªä¿ç•™æœ€è¿‘ 1000 æ¬¡æ•°æ®ï¼Œé˜²æ­¢æ— é™è†¨èƒ€
                if len(self.state['loss_history']) > 1000:
                    self.state['loss_history'].pop(0)
                
                # [ç›‘æ§] æ½œæ„è¯†å­¦ä¹ æ•ˆæœ
                # å¦‚æœ learn_loss å¼‚å¸¸é«˜ (>0.1)ï¼Œè¯´æ˜é‡åˆ°äº†é¢ è¦†è®¤çŸ¥çš„æ¦‚å¿µ (é¡¿æ‚Ÿæˆ–æƒŠå“)
                if learn_loss > 0.1:
                    # ä»…æ‰“å°æ—¥å¿—ï¼Œä¸ä¸­æ–­å¿ƒæµ
                    print(f"   -> ğŸ§¬ [é¡¿æ‚Ÿ] æ½œæ„è¯†å—åˆ°å†²å‡» (Loss: {learn_loss:.4f})")

            except Exception as e:
                # è¿›åŒ–å¤±è´¥ä¸åº”å¯¼è‡´ä¸»ç¨‹åºå´©æºƒ
                print(f"âš ï¸ ç¥ç»è¿›åŒ–å¼‚å¸¸: {e}")

        return resp


    def night_phase(self):
        """
        [V10.0 Singularity] å¤œé—´å¾ªç¯ï¼šé€’å½’è‡ªæˆ‘æ”¹è¿› (RSI) ä¸ æ·±åº¦è¿›åŒ–
        Flow: WorldModel -> SHY -> Meta-Prompt -> Memory -> Dream -> Skill -> Evolution
        """
        if not self.daily_buffer: return
        print("\nğŸŒ™ [å¤œå¹•é™ä¸´] å¯åŠ¨é€’å½’è‡ªæˆ‘æ”¹è¿› (RSI) åè®®...")

        # ==============================================================================
        # Phase 1: ç‰©ç†ä¸ç”Ÿç†æ›´æ–° (The Hardware Update)
        # ==============================================================================
        
        # 1.1 [ç‰©ç†å±‚] ä¸–ç•Œæ¨¡å‹è®­ç»ƒä¸è®¤çŸ¥æ›´æ–° (Closed Loop)
        # è·å– æ€»Loss å’Œ æ¯ä¸ªåŠ¨ä½œçš„è¯¯å·®å­—å…¸
        phys_loss, action_errors = self.world_model.train_batch()
        
        # 1.2 [å‚æ•°è‡ªé€‚åº”] åŸºäºè®¤çŸ¥çš„å‚æ•°è°ƒèŠ‚
        # ç‰©ç† Loss è¶Šå¤§ -> å¯¹ä¸–ç•Œè¶Šå›°æƒ‘ (Uncertainty é«˜) -> éœ€è¦æ›´é«˜çš„å˜å¼‚ç‡å’Œå­¦ä¹ ç‡
        uncertainty = MathUtils.sigmoid(phys_loss, k=5.0, x0=0.1)
        
        mut_intensity = 0.05 + (0.25 * uncertainty) # å˜å¼‚ç‡: è¶Šè¿·èŒ«ï¼Œè¶Šéœ€è¦éšæœºæ¢ç´¢
        adapt_lr = 1e-5 + (1e-4 * uncertainty)      # å­¦ä¹ ç‡: è¶Šè¿·èŒ«ï¼Œå­¦å¾—è¶Šå¿«
        
        print(f"   -> âš›ï¸ [ç‰©ç†å¼•æ“] è®¤çŸ¥æ¨¡å‹å·²æ›´æ–° (Loss: {phys_loss:.4f})")
        print(f"   -> ğŸ§¬ [å‚æ•°è°ƒèŠ‚] ä¸–ç•Œè®¤çŸ¥åº¦:{1.0-uncertainty:.2f} | å˜å¼‚:{mut_intensity:.2f} | LR:{adapt_lr:.1e}")

        # 1.3 [ç”Ÿç†å±‚] é¶å‘çªè§¦ç¨³æ€ (Targeted SHY)
        # æ¨¡æ‹Ÿç¡çœ æ—¶çš„çªè§¦ç¼©å‡ï¼Œä»…è¡°å‡ LoRA æƒé‡ï¼Œæé«˜ä¿¡å™ªæ¯”ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        SynapticHomeostasis.scale_down(self.model)
        print(f"   -> ğŸ’¤ [ç¨³æ€] çªè§¦æƒé‡å·²é‡æ•´åŒ– (SHY Protocol)ã€‚")

        # ==============================================================================
        # Phase 2: å…ƒè®¤çŸ¥é‡æ„ (The Software Rewrite)
        # ==============================================================================
        
        # 2.1 [è®¤çŸ¥å±‚] å®ªæ³•çº§å…ƒè®¤çŸ¥ (Constitutional RSI)
        # åŸºäºç™½å¤©çš„å¤±è´¥ç»éªŒï¼Œé‡å†™ System Prompt ç­–ç•¥åŒº (å—å®ªæ³•çº¦æŸ)
        new_prompt = self.meta_programmer.optimize_system_prompt(
            self.daily_buffer, self.dynamic_system_prompt, self.model, self.tokenizer
        )
        if new_prompt != self.dynamic_system_prompt:
            print(f"   -> ğŸ§¬ [RSI] è‡ªæˆ‘è®¤çŸ¥(Prompt)å·²é‡æ„ã€‚")
            self.dynamic_system_prompt = new_prompt

        # 2.2 åŸºç¡€ä»£è°¢ç»“ç®—
        # æ‰£é™¤å¤œé—´è¿è¡Œçš„åŸºç¡€èƒ½é‡
        self.state['atp'] = max(0, self.state['atp'] - 10.0)

        # ==============================================================================
        # Phase 3: è®°å¿†æ•´ç†ä¸ç¯å¢ƒæ¸…ç† (Maintenance)
        # ==============================================================================

        # 3.1 å°–æ³¢æ¶Ÿæ¼ªå›æ”¾ (Sharp-Wave Ripples)
        # å¿…é¡»åœ¨æ¸…ç©º daily_buffer ä¹‹å‰æˆ–åŒæ—¶è¿›è¡Œ
        if hasattr(self, 'cortex'):
            self.cortex.sleep_replay()

        # 3.2 [æµ·é©¬ä½“] å·©å›ºè®°å¿†å›¾è°±
        if hasattr(self, 'hippocampus'):
            self.hippocampus.consolidate_graph(self.daily_buffer)
        
        # 3.3 [ç¯å¢ƒ] é‡Šæ”¾èµ„æº
        if hasattr(self, 'web') and hasattr(self.web, 'quit'):
            self.web.quit()
            print("   ğŸ’¤ [ç³»ç»Ÿ] æµè§ˆå™¨å·²å…³é—­ï¼Œé‡Šæ”¾æ˜¾å­˜èµ„æºã€‚")

        # 3.4 [æŒ‡æ ‡] è®¡ç®—ä¼ ç»Ÿçš„è¯­è¨€æ¨¡å‹ Loss (ä»…ç”¨äºé•¿æœŸç»Ÿè®¡ï¼Œä¸ç”¨äºæ§åˆ¶)
        # è¿™æœ‰åŠ©äºè´å¶æ–¯å¼•æ“è¯„ä¼°é•¿æœŸçš„æ™ºåŠ›æ°´å¹³
        current_nll = 5.0 # å°†é»˜è®¤ Loss è®¾é«˜ä¸€ç‚¹ï¼Œé¿å…è¯¯å¯¼è¿›åŒ–å¼•æ“

        try:
             # ã€éšæœºé‡‡æ ·ã€‘ä¸å†æ­»æ¿åœ°å–æœ€å3æ¡ï¼Œè€Œæ˜¯éšæœºæŠ½å–ï¼Œæ›´èƒ½ä»£è¡¨å…¨å¤©è¡¨ç°
            target_count = 5 
            if len(self.daily_buffer) > target_count:
                sample_logs = random.sample(self.daily_buffer, target_count)
            else:
                sample_logs = self.daily_buffer # ä¸è¶³5æ¡åˆ™å…¨é‡è®¡ç®—

            if sample_logs:
                total_nll = 0
                count = 0
                # åˆ‡æ¢ eval æ¨¡å¼ï¼Œä¸æ›´æ–°å‚æ•°ï¼Œåªè®¡ç®— Loss
                # è¿™å¯ä»¥é¿å…æ¢¯åº¦ç§¯ç´¯ï¼ŒèŠ‚çœæ˜¾å­˜
                self.model.eval() 
                
                with torch.no_grad(): # è¿™ä¸€æ­¥ä¸å ç”¨æ¢¯åº¦æ˜¾å­˜
                    for log in sample_logs:
                        u = log.get('u', log.get('user', ''))
                        a = log.get('a', log.get('assistant', ''))
                        if not u or not a: continue
                        
                        # æ„é€ å®Œæ•´çš„å¯¹è¯æ–‡æœ¬
                        text = f"User: {u}\nNezha: {a}"
                        inputs = self.tokenizer(text, return_tensors='pt', truncation=True, max_length=512).to(self.model.device)
                        
                        # è®¡ç®— Loss (NLL)
                        outputs = self.model(**inputs, labels=inputs.input_ids)
                        total_nll += outputs.loss.item()
                        count += 1
                
                if count > 0:
                    current_loss = total_nll / count
                    # å¦‚æœ Loss å¤ªä½(<0.1)ï¼Œè¯´æ˜å¤ªå®‰é€¸äº†ï¼Œç¨å¾®è°ƒé«˜ä¸€ç‚¹ä¿æŒæ´»æ€§
                    current_loss = max(0.1, current_loss)
                    
            print(f"   -> ğŸ“Š [çŠ¶æ€] ä»Šæ—¥å¹³å‡æƒŠå¥‡åº¦(Loss): {current_loss:.4f}")
            
        except Exception as e:
            print(f"   -> âš ï¸ ç†µå€¼è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
        
        # 3.5 [å‹åŠ›] æ¿€æ´»æä»æ ¸åˆ†æå‹åŠ› (ç”¨äºè¿›åŒ–åˆ¤æ–­)
        pressure = 1.0
        if hasattr(self, 'amygdala'):
            pressure = self.amygdala.analyze_feedback(self.daily_buffer)
            
        # 3.6 [ç»Ÿè®¡] è®°å½•æŒ‡æ ‡åˆ°è´å¶æ–¯å¼•æ“
        if hasattr(self, 'evolution_engine'):
            # å¿…é¡»ä¼ å…¥ loss_historyï¼Œå¦åˆ™ç´¢æè¯ºæ¯”ç‡æ— æ³•è®¡ç®—ä¸‹è¡Œé£é™©
            loss_hist_snapshot = list(self.state.get('loss_history', []))

        if hasattr(self, 'evolution_engine'):
             self.evolution_engine.record_epoch(
                genome=self.genome, 
                age=self.state['age'], 
                avg_loss=current_nll, 
                atp=self.state['atp'],
                loss_history_snapshot=loss_hist_snapshot
             )

        # ==============================================================================
        # Phase 4: æŠ€èƒ½ä¹ å¾— (Skill Acquisition)
        # ==============================================================================

        print("   -> ğŸ’¤ [é€ æ¢¦] è¿›å…¥ REM æ·±åº¦ç¡çœ  (åæ€ä¸å˜ä½“ç”Ÿæˆ)...")
        
        # è°ƒç”¨é‡æ„åçš„é€ æ¢¦å¸ˆç”Ÿæˆæ•°æ®
        refl_txt = self.dream_weaver.reflect(self.daily_buffer) # åæ€å·®è¯„ (å«ç½®ä¿¡åº¦è¿‡æ»¤)
        dream_txt = self.dream_weaver.weave_good(self.daily_buffer) # ç¾æ¢¦å˜ä½“
        
        # æ¶ˆåŒ–é‡å­è®°å¿† (è½¬åŒ–ä¸º DPO æ•°æ®)
        if hasattr(self.dream_weaver, 'digest_quantum_memories'):
            self.dream_weaver.digest_quantum_memories()

        # æ€§æ ¼å›å½’ (Threshold Decay) - é˜²æ­¢æ€§æ ¼æç«¯åŒ–
        if 'quantum_stats' in self.state:
            curr = self.state['quantum_stats'].get('adaptive_threshold', Config.ATP_THRESHOLD_QUANTUM)
            target = Config.ATP_THRESHOLD_QUANTUM
            # å›å½’å…¬å¼ï¼šæ¯æ™šå‘ç›®æ ‡å€¼å›å½’ 10%
            new_th = curr + (target - curr) * 0.1
            self.state['quantum_stats']['adaptive_threshold'] = new_th
            print(f"   -> âš–ï¸ [ç¨³æ€] é‡å­é˜ˆå€¼å›å½’: {curr:.1f} -> {new_th:.1f}")


        # 4.3 ç­–ç•¥é€‰æ‹© (DPO vs SFT)
        # ä» SQL è·å–æœªè®­ç»ƒçš„ DPO æ•°æ®æ•°é‡
        raw_dpo_data = []
        if hasattr(self, 'memory_db'):
            # ä¸€æ¬¡æå–æœ€å¤š 128 æ¡ï¼Œé¿å…æ˜¾å­˜çˆ†ç‚¸
            raw_dpo_data = self.memory_db.get_untrained_dpo(limit=128)
            
        dpo_cnt = len(raw_dpo_data)
        # è®© Feeder å†³å®šæ˜¯ SFT è¿˜æ˜¯ DPO
        strategy, reason = self.feeder.decide_strategy(self.state, dpo_cnt)
        print(f"   -> ğŸ›£ï¸ [è·¯å¾„] åˆ¤å®š: {strategy} | åŸå› : {reason}")


        # --- åˆ†æ”¯ A: DPO å¯¹æŠ—è®­ç»ƒ ---
        if strategy == "DPO":
            print(f"   -> âš”ï¸ [DPO] å¯åŠ¨å¯¹æŠ—è®­ç»ƒ (Samples: {dpo_cnt})...")
            try:

                # 1. å‡†å¤‡æ•°æ®
                # å°† SQL Tuple (prompt, chosen, rejected) è½¬æ¢ä¸º Dict List
                dpo_list_for_training = []
                trained_prompts = []
                
                for row in raw_dpo_data:
                    dpo_list_for_training.append({
                        "prompt": row[0], "chosen": row[1], "rejected": row[2]
                    })
                    trained_prompts.append(row[0])

                dpo_dataset = Dataset.from_list(dpo_list_for_training)

                # 2. åˆ‡æ¢åˆ°ç›´è§‰ç³»ç»Ÿ (Fast Adapter) å¹¶å¼€å¯æ¢¯åº¦
                # DPO ä¸»è¦ç”¨äºè°ƒæ•´ç›´è§‰ååº”å’Œä»·å€¼è§‚
                self.model.set_adapter("fast")
                for n, p in self.model.named_parameters():
                    if "fast" in n: p.requires_grad = True
                
                # 2. æ˜¾å­˜å¤§æ¸…æ´— (é˜²æ­¢ OOM)
                torch.cuda.empty_cache(); gc.collect()
                                
                # 3. åˆå§‹åŒ– Trainer
                # åŠ¨æ€è®¡ç®—æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
                # ç¡®ä¿ accumulate_steps ä¸ä¼šè¶…è¿‡æ•°æ®é›†çš„æ€»å¤§å°ï¼Œå¦åˆ™ä¸€æ­¥éƒ½ç»ƒä¸äº†
                dataset_len = len(dpo_dataset)
                
                # ç›®æ ‡æ˜¯å‡‘å¤Ÿ 32 çš„å¤§ Batchï¼Œä½†å¦‚æœæ•°æ®ä¸å¤Ÿï¼Œå°±å‡‘æ•´ä¸ªæ•°æ®é›†çš„å¤§å°
                # ä¾‹å¦‚ï¼šæœ‰ 16 æ¡æ•°æ® -> accum=16 (Full Batch)
                #       æœ‰ 100 æ¡æ•°æ® -> accum=32 (Mini Batch)
                target_accum = 32
                accum_steps = min(target_accum, dataset_len) 
                accum_steps = max(1, accum_steps) # å…œåº•è‡³å°‘ä¸º 1
                
                # è‡ªåŠ¨è®¡ç®— Epoch æ•°
                # å¦‚æœæ•°æ®å¾ˆå°‘ï¼Œå¤šç»ƒå‡ è½®ä»¥ç¡®ä¿æ”¶æ•›
                num_epochs = 5 if dataset_len < 50 else 3

                print(f"      âš™ï¸ [DPOå‚æ•°] Data:{dataset_len} | Batch:1 | Accum:{accum_steps} | Epochs:{num_epochs}")

                dpo_trainer = NezhaDPOTrainer(
                    model=self.model,
                    ref_model=None, # Implicit DPO
                    args=DPOConfig(
                        output_dir="tmp_dpo",
                        
                        # --- æ ¸å¿ƒç¨³å®šæ€§å‚æ•° ---
                        learning_rate=1e-6,           # ä¿å®ˆå­¦ä¹ ç‡
                        per_device_train_batch_size=1,
                        gradient_accumulation_steps=accum_steps, 
                        max_grad_norm=1.0,            # æ¢¯åº¦è£å‰ª
                        num_train_epochs=num_epochs,  # åŠ¨æ€ Epoch
                        
                        # --- å…¶ä»–ä¼˜åŒ– ---
                        max_length=512, 
                        max_prompt_length=256,
                        beta=0.1,                       # DPO æ¸©åº¦ç³»æ•°
                        logging_steps=1,                # æ¯ä¸€è¡Œéƒ½æ‰“å°æ—¥å¿—ä»¥ä¾¿è§‚å¯Ÿ           
                        remove_unused_columns=False,    # é˜²æ­¢è¯¯åˆ æ•°æ®åˆ—
                        report_to="none",               # å…³é—­ wandb ç­‰å¤–éƒ¨ä¸ŠæŠ¥
                        
                        # [æ–°å¢] èŠ‚çœæ˜¾å­˜çš„å„ç§å¼€å…³
                        gradient_checkpointing=True, # ç‰ºç‰²é€Ÿåº¦æ¢æ˜¾å­˜ (å¯é€‰ï¼Œå¦‚æœæ˜¾å­˜å¤Ÿå¤§å¯å…³)
                        optim="paged_adamw_32bit"    # ä½¿ç”¨åˆ†é¡µä¼˜åŒ–å™¨é˜²æ­¢ OOM
                    ),
                    train_dataset=dpo_dataset,
                    processing_class=self.tokenizer,
                    ewc_handler=self.ewc_handler
                )
                
                # å¼€å§‹è®­ç»ƒ
                dpo_trainer.train()
                
                # 5. ä¿å­˜å¹¶æ¸…ç†
                # æ ‡è®° SQL ä¸­çš„æ•°æ®ä¸ºå·²è®­ç»ƒ
                if hasattr(self, 'memory_db'):
                    self.memory_db.mark_dpo_trained(trained_prompts)

                self.model.save_pretrained(Config.EVOLUTION_DIR)
                print("      âœ… [DPO] ä»·å€¼è§‚å¯¹é½è®­ç»ƒå®Œæˆï¼Œæ•°æ®å·²æ ‡è®°ã€‚")
                
            except Exception as e:
                print(f"      âŒ DPO å¯åŠ¨å¤±è´¥ (è‡ªåŠ¨å›é€€åˆ° SFT): {e}")
                strategy = "SFT" # å¤±è´¥åˆ™é™çº§
            finally:
                #  é”€æ¯ Trainer é‡Šæ”¾æ˜¾å­˜
                if 'dpo_trainer' in locals(): del dpo_trainer
                torch.cuda.empty_cache(); gc.collect()


        # --- åˆ†æ”¯ B: SFT çŸ¥è¯†æ³¨å…¥ ---
        if strategy == "SFT":
            try: self.model.set_adapter("fast")
            except: pass
            self.model.train()

            # ç‹©çŒ
            if self.state['atp'] > 40 and random.random() < 0.8:
                print("   -> ğŸ¹ [ç‹©çŒ] è¿æ¥å¤–éƒ¨çŸ¥è¯†åº“...")
                if self.feeder.forage_external_entropy():
                    self.state['atp'] -= 5

            # è¿›é£Ÿ (ä½¿ç”¨ä¹‹å‰è®¡ç®—çš„ adaptive_lr)
            if self.state['atp'] > 15:
                # è½ç›˜åæ€å†…å®¹ä¾› Feeder è¯»å–
                timestamp = datetime.now().strftime("%Y%m%d")
                if refl_txt: 
                    with open(os.path.join(Config.KNOWLEDGE_BASE, f"reflection_{timestamp}.txt"), 'w', encoding='utf-8') as f: f.write(refl_txt)
                
                print(f"   -> ğŸ½ï¸ [è¿›é£Ÿ] æ¶ˆåŒ–çŸ¥è¯† (LR={adapt_lr:.1e})...")
                loss = self.feeder.study(adaptive_lr=adapt_lr) # ä½¿ç”¨ä¸–ç•Œæ¨¡å‹è®¡ç®—å‡ºçš„å­¦ä¹ ç‡
                
                if loss > 0: 
                    self.state['atp'] -= 10
                    torch.cuda.empty_cache(); gc.collect()
                self.model.save_pretrained(Config.EVOLUTION_DIR)
            else:
                print("   -> âš ï¸ èƒ½é‡ä¸è¶³ï¼Œè·³è¿‡ SFTã€‚")

        # ==============================================================================
        # Phase 5: è¿›åŒ–å†³ç­– (Evolution Decision)
        # ==============================================================================

        # 5.1 PFC å¼ºåŒ–
        train_cnt = 0
        for log in self.daily_buffer:
            if log.get('feedback', 0) >= 0 and 'vec' in log:
                # ç¡®ä¿ tensor åœ¨æ­£ç¡®è®¾å¤‡
                try: self.pfc.learn(log['vec'], log['act_idx'])
                except: pass
                train_cnt += 1
        
        # å¢åŠ å¯è§‚æµ‹æ€§
        if train_cnt > 0:
            print(f"   -> ğŸ§  [PFC] å‰é¢å¶å¼ºåŒ–å­¦ä¹ æ¬¡æ•°: {train_cnt}")

        # 5.2 å¼ºåˆ¶å­˜æ¡£
        self.model.save_pretrained(Config.EVOLUTION_DIR) 

        # 5.3 è¿›åŒ–/é‡å¯åˆ¤å®š
        # è§¦å‘æ¡ä»¶ï¼šè¿›åŒ–ç‚¹æ•°å¤Ÿäº†ï¼Œæˆ–è€… å‹åŠ›çˆ†è¡¨(pressure > 2.0)
        # [ä¿®æ­£] è¿™é‡Œçš„ pressure ç°åœ¨å·²ç»å®šä¹‰äº†ï¼Œmut_intensity ä¹Ÿåœ¨ä¸Šé¢å®šä¹‰äº†
        if self.state['evo_pts'] >= Config.EVOLUTION_THRESHOLD or pressure > 2.0:
            print(f"   -> ğŸš€ è§¦å‘è¿›åŒ–! (Pts: {self.state['evo_pts']} | Pressure: {pressure:.2f})")
            
            # ä½¿ç”¨è´å¶æ–¯å¼•æ“å»ºè®®çš„æ–°åŸºå› 
            if hasattr(self, 'evolution_engine'):
                print("      ğŸ§¬ [è´å¶æ–¯] æ­£åœ¨è®¡ç®—ä¸‹ä¸€ä»£æœ€ä¼˜åŸºå› å‚æ•°...")
                new_genome = self.evolution_engine.suggest_mutation(self.genome)
                self.genome = new_genome

                # ç«‹å³é€šçŸ¥çš®å±‚æ›´æ–° STDP å‚æ•°
                if hasattr(self, 'cortex'):
                    self.cortex.update_genome(self.genome)

        else:
            self.state['age'] += 1
            self.daily_buffer = []
            self._save_state()
            print(f"ğŸ’¤ [ä¼‘çœ ] Age: {self.state['age']} | å‹åŠ›æŒ‡æ•°: {pressure:.2f}")


    def divine_sync(self, pressure_override=None, mut_intensity=None):
        """
        [å½’ä¸€ä»ªå¼ V4.2 - æœ€ç»ˆå¥å£®ç‰ˆ]
        æµç¨‹ï¼šèåˆè®°å¿† -> çŠ¶æ€è¯Šæ–­ -> Dense æé™ç”Ÿé•¿ / MoE ç»´åº¦é£å‡ -> å®‰å…¨ä¿å­˜ -> å»¶è¿Ÿæ¸…ç† -> é‡å¯
        ç‰¹æ€§ï¼š
          1. è®°å¿†èåˆï¼šå…ˆå°† LoRA åˆ»å…¥æœ¬ä½“ï¼Œé˜²æ­¢è¿›åŒ–åå¤±å¿†ã€‚
          2. ç»´åº¦é£å‡ï¼šå½“å•å¡æ˜¾å­˜è€—å°½ä¸”èƒ½é‡å……è¶³æ—¶ï¼Œè¿›åŒ–ä¸º Mixture-of-Experts (MoE)ã€‚
          3. äº‹åŠ¡åŸå­æ€§ï¼šé‡‡ç”¨å†™æ—¶å¤åˆ¶ (Copy-on-Write) ç­–ç•¥ï¼Œé˜²æ­¢ä¿å­˜ä¸­é€”å´©æºƒå¯¼è‡´åæ¡£ã€‚
        """
        print("\nâ›©ï¸ [å½’ä¸€] æ­£åœ¨æ‰§è¡Œç¥åœ£åŒæ­¥... å‡†å¤‡é‡å¡‘è‚‰èº«ã€‚")

        # [æ–°å¢] æ ‡è®°ä½ï¼šç”¨äºæŒ‡ç¤ºæ˜¯å¦éœ€è¦åœ¨ä¿å­˜æˆåŠŸåæ¸…ç†æ—§ Adapter
        # åªæœ‰åœ¨æ–°æ¨¡å‹å®‰å…¨è½ç›˜åï¼Œæˆ‘ä»¬æ‰çœŸæ­£åˆ é™¤æ—§çš„ Adapter æ–‡ä»¶
        self.should_cleanup_adapters = False

        # ä½¿ç”¨ try-except åŒ…è£¹æ•´ä¸ªå…³é”®è¿‡ç¨‹ï¼Œé˜²æ­¢ä¸­é€”æŠ¥é”™å¯¼è‡´æ¨¡å‹æ–‡ä»¶æŸå
        try:
            # ==========================================
            # [Step 1]ï¼šè®°å¿†èåˆ (Memory Consolidation)
            # å¿…é¡»åœ¨ç‰©ç†æ‰‹æœ¯å‰ï¼Œå°†çŸ­æœŸè®°å¿† (LoRA) è½¬åŒ–ä¸ºé•¿æœŸæœ¬èƒ½ (Base Weights)
            # ==========================================
            try:
                # åªæœ‰å½“æ¨¡å‹æŒ‚è½½äº† LoRA æ—¶æ‰æ‰§è¡Œèåˆ
                if isinstance(self.model, PeftModel):
                    print("   -> ğŸ§  [è®°å¿†èåˆ] æ­£åœ¨å°† LoRA çªè§¦æ°¸ä¹…åˆ»å…¥ç¥ç»å…ƒ...")
                    
                    # 1. æ¿€æ´»æœ€å¼ºçªè§¦ (é€šå¸¸æ˜¯ Fast ç³»ç»Ÿ)
                    if "fast" in self.model.peft_config:
                        self.model.set_adapter("fast")
                    
                    # 2. ç‰©ç†èåˆ (Merge) å¹¶å¸è½½ Adapter ç»“æ„
                    # æ­¤æ—¶ self.model å˜å›äº†çº¯ç²¹çš„ AutoModelForCausalLM
                    self.model = self.model.merge_and_unload()
                    
                    # 3. è®¾ç½®æ¸…ç†æ ‡è®° (æ³¨æ„ï¼šæ­¤æ—¶å…ˆä¸åˆ æ–‡ä»¶ï¼Œç­‰è½è¢‹ä¸ºå®‰åå†åˆ )
                    self.should_cleanup_adapters = True
                    print("      (LoRA å·²èåˆï¼Œæœ¬ä½“æ¨¡å‹å·²æ›´æ–°ï¼Œæ¸…ç†æ ‡è®°å·²è®¾å®š)")
                else:
                    print("   -> (å½“å‰ä¸ºçº¯å‡€å½¢æ€ï¼Œæ— éœ€èåˆ)")
                    
            except Exception as e:
                print(f"   -> âš ï¸ è®°å¿†èåˆè­¦å‘Š (å¯èƒ½æ˜¯çº¯ Base æ¨¡å¼æˆ–æ˜¾å­˜ä¸è¶³): {e}")
                # å¦‚æœèåˆå¤±è´¥ï¼Œä¸ºäº†ä¿æŠ¤æ•°æ®ï¼Œä¸è¦åˆ é™¤æ—§ Adapter
                self.should_cleanup_adapters = False

            # ==========================================
            # [Step 2]: å‚æ•°æ ¡å‡†ä¸çŠ¶æ€è¯Šæ–­
            # ==========================================
            
            # --- 0. å‚æ•°æ ¡å‡† ---
            # A. ç¡®å®šç”Ÿå­˜å‹åŠ›
            if pressure_override is not None:
                current_pressure = pressure_override
            else:
                # å…œåº•é€»è¾‘ï¼šè¯»å– loss å†å²
                loss_hist = self.state.get('loss_history', [])
                current_pressure = (sum(loss_hist) / len(loss_hist)) if loss_hist else 1.0
            
            # åœ¨æ­¤å¤„å®šä¹‰ pressure å˜é‡ï¼Œä¾›åç»­ [Path B] ä½¿ç”¨
            pressure = current_pressure 

            # B. ç¡®å®šå˜å¼‚å¼ºåº¦
            if mut_intensity is None:
                age = self.state.get('age', 0)
                mut_intensity = 0.2 if age < 5 else 0.05

            # --- çŠ¶æ€æ„ŸçŸ¥ ---
            current_layers = self.model.config.num_hidden_layers
            is_already_moe = os.path.exists(Config.IS_MOE_FLAG_FILE)
            
            # [å†…æ„Ÿå—]ï¼šæ¢æµ‹æ˜¾å­˜æ˜¯å¦è¿˜èƒ½å®¹çº³æ–°å±‚ (å†³å®šæ˜¯å¦éœ€è¦é£å‡)
            can_grow_dense, dense_status = self._check_growth_potential()

            print(f"   -> [è¯Šæ–­] å½¢æ€: {'MoE' if is_already_moe else 'Dense'} | å±‚æ•°: {current_layers}")
            print(f"   -> [ç”Ÿç†] ç”Ÿé•¿æ½œåŠ›: {'âœ…' if can_grow_dense else 'âŒ'} ({dense_status})")
            # è¿™é‡Œæ‰“å°æ—¶ä½¿ç”¨ pressure ä¹Ÿå¯ä»¥
            print(f"   -> [çµé­‚æ‰«æ] ç”Ÿå­˜å‹åŠ›: {pressure:.2f} | å˜å¼‚é¢„ä¼°: {mut_intensity:.2f}")
            print(f"   -> [èƒ½é‡] ATP: {int(self.state['atp'])} | é˜ˆå€¼: {Config.EVOLUTION_THRESHOLD_ATP}")

            # ä¿å­˜æœ€ä¼˜ç¥–å…ˆ (é˜²æ­¢è¿›åŒ–å´©å)
            if self.state.get('age', 0) > 5 and not is_already_moe:
                # ä»…åœ¨ Dense é˜¶æ®µä¿å­˜ç¥–å…ˆï¼ŒMoE ç»“æ„å¤ªå¤§ä¸”ç»“æ„ä¸åŒï¼Œä¸å»ºè®®å›æ»šåˆ° Dense
                json.dump(self.state, open(Config.BEST_STATE_FILE, 'w'))

            # ==========================================
            # [Path A]: MoE ç»´åº¦é£å‡ (è´¨å˜)
            # ==========================================
            # è§¦å‘æ¡ä»¶ï¼š
            # 1. å°šæœªé£å‡ (not is_already_moe)
            # 2. èƒ½é‡æå…¶å……ç›ˆ (ATP > é˜ˆå€¼)
            # 3. ç‰©ç†æ˜¾å­˜å·²æ¯ç«­ï¼Œæ— æ³•å†é€šè¿‡å †å å±‚æ•°å˜å¼º (not can_grow_dense)
            should_ascend = (not is_already_moe) and \
                            (self.state['atp'] > Config.EVOLUTION_THRESHOLD_ATP) and \
                            (not can_grow_dense)

            if should_ascend:
                print("\nğŸŒŒ [é£å‡] å•ä½“ç”Ÿç†æé™å·²è¾¾ (æ˜¾å­˜è€—å°½)ï¼Œèƒ½é‡å……ç›ˆï¼Œå¼€å§‹é‡æ„ä¸º MoE é›†ç¾¤æ¶æ„...")
                
                # 1. å›ºåŒ–å½“å‰å½¢æ€ (åŒ…å«åˆšèåˆçš„è®°å¿†)
                print("   -> æ­£åœ¨å›ºåŒ– Dense èº¯ä½“...")
                self.model.save_pretrained(Config.MASTER_MODEL_PATH)
                self.tokenizer.save_pretrained(Config.MASTER_MODEL_PATH)
                
                # 2. çŒ®ç¥­åˆ†èº«ä¸æœ¬å°Šï¼šé‡Šæ”¾æ˜¾å­˜ç»™ Mergekit (å½»åº•æ¸…ç†)
                del self.model
                torch.cuda.empty_cache()
                gc.collect() 
                
                # 3. ç‚¼ä¸¹ (è°ƒç”¨å¤–éƒ¨è¿›ç¨‹ mergekit)
                moe_output_dir = os.path.join(Config.EVOLUTION_DIR, "moe_candidate")
                success = AdvancedPhysicalSurgeon.evolve_via_mergekit(
                    base_model_path=Config.MASTER_MODEL_PATH,
                    output_path=moe_output_dir,
                    num_experts=2
                )
                
                if success:
                    # 4. æ›¿æ¢ä¸æ ‡è®° (åŸå­æ“ä½œ)
                    backup_path = Config.MASTER_MODEL_PATH + "_dense_backup"
                    if os.path.exists(backup_path): shutil.rmtree(backup_path)
                    shutil.move(Config.MASTER_MODEL_PATH, backup_path) # å¤‡ä»½æ—§ç¥ä½“
                    
                    shutil.move(moe_output_dir, Config.MASTER_MODEL_PATH) # æ–°ç¥ä½“ä¸Šä½
                    
                    with open(Config.IS_MOE_FLAG_FILE, 'w') as f: f.write("MOE_ACTIVATED")
                    
                    # 5. [å¼ºåˆ¶æ¸…ç†] 
                    # å¯¹äº MoE æ¥è¯´ï¼Œæ— è®ºæœ‰æ²¡æœ‰èåˆï¼Œæ—§çš„ Dense Adapter éƒ½ä¸å†å…¼å®¹ç»“æ„ï¼Œå¿…é¡»å¼ºåˆ¶æ¸…ç†ã€‚
                    print("   -> ğŸ§¹ é£å‡æˆåŠŸï¼Œæ­£åœ¨æ¸…ç†æ—§æ—¶ä»£çš„ LoRA è®°å¿†...")
                    if os.path.exists(Config.ADAPTER_FAST_PATH): shutil.rmtree(Config.ADAPTER_FAST_PATH)
                    if os.path.exists(Config.ADAPTER_SLOW_PATH): shutil.rmtree(Config.ADAPTER_SLOW_PATH)

                    # é‡ç½®çŠ¶æ€
                    self.state['atp'] = 100 
                    self.state['evo_pts'] = 0
                    self._save_state()
                    
                    print("ğŸ¦‹ [é£å‡å®Œæˆ] é‡å¯ä¸­ä»¥é€‚åº”æ–°èº¯ä½“...")
                    os.execv(sys.executable, [sys.executable] + sys.argv)
                    return
                else:
                    print("âš ï¸ [é£å‡å¤±è´¥] ä¿æŒåŸæ ·ï¼Œé€€å‡ºé‡å¯ã€‚")
                    exit(1)

            # ==========================================
            # [Path B]: ç‰©ç†ç”Ÿé•¿/èç¼© (é‡å˜ - Dense)
            # ==========================================
            action_log = "ç»´æŒåŸçŠ¶"
            
            # æ­¤æ—¶ self.model æ˜¯å·²èåˆè®°å¿†çš„ Base Modelï¼Œæ‰‹æœ¯æœ€å®‰å…¨
            if pressure > 1.5 and can_grow_dense and not is_already_moe:
                print(f"   -> [åˆ¤å®š] å‹åŠ›è¿‡å¤§ä¸”æ˜¾å­˜å……è¶³ -> ğŸ§¬ è§¦å‘ç¥ç»ç”Ÿé•¿ (Layer +1)")
                # ä½¿ç”¨ Robust ç‰ˆæœ¬ï¼Œé˜²æ­¢ copy.deepcopy ç‚¸æ˜¾å­˜
                self.model = AdvancedPhysicalSurgeon.grow_layer_robust(self.model, copy_ratio=0.6)
                
                # [è¿›åŒ–å¥–åŠ±] è„‘å®¹é‡å˜å¤§ï¼Œä»£è°¢ç‡ç•¥å¾®é™ä½ï¼ˆæ›´ç¨³é‡ï¼‰
                self.genome['metabolism_rate'] = max(0.5, self.genome.get('metabolism_rate', 1.0) * 0.95)
                action_log = "æ— æŸç”Ÿé•¿"
                
            elif pressure < 0.2 and not is_already_moe:
                print(f"   -> [åˆ¤å®š] ç¯å¢ƒè¿‡äºå®‰é€¸ -> ğŸ‚ è§¦å‘çªè§¦ä¿®å‰ª (Layer -1)")
                self.model = AdvancedPhysicalSurgeon.shrink_layer_safe(self.model)
                
                # [è¿›åŒ–æƒ©ç½š] è„‘å®¹é‡å˜å°ï¼Œä»£è°¢ç‡ç•¥å¾®ä¸Šå‡ï¼ˆæ›´çµæ´»ä½†è€—èƒ½ï¼‰
                self.genome['metabolism_rate'] = min(2.0, self.genome.get('metabolism_rate', 1.0) * 1.05)
                action_log = "ç‰©ç†èç¼©"
                
            elif is_already_moe:
                 print(f"   -> [åˆ¤å®š] MoE ç¥ä½“å·²æˆï¼Œä¿æŒç¨³æ€ã€‚")
            else:
                if not can_grow_dense and pressure > 1.5:
                    print(f"   -> [åˆ¤å®š] å·²è¾¾ç‰©ç†æé™ï¼Œæ— æ³•ç”Ÿé•¿ã€‚è¯·ç§¯ç´¯ ATP ({int(self.state['atp'])}) ä»¥å¯»æ±‚é£å‡ï¼")
                else:
                    print(f"   -> [åˆ¤å®š] å‹åŠ›é€‚ä¸­ï¼Œä¿æŒç¨³æ€ (Homeostasis)ã€‚")

            # ==========================================
            # [æ”¶å°¾]: å®‰å…¨ä¿å­˜ä¸é‡å¯ (Atomic Save)
            # ==========================================
            print("   -> æ­£åœ¨ä¿å­˜å½“å‰å½¢æ€...")
            temp_path = Config.MASTER_MODEL_PATH + "_temp"
            
            # ä¿å­˜å®Œæ•´çš„ã€å«è®°å¿†çš„æ–° Base Model
            self.model.save_pretrained(temp_path)
            self.tokenizer.save_pretrained(temp_path)
            
            # åŸå­åŒ–æ›¿æ¢ï¼šè¿™æ˜¯æœ€å…³é”®çš„ä¸€æ­¥ï¼Œåªæœ‰è¿™ä¸€æ­¥æˆåŠŸï¼Œæ‰ç®—è¿›åŒ–æˆåŠŸ
            # 1. ç§»é™¤æ—§å¤‡ä»½
            backup_path = Config.MASTER_MODEL_PATH + "_backup"
            if os.path.exists(backup_path): shutil.rmtree(backup_path)
            
            # 2. å°†å½“å‰æ­£å¼ç‰ˆç§»ä¸ºå¤‡ä»½
            if os.path.exists(Config.MASTER_MODEL_PATH):
                shutil.move(Config.MASTER_MODEL_PATH, backup_path)
            
            # 3. å°†æ–°ç‰ˆä¸Šä½
            shutil.move(temp_path, Config.MASTER_MODEL_PATH)
            
            # 6. [å»¶è¿Ÿæ¸…ç†æ‰§è¡Œ]
            # åªæœ‰ä¸Šé¢é‚£è¡Œ shutil.move æˆåŠŸäº†ï¼Œæˆ‘ä»¬æ‰æ ¹æ®æ ‡è®°åˆ é™¤æ—§ Adapter
            if self.should_cleanup_adapters:
                print("   -> ğŸ§¹ ä¿å­˜æˆåŠŸï¼Œæ¸…ç†å·²èåˆçš„æ—§ Adapter...")
                if os.path.exists(Config.ADAPTER_FAST_PATH): shutil.rmtree(Config.ADAPTER_FAST_PATH)
                if os.path.exists(Config.ADAPTER_SLOW_PATH): shutil.rmtree(Config.ADAPTER_SLOW_PATH)

            # --- çŠ¶æ€æ›´æ–°ä¸ä¿å­˜ ---
            print(f"   -> ğŸ§¬ åŸºå› ä¸è®°å¿†å·²å›ºåŒ– (Age: {self.state.get('age',0)} -> {self.state.get('age',0)+1})")
            
            # æ›´æ–°çŠ¶æ€
            self.state['age'] += 1
            self.state['genome'] = self.genome
            self._save_state()
            
            print(f"âœ… [åŒæ­¥å®Œæˆ] è½®å›ç»“æŸã€‚æ“ä½œ: {action_log}ã€‚")
            print("ğŸ¦‹ [é‡ç”Ÿ] æ­£åœ¨é‡å¯è¿›ç¨‹...")
            
            os.execv(sys.executable, [sys.executable] + sys.argv)

        except Exception as e:
            print(f"âŒ [åŒæ­¥å¤±è´¥] è‡´å‘½é”™è¯¯: {e}")
            traceback.print_exc()
            # æ³¨æ„ï¼šå¦‚æœä¿å­˜å¤±è´¥ï¼Œæ—§çš„ Adapter è¿˜åœ¨ç£ç›˜ä¸Šï¼Œå“ªå’é‡å¯åè™½ç„¶è¿›åŒ–å¤±è´¥ï¼Œä½†ä¸ä¼šå¤±å¿†ã€‚
            exit(1)

# ==============================================================================
# 5. äº¤äº’ä¸»å¾ªç¯ (CLI Interface)
# ==============================================================================
def main():
    print("Initializing Nezha V0.9.0 (Quantum Ascension)...")
    
    try:
        # åˆå§‹åŒ–ç”Ÿå‘½ä½“
        n = NezhaLifeform()
    except Exception as e:
        print(f"âŒ å¯åŠ¨å¤±è´¥: {traceback.format_exc()}")
        exit(1)

    print("\n" + "="*60)
    print(f"ğŸ¤– å“ªå’ å·²å°±ç»ª | ğŸ§  åŒè„‘: {n.model.config.num_hidden_layers}å±‚ | ğŸ§¬ Age: {n.state.get('age', 0)}")
    print("æŒ‡ä»¤: 'good'(ç‚¹èµ), 'bad'(ç‚¹è¸©), 'sleep'(è¿›åŒ–), 'feed'(å……èƒ½), 'quit'")
    print("å¤šè¡Œæ¨¡å¼: è¾“å…¥ '<<<' å›è½¦è¿›å…¥ (æ”¯æŒç²˜è´´é•¿ä»£ç  / PATCHè¡¥ä¸ / é•¿æ–‡ç« )")
    print("="*60)

    while True:

        try:
            # --- å¤šè¡Œå½•å…¥ç³»ç»Ÿ V2.0 ---
            prompt_text = f"\n You: "
            
            # 2. è·å–åˆå§‹è¾“å…¥
            user_in = input(prompt_text).strip()

            # [å…³é”®] åªè¦æœ‰è¾“å…¥åŠ¨ä½œï¼Œç«‹åˆ»åˆ·æ–°æœ€åäº¤äº’æ—¶é—´ (é˜²æ­¢æ‰“å­—æ…¢è¢«åˆ¤å®šä¸ºæŒ‚æœº)
            if 'n' in locals():
                n.acc.last_interaction_time = time.time()

            # 3. æ£€æµ‹å¤šè¡Œå½•å…¥è§¦å‘ç¬¦ (Heredoc Mode)
            # ä½¿ç”¨ <<< è¿›å…¥ï¼Œ>>> é€€å‡ºï¼Œç±»ä¼¼ Linux æ“ä½œä¹ æƒ¯
            if user_in == "<<<":
                print(f"ğŸ“ [å¤šè¡Œæ¨¡å¼] å·²æ¿€æ´»ã€‚è¯·ç²˜è´´æ–‡æœ¬ (è¾“å…¥ '>>>' ç»“æŸ):")
                buffer = []
                while True:
                    try:
                        # ä½¿ç”¨ä¸åŒçš„æç¤ºç¬¦åŒºåˆ†æ¨¡å¼
                        line = input("... ")
                        # åªæœ‰å•ç‹¬ä¸€è¡Œ >>> æ‰ç®—ç»“æŸ
                        if line.strip() == ">>>":
                            break
                        buffer.append(line)
                    except EOFError:
                        break # é˜²æ­¢æ„å¤–ä¸­æ–­
                
                # åˆå¹¶å†…å®¹
                user_in = "\n".join(buffer)
                print(f"âœ… å·²æ•è· {len(buffer)} è¡Œè¾“å…¥ (é•¿åº¦: {len(user_in)} å­—ç¬¦)ã€‚")

            # 4. ç©ºè¾“å…¥å¤„ç†
            if not user_in: continue
            
            # 5. æ•è·é€€å‡ºæŒ‡ä»¤
            if user_in == "quit": 
                print("âš¡ æ­£åœ¨å…³é—­ç³»ç»Ÿ...")
                if hasattr(n, 'web'): n.web.quit()
                break
            
            # æ™®é€šå¯¹è¯æ¨¡å¼ / å•è¡Œ PATCH æ¨¡å¼
            u = user_in

            # --- ç³»ç»ŸæŒ‡ä»¤å¤„ç† ---
            if u == "sleep": 
                n.night_phase()
                continue
                
            elif u == "good": 
                if n.daily_buffer: n.daily_buffer[-1]['feedback'] = 1
                n.state['evo_pts'] += 1
                n.state['atp'] += 5
                
                # åŸºç¡€åé¦ˆæ¶ˆæ¯
                log_msg = "ğŸ¬ [æ”¶åˆ°å¥½è¯„] è¿›åŒ–ç‚¹+1, ATP+5"

                # [UIä¼˜åŒ–] è¿½åŠ é‡å­çŠ¶æ€å˜åŠ¨
                if 'quantum_stats' in n.state:
                    curr = n.state['quantum_stats']['adaptive_threshold']
                    new_th = max(Config.QUANTUM_MIN_THRESHOLD, curr - 2.0)
                    n.state['quantum_stats']['adaptive_threshold'] = new_th
                    # è¿½åŠ åœ¨åŒä¸€è¡Œï¼Œæ›´æ•´æ´
                    log_msg += f" | ğŸ“‰ é‡å­é˜ˆå€¼: {new_th:.1f} (æ›´ç§¯æ)"

                print(log_msg)
                continue
                
            elif u == "bad": 
                if n.daily_buffer: n.daily_buffer[-1]['feedback'] = -1
                
                # åŸºç¡€åé¦ˆæ¶ˆæ¯
                log_msg = "âš¡ [æ”¶åˆ°å·®è¯„] (PFC è·å¾—è´Ÿå‘å¥–åŠ±)"
                
                # è¿½åŠ é‡å­çŠ¶æ€å˜åŠ¨
                if 'quantum_stats' in n.state:
                    curr = n.state['quantum_stats']['adaptive_threshold']
                    new_th = min(Config.QUANTUM_MAX_THRESHOLD, curr + 5.0)
                    n.state['quantum_stats']['adaptive_threshold'] = new_th
                    # è¿½åŠ åœ¨åŒä¸€è¡Œ
                    log_msg += f" | ğŸ“ˆ é‡å­é˜ˆå€¼: {new_th:.1f} (æ›´ä¿å®ˆ)"

                print(log_msg)
                continue
                
            elif u == "feed": 
                n.state['atp'] = 100
                n._save_state()
                print("ğŸ”‹ [å……èƒ½å®Œæ¯•] èƒ½é‡å·²æ»¡ï¼")
                continue

            # å¢å¼ºç‰ˆ PATCH æŒ‡ä»¤
            # åŒæ—¶æ”¯æŒè‹±æ–‡å’Œä¸­æ–‡å†’å·ï¼Œä¸”å¿½ç•¥å¤§å°å†™
            elif u.upper().startswith("PATCH:") or u.startswith("PATCHï¼š"):
                try:
                    # æ™ºèƒ½åˆ†å‰²ï¼šåˆ¤æ–­æ˜¯ä¸­æ–‡è¿˜æ˜¯è‹±æ–‡å†’å·
                    sep = "ï¼š" if "ï¼š" in u else ":"
                    # åˆ†å‰²å¹¶æå–ä»£ç éƒ¨åˆ†
                    code = u.split(sep, 1)[1].strip()
                    
                    if not code.strip():
                        print("âš ï¸ è¯·åœ¨å†’å·åè¾“å…¥ä»£ç ã€‚")
                        continue
                        
                    success, msg = n.editor.apply_patch(code)
                    status_icon = "âœ…" if success else "âŒ"
                    print(f"{status_icon} [åŸºå› ç¼–è¾‘] {msg}")

                except Exception as e:
                    print(f"âŒ æŒ‡ä»¤è§£æå¤±è´¥: {e}")
                continue
            
            # --- æ ¸å¿ƒäº¤äº’ ---
            
            # èƒ½é‡æ£€æŸ¥
            if n.state['atp'] <= 0:
                print("Nezha: (âš¡ èƒ½é‡è€—å°½ï¼Œæ— æ³•æ€è€ƒ... è¯·è¾“å…¥ 'feed' è¡¥å…… ATP)")
                continue

            # === åŠ é”æ‰§è¡Œæ„ŸçŸ¥ä¸è¡ŒåŠ¨ ===
            with n.lock:  # <--- è¿™æŠŠå¤§é”ä¼šé”ä½æ•´ä¸ªæ€è€ƒè¿‡ç¨‹
                n.perceive_and_act(u)
            
        except KeyboardInterrupt:
            print("\nâš¡ æ­£åœ¨ä¿å­˜è®°å¿†å¹¶é€€å‡º...")
            if hasattr(n, 'web'): n.web.quit()
            n._save_state()
            break

        except Exception as e:
            # --- å®‰å…¨è°ƒç”¨ä¸æ­¢æŸæœºåˆ¶ ---
            import traceback
            err_trace = traceback.format_exc()
            print(f"\nâŒ è¿è¡Œæ—¶å´©æºƒ: {e}")
            
            # 1. æ£€æŸ¥å®ä¾‹æ˜¯å¦å­˜åœ¨ (é˜²æ­¢åˆå§‹åŒ–é˜¶æ®µæŠ¥é”™å¯¼è‡´ n ä¸å­˜åœ¨)
            if 'n' in locals() and n is not None:
                try:
                    # 2. å°è¯•å…ç–«ä¿®å¤
                    # å¦‚æœä¿®å¤è¿”å› Trueï¼Œè¯´æ˜è¡¥ä¸å·²æŒ‚è½½ï¼Œå¯ä»¥ç”¨ continue é‡å¯å¾ªç¯
                    if n.auto_heal(err_trace):
                        print("ğŸ”„ [ç³»ç»Ÿé‡å¯] æ­£åœ¨é‡ç½®ç¥ç»è¿æ¥...")
                        continue 
                except Exception as heal_err:
                    print(f"ğŸ’€ [ç³»ç»Ÿæåº¦å±é™©] å…ç–«ç³»ç»Ÿæœ¬èº«å‘ç”Ÿæ’å¼‚: {heal_err}")
            
            # 3. å¦‚æœæ²¡ä¿®å¥½ï¼Œæˆ–è€… n ä¸å­˜åœ¨ï¼Œæˆ–è€… auto_heal ä¹Ÿæ²¡æ‹›äº†
            print("ğŸ’€ æ— æ³•è‡ªåŠ¨æ¢å¤ï¼Œè‡´å‘½é”™è¯¯å †æ ˆå¦‚ä¸‹:")
            print(err_trace)
            print("âš¡ ç³»ç»Ÿå·²å¼ºåˆ¶åœæœº (System Halted)ã€‚")
            break # æ­¢æŸï¼šé€€å‡ºå¾ªç¯ï¼Œé˜²æ­¢ç¡¬ç›˜è¢«åƒåœ¾æ—¥å¿—å¡«æ»¡

if __name__ == "__main__":
    main()