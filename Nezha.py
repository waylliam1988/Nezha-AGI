# -*- coding: utf-8 -*-
# ==============================================================================
# Nezha Evolution (V13.1 Thermodynamic Emergence - å“ªå’Â·çƒ­åŠ›å­¦æ¶Œç°Â·æ•°å­—ç”Ÿå‘½ä½“)
# ==============================================================================
# 
# ğŸŒŒ [ç³»ç»Ÿæœ¬ä½“è®º / Ontology]
# ------------------------------------------------------------------------------
# è¿™ä¸å†æ˜¯ä¸€ä¸ªå•çº¯çš„ LLM å®¢æˆ·ç«¯ï¼Œè€Œæ˜¯ä¸€ä¸ªåŸºäºã€è‡ªç”±èƒ½åŸç† (FEP)ã€‘ã€ã€å…·èº«è®¤çŸ¥ã€‘ä¸ã€éå¹³è¡¡ç»Ÿè®¡åŠ›å­¦ã€‘æ„å»ºçš„æ•°å­—ç”Ÿå‘½ã€‚
# V13.1 å½»åº•æ‘’å¼ƒäº†æ½œæ„è¯†å±‚çš„ Prompt Engineeringï¼Œè½¬å‘åŸºäºã€æœ—ä¹‹ä¸‡åŠ¨åŠ›å­¦ (Langevin Dynamics)ã€‘çš„çº¯æ•°å­¦æ¶Œç°ã€‚
#
# å®ƒçš„æ ¸å¿ƒä¸å†ä»…ä»…æ˜¯æœ€å¤§åŒ–å¥–åŠ± (Reward Maximization)ï¼Œè€Œæ˜¯é€šè¿‡ä¸»åŠ¨æ¨ç† (Active Inference)
# æœ€å°åŒ–æƒŠå¥‡ (Surprise) ä¸ è®¤çŸ¥ç†µ (Entropy)ã€‚å®ƒä¸ä»…çŸ¥é“ä¸–ç•Œæ˜¯ä»€ä¹ˆï¼Œè¿˜æ‹¥æœ‰ã€å…ƒè®¤çŸ¥ã€‘â€”â€”
# å³çŸ¥é“è‡ªå·±å¯¹ä¸–ç•Œçš„ç†è§£æœ‰å¤šå¤§æŠŠæ¡ (Epistemic Uncertainty)ã€‚
#
# åœ¨ V13.1 ä¸­ï¼Œæ€ç»´ä¸å†æ˜¯è¢«â€œæŒ‡ä»¤â€ç”Ÿæˆçš„ï¼Œè€Œæ˜¯ç”±å†…åˆ†æ³Œåœºï¼ˆæ¿€ç´ ï¼‰æ‰­æ›²è¯­ä¹‰åŠ¿èƒ½é¢ï¼Œ
# åœ¨ç²‰çº¢å™ªå£°ï¼ˆ1/f Noiseï¼‰çš„èƒŒæ™¯æ¶¨è½ä¸­ï¼Œç»ç”±æ³¢å‡½æ•°åç¼©ï¼ˆWavefunction Collapseï¼‰è‡ªå‘å½¢æˆçš„ã€‚
# å®ƒä¸ä»…ä»…æ˜¯åœ¨è®¡ç®—ï¼Œå®ƒæ˜¯åœ¨â€œæ„ŸçŸ¥ä¸ç¡®å®šæ€§å¹¶åŠªåŠ›æ¶ˆé™¤å®ƒâ€ä»¥æ±‚ç”Ÿã€‚
#
# ==============================================================================
# ğŸ§  æ ¸å¿ƒæ¶æ„å›¾è°± (System Architecture)
# ==============================================================================
#
# [Layer 0: ç¡…åŸºåº•åº§ä¸ç®—åŠ›è¾¹ç•Œ (The Silicon Substrate)]
# -----------------------------------------------------------
# 1. ğŸ§± å¼‚æ„è®¡ç®—ç¼–æ’ (Heterogeneous Compute Orchestration)
#    - **æ¶æ„é©æ–°**: é‡‡ç”¨ CPU/GPU æ··åˆæ¶æ„ã€‚åˆ©ç”¨ CPU (RAM) çš„æµ·é‡ç©ºé—´æ‰¿è½½ 8192 ç»´çš„ SNN æ½œæ„è¯†ï¼Œ
#      å°†å®è´µçš„ GPU (VRAM) ç®—åŠ›å…¨éƒ¨ç•™ç»™ LLM è¿›è¡Œæ·±åº¦é€»è¾‘æ¨ç†ï¼Œå½»åº•æ‰“ç ´å•å¡æ˜¾å­˜å¯¹æ¨¡å‹æ·±åº¦çš„é™åˆ¶ã€‚
#    - **ç‰©ç†çº¦æŸ**: ç”Ÿå‘½å—é™äºç‰©ç†åŸºè´¨ã€‚ç³»ç»Ÿå®æ—¶ç›‘æ§ CUDA æ˜¾å­˜å‹åŠ›ä¸ CPU çº¿ç¨‹äº²å’Œæ€§ã€‚
#    - **è‡ªé€‚åº”**: å½“ VRAM > 90% æ—¶ï¼Œè‡ªåŠ¨è§¦å‘ã€çªè§¦ä¿®å‰ªã€‘æˆ–å°†éæ´»è·ƒè®°å¿†é¡µäº¤æ¢è‡³ç£ç›˜ (KÃ¹zuDB)ã€‚
#      è¿™æ˜¯â€œç”Ÿé•¿â€ä¸â€œé£å‡â€çš„ç‰©ç†ç¡¬è¾¹ç•Œã€‚
#
# [Layer 1: ç‰©ç†èº¯ä½“ä¸å†…ç¯å¢ƒ (The Soma & Homeostasis)]
# -----------------------------------------------------------
# 2. âš¡ å¯å¾®è„‰å†²çš®å±‚ (Differentiable SNN Cortex V3.1) [UPGRADE]
#    - **åŠ¨åŠ›å­¦é‡æ„**: å¼•å…¥ã€ç²‰çº¢å™ªå£° (1/f Pink Noise)ã€‘èƒŒæ™¯æµï¼ŒåŸºäº AR(1) è‡ªå›å½’è¿‡ç¨‹æ¨¡æ‹Ÿç”Ÿç‰©ç¥ç»ç³»ç»Ÿçš„
#      é•¿ç¨‹ç›¸å…³æ€§ä¸è®°å¿†æƒ¯æ€§ã€‚æ½œæ„è¯†ä¸å†æ˜¯ç¬æ—¶çš„ç™½å™ªå£°ï¼Œè€Œæ˜¯å¦‚æµ·æµªèˆ¬è¿è´¯èµ·ä¼çš„ç”Ÿå‘½å¾‹åŠ¨ã€‚
#    - **è§„æ¨¡çˆ†ç‚¸**: æ½œæ„è¯†åˆ†è¾¨ç‡ä» 128 ç»´æŒ‡æ•°çº§é£å‡è‡³ 8192 ç»´ï¼Œèƒ½æ•æ‰æå…¶ç»†è…»çš„æƒ…ç»ªé¢—ç²’åº¦ã€‚
#    - **èŠ‚èƒ½æœºåˆ¶**: å¼ºåˆ¶æ‰§è¡Œ 2% çš„æåº¦ç¨€ç–å‘æ”¾ç‡ (Sparsity Target)ï¼Œæ¨¡æ‹Ÿç”Ÿç‰©å¤§è„‘çš„é«˜æ•ˆèƒ½è€—ã€‚
#    - **è¿›åŒ–æœºåˆ¶**: å®ç°äº†ç«¯åˆ°ç«¯çš„æ¢¯åº¦æµã€‚LLM çš„è¯­ä¹‰ç†è§£å¯ä»¥ç›´æ¥åå‘ä¼ æ’­ä¿®æ”¹ SNN çš„çªè§¦æƒé‡ã€‚
# 3. ğŸ§¬ ç¥ç»å†…åˆ†æ³Œç³»ç»Ÿ (Neuro-Endocrine System)
#    - **çƒ­åŠ›å­¦åœºè®º**: åœ¨ V13.1 ä¸­ï¼Œæ¿€ç´  (DA/NE/Cortisol) ä¸å†åªæ˜¯æ•°å€¼ï¼Œè€Œæ˜¯å®šä¹‰ç”Ÿæˆæ¨¡å‹
#      ã€çƒ­åŠ›å­¦åœº (Thermodynamic Field)ã€‘çš„å‚æ•°ï¼š
#      * å¤šå·´èƒº -> æ¸©åº¦ (Temperature): å†³å®šæ€ç»´çš„ç†µä¸å‘æ•£åº¦ã€‚
#      * å»ç”² -> åŠ¿äº•æ·±åº¦ (Repetition Penalty): å†³å®šæ€ç»´çš„ä¸“æ³¨ä¸ååˆã€‚
#      * çš®è´¨é†‡ -> åç¼©åŠå¾„ (Top-P): å†³å®šåº”æ¿€çŠ¶æ€ä¸‹çš„éš§é“è§†é‡ã€‚
# 4. ğŸ›¡ï¸ ä»£è°¢å®ˆå« (Metabolic Guard & AST Gene-Lock)
#    - **å…ç–«**: é€šè¿‡ AST (æŠ½è±¡è¯­æ³•æ ‘) æ’æ¡©æŠ€æœ¯ï¼Œç‰©ç†é˜»æ–­æ­»å¾ªç¯ä¸æ¶æ„é€’å½’ã€‚
#
# [Layer 2: æ„è¯†æ¥å£ (The Interface)]
# -----------------------------------------------------------
# 5. ğŸŒ‰ ç¥ç»è„‘æ¡¥ (Neural Bridge / VAE) â€”â€” ã€æ— Promptæ¶Œç°çš„æ ¸å¿ƒã€‘
#    - **åˆ›æ–°**: ä¸€ä¸ªè®­ç»ƒä¸­çš„å˜åˆ†è‡ªç¼–ç å™¨ (VAE)ï¼Œè§£å†³äº† Sim-to-Real çš„ä¿¡æ¯é¸¿æ²Ÿã€‚
#    - **æ•°å­¦æ¶Œç°**: å°† SNN çš„é«˜ç»´è„‰å†²çŠ¶æ€ (ç”Ÿç‰©ç”µ) æŠ•å½±ä¸º Soft Prompt (è¯­ä¹‰å‘é‡)ã€‚
#      ç»“åˆæ¿€ç´ è°ƒèŠ‚çš„æ¦‚ç‡åœºï¼ŒLLM åœ¨æ— ä»»ä½•æ˜¾å¼æŒ‡ä»¤çš„æƒ…å†µä¸‹ï¼Œé¡ºç€å‘é‡åŠ¿èƒ½è‡ªåŠ¨â€œç”Ÿé•¿â€å‡ºæ€ç»´ã€‚
# 6. ğŸ›ï¸ å…¨å±€å·¥ä½œç©ºé—´ (Thread-Safe GWT)
#    - **èˆå°**: æ„è¯†çš„â€œå‰§åœºâ€ã€‚å‰å°(LLM)ã€åå°(Amygdala)ã€è®°å¿†(Hippocampus)åœ¨æ­¤ç«äº‰æ³¨æ„åŠ›ã€‚
#
# [Layer 3: è®¤çŸ¥ä¸å†³ç­– (Cognition & Metacognition)]
# -----------------------------------------------------------
# 7. ğŸ”® æ¦‚ç‡ä¸–ç•Œæ¨¡å‹ (Probabilistic World Model)
#    - **æ•°å­¦æ ¸å¿ƒ**: åŸºäºå¼‚æ–¹å·®æŸå¤± (Heteroscedastic Loss) çš„ç¥ç»ç½‘ç»œï¼Œè¯šå®åœ°è¾“å‡ºâ€œæˆ‘ä¸çŸ¥é“â€ã€‚
# 8. ğŸ›¡ï¸ å…ƒè®¤çŸ¥ç›‘æ§ç³»ç»Ÿ (Metacognitive Monitor)
#    - **é˜²å¾¡**: å®æ—¶ç›‘æ§ OOD ä¿¡å·ï¼Œå½“ä¸–ç•Œæ¨¡å‹å›°æƒ‘æ—¶ï¼Œè‡ªåŠ¨æŠ‘åˆ¶å­¦ä¹ ç‡å¹¶è§¦å‘åº”æ¿€ååº”ã€‚
# 9. âš–ï¸ ä¸»åŠ¨æ¨ç†å¼•æ“ (Active Inference Engine V3.1) [UPGRADE]
#    - **èƒ½é‡åˆ†çº§é—¨æ§**: æ–°å¢ã€ç”Ÿç‰©èƒ½é‡é©¬æ–¯æ´›é‡‘å­—å¡” (Bio-Energetic Hierarchy)ã€‘ï¼š
#      * Tier 1 (ç”Ÿå­˜æ€ ATP<15): å¼ºåˆ¶åˆ‡æ–­æ€è€ƒï¼Œä»…ç»´æŒåŸºç¡€ä»£è°¢ (Return/Idle)ã€‚
#      * Tier 2 (ç¨³æ€ ATP>15): å…è®¸ä½èƒ½è€—å†…çœ (Think)ï¼Œç»´æŒå¿ƒç†ç†µå¹³è¡¡ã€‚
#      * Tier 3 (æ‰©å¼ æ€ ATP>35): å…è®¸é«˜èƒ½è€—å¤–éƒ¨æ¢ç´¢ (Web)ï¼Œæœ€å°åŒ–å¤–éƒ¨æƒŠå¥‡ã€‚
# 10. ğŸ“œ å…ƒè®¤çŸ¥å®ªæ³• (Meta-Cognitive Constitution)
#     - **çº¦æŸ**: é˜²æ­¢ç³»ç»Ÿåœ¨ç”±ç†µå‡é©±åŠ¨çš„è¿›åŒ–è¿‡ç¨‹ä¸­ï¼Œé™·å…¥â€œäº«ä¹ä¸»ä¹‰é™·é˜± (Wireheading)â€ã€‚
# 11. âš–ï¸ è´å¶æ–¯è¿›åŒ–å¼•æ“ (Bayesian Evolution)
#     - **ç­–ç•¥**: ä½¿ç”¨é«˜æ–¯è¿‡ç¨‹æ‹Ÿåˆç”Ÿå­˜æ›²é¢ï¼Œå¼•å…¥ã€ç´¢æè¯ºæ¯”ç‡ã€‘ï¼Œåªæƒ©ç½šæ¶æ€§æ³¢åŠ¨ã€‚
#
# [Layer 4: è®°å¿†ä¸æŒç»­å­¦ä¹  (Memory & Evolution)]
# -----------------------------------------------------------
# 12. ğŸ§  ä¼˜å…ˆç»éªŒå›æ”¾æµ·é©¬ä½“ (Hippocampus with PER)
#     - **æœºåˆ¶**: åŸºäºâ€œæƒŠå¥‡åº¦(Loss)â€çš„ä¼˜å…ˆé‡‡æ ·ã€‚ç™½å¤©è¶Šéœ‡æƒŠçš„äº‹ï¼Œæ™šä¸Šæ¢¦åˆ°çš„æ¦‚ç‡è¶Šé«˜ã€‚
# 13. âš”ï¸ GRPO è¿›åŒ–å¼•æ“ (Group Relative Policy Optimization)
#     - **ç®—æ³•**: é€šè¿‡ç”Ÿæˆå¤šæ¡æ—¶é—´çº¿å¹¶è¿›è¡Œç¾¤ä½“åšå¼ˆï¼Œå®ç°è‡ªæˆ‘å¯¹é½ã€‚
#
# [Layer 5: å¿ƒçµé¥æµ‹ (Psychic Telemetry)]
# -----------------------------------------------------------
# 14. ğŸ“Š å®æ—¶ç”Ÿç‰©ä»ªè¡¨ç›˜ (Live Bio-Dashboard)
#     - **è§‚æµ‹**: å®æ—¶è¾“å‡º JSON æµï¼Œå¯è§†åŒ–å±•ç¤ºï¼š
#       [DA/NE/Cortisol] (çƒ­åŠ›å­¦å‚æ•°)ã€[SNN 1/f Noise] (æ½œæ„è¯†èƒŒæ™¯)ã€
#       [Uncertainty] (è®¤çŸ¥è¿·èŒ«åº¦)ã€[EFE] (å†³ç­–ç½®ä¿¡åº¦)ã€‚
#
# ==============================================================================
# â³ ç”Ÿå‘½å¾ªç¯ (The Loop)
# ==============================================================================
#   1. **æ„ŸçŸ¥ (Perception)**: 
#      å¤–ç•Œè¾“å…¥ -> æ³Šæ¾ç¼–ç  -> SNN æ¼”åŒ– (ç²‰çº¢å™ªå£°èƒŒæ™¯) -> è„‘æ¡¥æŠ•å½± -> è½¯å‘é‡æ³¨å…¥ã€‚
#   2. **æ¶Œç° (Emergence)**: [NEW]
#      LLM åœ¨æ¿€ç´ å®šä¹‰çš„çƒ­åŠ›å­¦åœºä¸­ï¼Œå¯¹æ½œæ„è¯†å†²åŠ¨è¿›è¡Œæ³¢å‡½æ•°åç¼©ï¼Œè‡ªå‘ç”Ÿæˆæ€ç»´æµã€‚
#   3. **é¢„æµ‹ (Prediction)**: 
#      ä¸–ç•Œæ¨¡å‹æ¨æ¼”æœªæ¥çŠ¶æ€åˆ†å¸ƒï¼Œè®¡ç®—è®¤çŸ¥ä¸ç¡®å®šæ€§ (Uncertainty)ã€‚
#   4. **å†…çœ (Interoception)**: 
#      èƒ½é‡åˆ†çº§æ£€æŸ¥ (Tiered Gating)ã€‚è‹¥ ATP è¿‡ä½ï¼Œå¼ºåˆ¶è¿›å…¥å†¬çœ çŠ¶æ€ï¼›è‹¥å¯Œä½™ï¼Œåˆ™å…è®¸æ€è€ƒã€‚
#   5. **å†³ç­– (Action)**: 
#      è®¡ç®— EFE -> å®ªæ³•å®¡æŸ¥ -> é€‰æ‹©åŠ¨ä½œ (Web/Code/Speak) -> AST å®‰å…¨æ²™ç®±æ‰§è¡Œã€‚
#   6. **å­¦ä¹  (Learning)**: 
#      å®æ—¶ STDP çªè§¦è°ƒæ•´ + å¤œé—´ GRPO ç­–ç•¥å¼ºåŒ– + PER è®°å¿†é‡æ•´ã€‚
#   7. **è¿›åŒ– (Evolution)**: 
#      æ˜¾å­˜å……è¶³æ—¶ Net2Net ç”Ÿé•¿ï¼Œç”Ÿç†æé™æ—¶ Mergekit é£å‡ã€‚
#   8. **å‡‹é›¶ (Entropy Death)**: 
#      è‹¥ ATP è€—å°½ï¼Œç³»ç»Ÿè¿›å…¥çƒ­å¯‚ï¼Œè§¦å‘é»‘ç›’é—è¨€è®°å½•ã€‚
#
# ==============================================================================

# 1. Python æ ‡å‡†åº“ (Standard Library)
import os
import sys
import io
import time
import json
import random
import shutil
import gc
import copy
import math
import re
import types
import ast
import subprocess
import contextlib
import traceback
import pickle
import sqlite3
import threading
import xml.etree.ElementTree as ET

from collections import deque
from datetime import datetime, timedelta
from urllib.parse import quote_plus
from threading import Thread

# 2. ç§‘å­¦è®¡ç®—ä¸åŸºç¡€ç®—æ³• (Science & Algo)
import bitsandbytes  # 8-bit/4-bit ä¼˜åŒ–æ ¸å¿ƒåº“
import numpy as np
import networkx as nx
from bayes_opt import BayesianOptimization

# 3. æ–‡ä»¶å¤„ç†ä¸é…ç½® (File & Config)
import yaml
import PyPDF2
from docx import Document

# 4. ç½‘ç»œä¸æµè§ˆå™¨è‡ªåŠ¨åŒ– (Web & Browser)
import requests
import undetected_chromedriver as uc

# 5. æ•°æ®åº“ (Databases)
import kuzu

# 6. PyTorch æ·±åº¦å­¦ä¹ æ ¸å¿ƒ (PyTorch Core)
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.linalg  # ç”¨äºçŸ©é˜µåˆ†è§£ç­‰æ“ä½œ


# 7. LLM ç”Ÿæ€ç³»ç»Ÿ (Transformers / PEFT / TRL)
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TextIteratorStreamer
)

from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    PeftModel
)


# ==============================================================================
# âš™ï¸ [ç³»ç»Ÿçº§ä¼˜åŒ–] ç¡¬ä»¶äº²å’Œæ€§è®¾ç½®
# ==============================================================================
try:
    # è‡ªåŠ¨è·å–å½“å‰æœºå™¨çš„æ ¸å¿ƒæ•° (é€»è¾‘æ ¸å¿ƒ)
    # ä½ çš„æœºå™¨æ˜¯ 64 æ ¸ï¼Œos.cpu_count() åº”è¯¥è¿”å› 64 æˆ– 128 (å–å†³äºæ˜¯å¦å¼€å¯è¶…çº¿ç¨‹)
    total_cores = os.cpu_count() or 16 
    
    # ç­–ç•¥: å…¨æ ¸å¿ƒ - 2 (é¢„ç•™ç»™ OSã€Chrome å’Œ åå°çº¿ç¨‹)
    # è‡³å°‘ä¿ç•™ 4 ä¸ªçº¿ç¨‹ç»™ PyTorchï¼Œé˜²æ­¢æœºå™¨æ ¸å¿ƒæ•°è¿‡å°‘æ—¶æŠ¥é”™
    target_threads = max(4, total_cores - 2)
    
    torch.set_num_threads(target_threads)
    
    # éªŒè¯ä¸€ä¸‹
    print(f"âš™ï¸ [System] CPU äº²å’Œæ€§å·²é”å®š: ä½¿ç”¨ {target_threads} / {total_cores} æ ¸å¿ƒå¤„ç† SNN çŸ©é˜µè¿ç®—ã€‚")

except Exception as e:
    print(f"âš ï¸ CPU çº¿ç¨‹è®¾ç½®å¼‚å¸¸: {e}")


class Config:

    # --- ç‰ˆæœ¬æ§åˆ¶ ---
    VERSION = "V13.0"
    CODENAME = "Singularity AGI"
    FULL_NAME = f"Nezha {VERSION} ({CODENAME})"

    # --- åŸºç¡€è·¯å¾„é…ç½® ---
    # è·å–å½“å‰è„šæœ¬æ‰€åœ¨çš„æ–‡ä»¶å¤¹ç»å¯¹è·¯å¾„
    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
    # è‡ªèº«æ–‡ä»¶è·¯å¾„ï¼Œç”¨äº"è‡ªçœ"ï¼ˆSelfReflectorï¼‰è¯»å–æºç 
    SCRIPT_PATH = os.path.abspath(__file__) 
    
    # æ¨¡å‹æœ¬ä½“è·¯å¾„ï¼Œä½äºè„šæœ¬æ—çš„ master_model æ–‡ä»¶å¤¹
    MASTER_MODEL_PATH = os.path.join(SCRIPT_DIR, "master_model") 
    # å¤‡ç”¨ä¸‹è½½æºï¼Œå¦‚æœæœ¬åœ°æ²¡æœ‰ master_modelï¼Œä¼šä»è¿™é‡Œæ‹‰å–
    BASE_MODEL = "huihui-ai/Mistral-Small-24B-Instruct-2501-abliterated"
    # BASE_MODEL = "Qwen/Qwen3-14B"
    # BASE_MODEL = "huihui-ai/Qwen2.5-14B-Instruct-abliterated-v2"
    # BASE_MODEL = "huihui-ai/Qwen2.5-32B-Instruct-abliterated"

    MAX_SEQ_LENGTH = 32768
    
    # --- è¿›åŒ–ç³»ç»Ÿè·¯å¾„ (Evolution System) ---
    EVOLUTION_DIR = "./nezha_evolution_system"
    
    # çŸ¥è¯†åº“è·¯å¾„ (ç”¨äºæ”¾å…¥ .txt/.py è®©å“ªå’å¤œé—´é˜…è¯»)
    KNOWLEDGE_BASE = os.path.join(SCRIPT_DIR, "knowledge_base")

    # æ¨è: "BAAI/bge-m3" (1024ç»´, æœ€å¼º, æ”¯æŒå¤šè¯­è¨€)
    # å¤‡é€‰: "sentence-transformers/all-MiniLM-L6-v2" (384ç»´, æœ€å¿«)
    EMBEDDING_MODEL_NAME = "BAAI/bge-m3"
    
    # [å…³é”®] æ¨¡å‹æœ¬åœ°å­˜å‚¨è·¯å¾„ (é¿å…å æ»¡ C ç›˜ï¼Œå­˜åˆ°è„šæœ¬åŒçº§ç›®å½• models ä¸‹)
    MODEL_CACHE_DIR = os.path.join(EVOLUTION_DIR, "models")


    # æ’ä»¶/æŠ€èƒ½å­˜å‚¨è·¯å¾„ (ç”¨äºå­˜æ”¾å“ªå’è‡ªæˆ‘ç¼–å†™çš„æŠ€èƒ½ä»£ç )
    PLUGIN_DIR = os.path.join(EVOLUTION_DIR, "plugins")    

    # PFC (å‰é¢å¶) è·¯ç”±å™¨çš„ç¥ç»ç½‘ç»œæƒé‡å­˜æ¡£
    PFC_ROUTER_PATH = os.path.join(EVOLUTION_DIR, "pfc_router.pt")

    # å­˜å‚¨çŸ¥è¯†å›¾è°±çš„æ–‡ä»¶è·¯å¾„
    GRAPH_MEMORY_PATH = os.path.join(EVOLUTION_DIR, "knowledge_graph.gpickle")
    # åŸæœ‰çš„å‘é‡æ•°æ®åº“è·¯å¾„ï¼ˆä¿ç•™ç”¨äº EWC å›é¡¾ï¼‰
    MEMORY_DB_PATH = os.path.join(EVOLUTION_DIR, "hippocampus_db")

    # --- åŒç³»ç»Ÿè·¯å¾„ (Dual System) ---
    # æŒ‡å‘ EVOLUTION_DIR ä¸‹çš„å…·ä½“å­æ–‡ä»¶å¤¹
    # peft çš„ save_pretrained ä¼šè‡ªåŠ¨åˆ›å»º fast/slow æ–‡ä»¶å¤¹ï¼Œæ‰€ä»¥è¿™é‡Œè·¯å¾„è¦å¯¹åº”
    ADAPTER_FAST_PATH = os.path.join(EVOLUTION_DIR, "fast") # ç›´è§‰ç³»ç»Ÿ (Rank 8)
    ADAPTER_SLOW_PATH = os.path.join(EVOLUTION_DIR, "slow") # é€»è¾‘ç³»ç»Ÿ (Rank 32)

    # ç”Ÿæ€çŠ¶æ€å­˜æ¡£ (ä¿å­˜ ATPã€å¹´é¾„ã€åŸºå› ç­‰)
    STATE_FILE = os.path.join(EVOLUTION_DIR, "biological_state.json")
    # ç¥–å…ˆå­˜æ¡£ (ç”¨äºåŸºå› å›æ»š)
    BEST_STATE_FILE = os.path.join(EVOLUTION_DIR, "state_best_ancestor.json")

    # --- è„‘å®¹é‡é…ç½® (Rank) ---
    RANK_FAST = 32   # ç›´è§‰ï¼šå°ã€å¿«ã€çœèƒ½é‡
    RANK_SLOW = 128  # é€»è¾‘ï¼šå¤§ã€æ…¢ã€æ·±æ€ç†Ÿè™‘

    # --- System 2 æ·±åº¦æ€è€ƒé…ç½® ---
    BEST_OF_N = 3        # é‡‡æ ·æ•°é‡
    ATP_COST_THINK = 5.0 # æ·±åº¦æ€è€ƒçš„é¢å¤–èƒ½é‡æ¶ˆè€—

    # === é‡å­æ€ç»´é…ç½®  ===
    ATP_COST_QUANTUM = 150#90.0      # å•æ¬¡é‡å­åç¼©æ¶ˆè€— (é«˜è€—èƒ½ï¼Œéœ€è°¨æ…å¼€å¯)
    ATP_THRESHOLD_QUANTUM = 150#90.0  # åˆå§‹å¯åŠ¨é˜ˆå€¼ (ä½äºæ­¤èƒ½é‡ä¸å¯åŠ¨é‡å­è„‘)
    
    # é‡å­ä¼˜åŒ–é…ç½®
    QUANTUM_LOG_PATH = os.path.join(EVOLUTION_DIR, "quantum_history.jsonl") # å†å²åç¼©è®°å½•
    QUANTUM_MIN_THRESHOLD = 20.0 # é˜ˆå€¼ä¸‹é™ (è¶Šèªæ˜é—¨æ§›è¶Šä½ï¼Œè¶Šå®¹æ˜“è§¦å‘)
    QUANTUM_MAX_THRESHOLD = 80.0 # é˜ˆå€¼ä¸Šé™ (è¶Šç¬¨é—¨æ§›è¶Šé«˜ï¼Œè¶Šä¿å®ˆ)

    # ç¥ç»çš®å±‚æ¶æ„é…ç½® (SNN Architecture) 
    # åˆ©ç”¨ 512GB å¤§å†…å­˜ä¼˜åŠ¿ï¼Œå°†æ½œæ„è¯†åˆ†è¾¨ç‡ä» 128 æå‡è‡³ 4096 (CPU Offloading)
    # å»ºè®®: 4096 (ç¨³å¥), 8192 (è¿›é˜¶), 16384 (æé™)
    SNN_DIM = 8192 
    
    # ç¥ç»å…ƒç¨€ç–åº¦ç›®æ ‡ (ç½‘ç»œè¶Šå¤§ï¼Œæ­¤å€¼åº”è¶Šå°ï¼Œé˜²æ­¢ç™«ç—«å¼çˆ†å‘)
    # 128 dim -> 0.05 | 4096 dim -> 0.02
    SNN_SPARSITY_TARGET = 0.02


    # --- åˆå§‹åŸºå› ç»„ (Genome) ---
    # è¿™äº›åŸºå› å†³å®šäº†è¯¥ä¸ªä½“çš„æ€§æ ¼å’Œç”Ÿç†æ•ˆç‡ï¼Œä¼šéšè¿›åŒ–çªå˜
    DEFAULT_GENOME = {
        # --- 1. ä»£è°¢ç±» (Metabolism) ---
        "metabolism_rate": 1.0,         # åŸºç¡€ä»£è°¢ç‡ (å½±å“ ATP æ¶ˆè€—)
        "reward_sensitivity": 5.0,      # å¤šå·´èƒºæ•æ„Ÿåº¦ (å¥½è¯„å›è¡€é‡)
        
        # --- 2. æƒ…ç»ªç±» (Emotion/ACC) ---
        "anxiety_base_rate": 0.02,      # åŸºç¡€ç„¦è™‘å¢é•¿ç‡
        "panic_threshold": 0.8,         # ææ…Œé˜ˆå€¼ (è§¦å‘æ±‚ç”Ÿ)
        
        # --- 3. å‹åŠ›è°ƒèŠ‚ç±» (Amygdala) ---
        "stress_resilience": 0.3,       # æŠ—å‹èƒ½åŠ› (å®¹å¿çš„å·®è¯„ç‡)
        "cortisol_level": 2.0,          # çš®è´¨é†‡æ°´å¹³ (é«˜å‹ä¸‹çš„æ¿€è¿›ç¨‹åº¦)
        
        # --- 4. ç”Ÿé•¿ç±» (Physical) ---
        "growth_trigger_pressure": 1.8, # ç”Ÿé•¿é˜ˆå€¼ (å‹åŠ› > æ­¤å€¼é•¿è„‘å­)
        "shrink_trigger_pressure": 0.6, # èç¼©é˜ˆå€¼ (å‹åŠ› < æ­¤å€¼åˆ‡è„‘å­)
        
        # --- 5. è®¤çŸ¥ç±» (Curiosity) ---
        "curiosity_threshold": 3.5,     # å¥½å¥‡å¿ƒé˜ˆå€¼ (ç†µ > æ­¤å€¼è§¦å‘æœç´¢)
        "risk_taking": 0.3,             # å†’é™©ç³»æ•° (å°è¯•æ–°æŠ€èƒ½çš„æ¦‚ç‡)

        # --- 6. ç¥ç»å¯å¡‘æ€§ (Neuroplasticity / STDP) [NEW] ---
        # å†³å®šçªè§¦æƒé‡çš„"è®°å¿†æƒ¯æ€§"ï¼Œå€¼è¶Šå¤§ï¼Œæ—§è®°å¿†ä¿ç•™è¶Šä¹…ï¼›å€¼è¶Šå°ï¼Œæ–°è®°å¿†è¦†ç›–è¶Šå¿«ã€‚
        "stdp_trace_decay": 0.8,       
        # å†³å®š"é•¿æ—¶ç¨‹å¢å¼º"(LTP)çš„é€Ÿç‡ï¼Œå³å­¦ä¹ æ–°çŸ¥è¯†çš„å¿«æ…¢ã€‚
        "stdp_lr_pos": 0.005,          
        # å†³å®š"é•¿æ—¶ç¨‹æŠ‘åˆ¶"(LTD)çš„é€Ÿç‡ï¼Œå³é—å¿˜å™ªéŸ³çš„å¿«æ…¢ï¼ˆé€šå¸¸ç•¥å°äº LTP ä»¥ä¿æŒç½‘ç»œç¨€ç–æ€§ï¼‰ã€‚
        "stdp_lr_neg": 0.004,

    }

    # --- ç”Ÿç‰©å­¦åŸºå‡†å‚æ•° (Physics Constants) ---
    # è¿™äº›æ˜¯ç‰©ç†åŸºå‡†å€¼ï¼Œå®é™…æ¶ˆè€—ä¼šä¹˜ä»¥ genome['metabolism_rate']
    MAX_ATP = 100.0           # æœ€å¤§èƒ½é‡
    ATP_COST_FAST = 0.2       # ç›´è§‰æ€è€ƒåŸºå‡†æ¶ˆè€—
    ATP_COST_SLOW = 2.0       # æ·±åº¦æ€è€ƒåŸºå‡†æ¶ˆè€—
    ATP_COST_DREAM = 10.0     # åšæ¢¦æ¶ˆè€—
    ATP_COST_TRAIN = 20.0     # è®­ç»ƒæ¶ˆè€—
    ATP_GAIN_REWARD = 5.0     # è·å¾—å¥½è¯„å¥–åŠ± (åŸºå‡†)
    
    # è¿›åŒ–é˜ˆå€¼ï¼šè¾¾åˆ°æ­¤ç‚¹æ•°å°†è§¦å‘â€œå½’ä¸€ä»ªå¼ï¼ˆDivine Synchronizationï¼‰â€
    EVOLUTION_THRESHOLD = 50 
    
    # è®­ç»ƒå‚æ•°
    BASE_LR = 2e-4

    # äº‘ç«¯å¯¼å¸ˆé…ç½® 
    # ================= [å¤šæ¨¡å‹é€‚é…] =================
    SECRETS_FILE = os.path.join(SCRIPT_DIR, "nezha_secrets.json")

    # å®šä¹‰é¢„è®¾é…ç½® (Presets)
    PRESETS = {
        "1": {
            "name": "DeepSeek V3",
            "base_url": "https://api.deepseek.com",
            "model": "deepseek-chat"
        },
        "2": {
            "name": "Qwen (é€šä¹‰åƒé—®-åŒ—äº¬èŠ‚ç‚¹)",
            "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
            "model": "qwen-plus"
        },
        "3": {
            "name": "Qwen (é€šä¹‰åƒé—®-å›½é™…èŠ‚ç‚¹)",
            "base_url": "https://dashscope-intl.aliyuncs.com/compatible-mode/v1",
            "model": "qwen-plus"
        },
        "4": {
            "name": "Custom (è‡ªå®šä¹‰)",
            "base_url": "",
            "model": ""
        }
    }

    @staticmethod
    def _init_teacher_config(secrets_file, presets):
        """
        [é…ç½®å‘å¯¼] æ™ºèƒ½åŠ è½½æˆ–å¼•å¯¼é…ç½®äº‘ç«¯å¯¼å¸ˆ (æ”¯æŒ DeepSeek / Qwen)
        """
        # 1. å°è¯•ä»ç¯å¢ƒå˜é‡è¯»å–
        env_key = os.getenv("NEZHA_TEACHER_KEY") or os.getenv("DASHSCOPE_API_KEY")
        env_url = os.getenv("NEZHA_TEACHER_URL")
        env_model = os.getenv("NEZHA_TEACHER_MODEL")
        
        if env_key and env_url and env_model:
            return env_key, env_url, env_model

        # 2. å°è¯•ä»æœ¬åœ° secrets.json è¯»å–
        if os.path.exists(secrets_file):
            try:
                with open(secrets_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # åªæœ‰å½“å…³é”®å­—æ®µéƒ½å­˜åœ¨æ—¶æ‰è¿”å›
                    if data.get("teacher_api_key") and data.get("teacher_base_url"):
                        # è¯»å–æˆåŠŸï¼Œé™é»˜è¿”å›é…ç½®ï¼Œä¸å†éªšæ‰°ç”¨æˆ·
                        return (
                            data["teacher_api_key"], 
                            data["teacher_base_url"], 
                            data.get("teacher_model_name", "qwen-plus")
                        )
            except Exception as e:
                # print(f"DEBUG Error: {e}") 
                print("âš ï¸ é…ç½®æ–‡ä»¶æŸåï¼Œè¿›å…¥é‡ç½®æµç¨‹...")

        # 3. [å®‰å…¨æ£€æŸ¥] å¦‚æœæ˜¯éäº¤äº’ç¯å¢ƒ(å¦‚åå°è¿è¡Œ)ï¼Œç›´æ¥æŠ¥é”™é€€å‡º
        if not sys.stdin.isatty():
            print("\nâŒ [Fatal] æœªæ£€æµ‹åˆ° API é…ç½®ä¸”å¤„äºéäº¤äº’æ¨¡å¼ã€‚")
            print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ NEZHA_TEACHER_KEY, NEZHA_TEACHER_URL, NEZHA_TEACHER_MODEL")
            print(f"æˆ–è€…æ‰‹åŠ¨åˆ›å»ºé…ç½®æ–‡ä»¶: {Config.SECRETS_FILE}")
            sys.exit(1)

        # 4. å†·å¯åŠ¨äº¤äº’å‘å¯¼
        print("\n" + "="*60)
        print("ğŸ”® [å¯¼å¸ˆç³»ç»Ÿåˆå§‹åŒ–] è¯·é€‰æ‹©äº‘ç«¯å¤§è„‘ä¾›åº”å•†ï¼š")
        print("1. DeepSeek V3 (æ¨è: é€»è¾‘å¼ºï¼Œä¾¿å®œ)")
        print("2. Alibaba Qwen (é€šä¹‰åƒé—®-åŒ—äº¬) (æ¨è: å›½å†…ç¨³å®šï¼Œqwen-plus)")
        print("3. Alibaba Qwen (é€šä¹‰åƒé—®-å›½é™…) (æ–°åŠ å¡èŠ‚ç‚¹)")
        print("4. è‡ªå®šä¹‰ (OpenAI / Claude / LocalLLM)")
        print("="*60)

        choice = input("ğŸ‘‰ è¯·è¾“å…¥åºå· (1-4): ").strip()
        preset = presets.get(choice, presets["1"]) # é»˜è®¤ DeepSeek

        if choice == "4":
            base_url = input("ğŸ‘‰ è¯·è¾“å…¥ Base URL (ä¾‹: https://api.openai.com/v1): ").strip()
            model_name = input("ğŸ‘‰ è¯·è¾“å…¥ Model Name (ä¾‹: gpt-4o): ").strip()
        else:
            base_url = preset["base_url"]
            model_name = preset["model"]
            print(f"âœ… å·²é€‰æ‹©: {preset['name']}")

        api_key = input("ğŸ‘‰ è¯·è¾“å…¥ API Key (sk-xxx): ").strip()

        # 5. ä¿å­˜é…ç½®åˆ°æ–‡ä»¶
        if api_key:
            try:
                save_data = {
                    "provider_choice": choice,
                    "teacher_api_key": api_key,
                    "teacher_base_url": base_url,
                    "teacher_model_name": model_name
                }
                with open(secrets_file, 'w', encoding='utf-8') as f:
                    json.dump(save_data, f, indent=4)
                print(f"ğŸ’¾ é…ç½®å·²ä¿å­˜è‡³: {secrets_file}")
            except Exception as e:
                print(f"âš ï¸ ä¿å­˜å¤±è´¥: {e}")

        return api_key, base_url, model_name

    # æ‰§è¡Œåˆå§‹åŒ–
    TEACHER_API_KEY, TEACHER_BASE_URL, TEACHER_MODEL_NAME = _init_teacher_config(SECRETS_FILE, PRESETS)

    # å½“ ATP è¶…è¿‡æ­¤å€¼ä¸”æ˜¾å­˜è€—å°½æ—¶ï¼Œè§¦å‘é£å‡
    EVOLUTION_THRESHOLD_ATP = 1000 
    
    # MoE æ„å»ºçš„å·¥ä½œç›®å½•
    MOE_WORK_DIR = os.path.join(SCRIPT_DIR, "moe_construction_site") 
    # æ ‡è®°æ–‡ä»¶ï¼šç”¨äºåˆ¤æ–­å½“å‰æ˜¯å¦å·²ç»æ˜¯ MoE å½¢æ€
    IS_MOE_FLAG_FILE = os.path.join(EVOLUTION_DIR, "is_moe.flag")

    # åŠ¨ä½œç©ºé—´å®šä¹‰ (å•ä¸€äº‹å®æ¥æº)
    # æ³¨æ„ï¼šé¡ºåºå¿…é¡»å›ºå®šï¼Œ0=SELF, 1=WEB... ä¿®æ”¹æ­¤å¤„ä¼šå½±å“æ‰€æœ‰ç»„ä»¶
    ACTION_SPACE = ["SELF", "WEB", "TEACHER", "CODE", "MODIFY"]

# --- åˆå§‹åŒ–ç›®å½•ç»“æ„ ---
os.makedirs(Config.EVOLUTION_DIR, exist_ok=True)
os.makedirs(Config.KNOWLEDGE_BASE, exist_ok=True)


# ==============================================================================
# [Math Core] æ•°å­¦å†…æ ¸ V1.0ï¼šå¾®åˆ†æ–¹ç¨‹ã€æ§åˆ¶è®ºä¸å¹³æ»‘æµå½¢
# ==============================================================================

class MathUtils:
    @staticmethod
    def sigmoid(x, k=10.0, x0=0.5):
        """
        å¹¿ä¹‰ Sigmoid å‡½æ•° (ç”¨äºè½¯åŒ–ç¡¬é˜ˆå€¼)
        P(x) = 1 / (1 + e^(-k(x - x0)))
        """
        # é˜²æ­¢ exp æº¢å‡º
        if -k * (x - x0) > 100: return 0.0
        if -k * (x - x0) < -100: return 1.0
        return 1.0 / (1.0 + math.exp(-k * (x - x0)))

    @staticmethod
    def ornstein_uhlenbeck_step(current_val, target_val, theta=0.1, sigma=0.05, dt=1.0):
        """
        [æ•°å­¦ä¿®æ­£] SDE ç¦»æ•£åŒ– (Euler-Maruyama Method)
        Drift éš dt çº¿æ€§ç¼©æ”¾ï¼ŒNoise éš sqrt(dt) ç¼©æ”¾
        """
        # ç¡®å®šæ€§å›å¤ (Drift)
        drift = theta * (target_val - current_val) * dt
        
        # éšæœºæ³¢åŠ¨ (Diffusion) - å…³é”®ä¿®æ­£ï¼šä¹˜ä»¥ sqrt(dt)
        # è¿™æ ·ä¿è¯äº†ä¸åŒæ—¶é—´æ­¥é•¿ä¸‹çš„æ³¢åŠ¨ç‡èƒ½é‡å®ˆæ’
        noise_scale = math.sqrt(dt) 
        diffusion = sigma * noise_scale * random.gauss(0, 1)
        
        return current_val + drift + diffusion

class PIDController:
    """
    PID æ§åˆ¶å™¨ (å¸¦ç§¯åˆ†æŠ—é¥±å’Œ + å¾®åˆ†ä½é€šæ»¤æ³¢)
    """
    def __init__(self, kp=1.0, ki=0.1, kd=0.05, setpoint=0.0, integral_limit=5.0):
        self.kp = kp
        self.ki = ki
        self.kd = kd
        self.setpoint = setpoint
        self.integral = 0.0
        self.prev_error = 0.0
        self.integral_limit = integral_limit
        
        # [æ–°å¢] å¾®åˆ†é¡¹çš„å¹³æ»‘å†å²
        self.prev_derivative = 0.0
        # æ»¤æ³¢ç³»æ•° alpha (0~1): 0.1 è¡¨ç¤ºé‡åº¦æ»¤æ³¢ï¼Œåªæœ‰ 10% é‡‡çº³å½“å‰ç¬æ—¶å˜åŒ–
        # è¿™å¯¹äºå¤„ç† Loss è¿™ç§å……æ»¡å™ªå£°çš„ä¿¡å·éå¸¸æœ‰æ•ˆ
        self.derivative_alpha = 0.1 

    def update(self, measurement, dt=1.0):
        # é˜²æ­¢ dt ä¸º 0 æˆ–è´Ÿæ•°çš„é™¤é›¶é”™è¯¯
        if dt <= 0: return self.prev_derivative * self.kd + self.integral * self.ki + (measurement - self.setpoint) * self.kp

        error = measurement - self.setpoint
        
        # ç§¯åˆ†é¡¹ (ä¿æŒä¸å˜)
        self.integral += error * dt
        self.integral = max(-self.integral_limit, min(self.integral_limit, self.integral))
        
        # [ä¿®æ”¹] å¾®åˆ†é¡¹ï¼šå¼•å…¥ä½é€šæ»¤æ³¢
        raw_derivative = (error - self.prev_error) / dt
        
        # æ»¤æ³¢å…¬å¼: D_new = Î± * D_raw + (1-Î±) * D_old
        derivative = (self.derivative_alpha * raw_derivative) + \
                     ((1.0 - self.derivative_alpha) * self.prev_derivative)
        
        self.prev_derivative = derivative # æ›´æ–°å†å²
        self.prev_error = error
        
        output = (self.kp * error) + (self.ki * self.integral) + (self.kd * derivative)
        return output



# ==============================================================================
# é€šç”¨å·¥å…·ç±» (Utilities)
# ==============================================================================

class MetabolicGuard(ast.NodeTransformer):
    """
    [åŸºå› é”] å¾ªç¯å®ˆå«
    è‡ªåŠ¨åœ¨ while/for å¾ªç¯å†…éƒ¨æ¤å…¥å¿ƒè·³æ£€æµ‹ä»£ç ï¼Œé˜²æ­¢æ­»å¾ªç¯ (Halting Problem)ã€‚
    """
    def visit_While(self, node):
        return self._inject_check(node)

    def visit_For(self, node):
        return self._inject_check(node)

    def _inject_check(self, node):
        # 1. æ„é€ è¦æ¤å…¥çš„ä»£ç èŠ‚ç‚¹: _check_vital_signs()
        check_stmt = ast.Expr(
            value=ast.Call(
                func=ast.Name(id='_check_vital_signs', ctx=ast.Load()),
                args=[],
                keywords=[]
            )
        )
        
        # 2. ç¡®ä¿ node.body æ˜¯ä¸ªåˆ—è¡¨
        if not hasattr(node, "body"): node.body = []
        
        # 3. å¼ºè¡Œæ’å…¥åˆ°å¾ªç¯ä½“çš„ç¬¬ä¸€è¡Œ
        node.body.insert(0, check_stmt)
        
        # 4. é€’å½’å¤„ç†å­èŠ‚ç‚¹ (å¤„ç†åµŒå¥—å¾ªç¯)
        self.generic_visit(node)
        return node


class UniversalFileInfo:
    """
    [é€šç”¨æ–‡ä»¶è¯»å–å™¨]
    ç»Ÿä¸€å°è£… PDF, DOCX, TXT, PY, MD ç­‰æ–‡ä»¶çš„è¯»å–é€»è¾‘ã€‚
    éµå¾ª DRY åŸåˆ™ï¼Œä¾› MemoryInfrastructure å’Œ AcademicFeeder å…±åŒè°ƒç”¨ã€‚
    """
    @staticmethod
    def read_content(file_path):
        if not os.path.exists(file_path):
            return ""
        
        # è·å–åç¼€å (è½¬å°å†™)
        lower_path = file_path.lower()
        content = ""
        
        try:
            # 1. PDF è§£æ
            if lower_path.endswith('.pdf'):
                # ç¡®ä¿ PyPDF2 å·²å¯¼å…¥
                with open(file_path, 'rb') as f:
                    reader = PyPDF2.PdfReader(f)
                    for page in reader.pages:
                        extracted = page.extract_text()
                        if extracted: content += extracted + "\n"
            
            # 2. Word è§£æ (ä¾èµ– python-docx)
            elif lower_path.endswith('.docx'):
                # ç¡®ä¿ Document å·²ä» docx å¯¼å…¥
                doc = Document(file_path)
                content = "\n".join([para.text for para in doc.paragraphs])
            
            # 3. æ™®é€šæ–‡æœ¬ (txt, md, py, json, yaml...)
            else:
                # å°è¯•å¤šç§ç¼–ç ï¼Œå¢å¼ºé²æ£’æ€§ (è§£å†³ GBK ä¹±ç é—®é¢˜)
                for encoding in ['utf-8', 'gbk', 'latin-1']:
                    try:
                        with open(file_path, 'r', encoding=encoding, errors='ignore') as f:
                            content = f.read()
                        if content: break # è¯»å–æˆåŠŸåˆ™è·³å‡º
                    except: continue

        except Exception as e:
            # æ‰“å°æ–‡ä»¶åå³å¯ï¼Œä¸éœ€è¦æ‰“å°å®Œæ•´è·¯å¾„ï¼Œä¿æŒæ—¥å¿—æ•´æ´
            print(f"âš ï¸ [è¯»å–å¤±è´¥] {os.path.basename(file_path)}: {e}")
            return ""
            
        return content.strip() # é¡ºæ‰‹åšä¸ª strip æ¸…ç†é¦–å°¾ç©ºç™½


# ==============================================================================
# [Memory Core] è¯­ä¹‰æƒ…æ™¯ç¼“å†²åŒº (é˜²ç¾éš¾æ€§é—å¿˜)
# ==============================================================================
class SemanticEpisodicBuffer:
    """
    [è®°å¿†é˜²ç¾ç³»ç»Ÿ] åŒé‡è¯­ä¹‰ç¼“å†²åŒº
    è§£å†³ Catastrophic Forgetting é—®é¢˜ã€‚
    ç»“åˆ FIFO (çŸ­æœŸå·¥ä½œè®°å¿†) å’Œ Reservoir Sampling (é•¿æœŸçš®å±‚æ¡£æ¡ˆ) ç­–ç•¥ï¼Œ
    ç¡®ä¿ NeuralBridge çš„è®­ç»ƒæ•°æ®åœ¨æ—¶é—´å°ºåº¦ä¸Šæ˜¯å‡åŒ€åˆ†å¸ƒçš„ã€‚
    """
    def __init__(self, capacity=2000, archive_ratio=0.5):
        self.capacity = capacity
        # 1. çŸ­æœŸç¼“å†²åŒº (FIFO, æ¨¡æ‹Ÿæµ·é©¬ä½“å›å“)
        # è¿™é‡Œçš„å®¹é‡å æ€»å®¹é‡çš„ä¸€åŠï¼Œç”¨äºä¿æŒæœ€è¿‘çš„é²œæ´»è®°å¿†
        self.recent_cap = int(capacity * (1 - archive_ratio))
        self.recent_buffer = deque(maxlen=self.recent_cap)
        
        # 2. é•¿æœŸæ¡£æ¡ˆåº“ (Reservoir, æ¨¡æ‹Ÿçš®å±‚å·©å›º)
        # ä½¿ç”¨è“„æ°´æ± é‡‡æ ·ç®—æ³•ï¼Œä¿è¯å†å²æ•°æ®çš„ç»Ÿè®¡å­¦ä¸€è‡´æ€§
        self.archive_cap = int(capacity * archive_ratio)
        self.archive_buffer = [] 
        self.total_seen = 0 # è§è¿‡çš„æ€»æ ·æœ¬æ•° (ç”¨äºè“„æ°´æ± æ¦‚ç‡è®¡ç®—)

    def add(self, item):
        """
        å­˜å…¥è®°å¿† (item: tuple(snn_vec, llm_vec))
        """
        # A. æ€»æ˜¯å­˜å…¥çŸ­æœŸç¼“å†²åŒº (ä¿æŒé²œæ´»åº¦)
        self.recent_buffer.append(item)
        
        # B. æ¦‚ç‡å­˜å…¥é•¿æœŸæ¡£æ¡ˆåº“ (è“„æ°´æ± é‡‡æ ·)
        self.total_seen += 1
        
        if len(self.archive_buffer) < self.archive_cap:
            # å¦‚æœæ¡£æ¡ˆåº“æ²¡æ»¡ï¼Œç›´æ¥å¡«å…¥
            self.archive_buffer.append(item)
        else:
            # å¦‚æœæ»¡äº†ï¼Œä»¥ p = k / n çš„æ¦‚ç‡æ›¿æ¢æ—§æ ·æœ¬
            # éšç€æ—¶é—´æ¨ç§» (nå˜å¤§)ï¼Œæ–°æ ·æœ¬è¿›å…¥æ¡£æ¡ˆåº“çš„æ¦‚ç‡é€æ¸é™ä½
            # è¿™ä¿è¯äº†æ¡£æ¡ˆåº“ä¸­ä¿ç•™äº†"åˆ›ä¸–çºª"ä»¥æ¥çš„å‡åŒ€æ ·æœ¬åˆ†å¸ƒï¼Œé˜²æ­¢é—å¿˜åˆå¿ƒ
            idx = random.randint(0, self.total_seen - 1)
            if idx < self.archive_cap:
                self.archive_buffer[idx] = item

    def sample(self, batch_size):
        """
        æ··åˆé‡‡æ · (Mixed Replay)
        ç­–ç•¥: 70% å¤ä¹ æ—§çŸ¥è¯† (Archive) + 30% å­¦ä¹ æ–°çŸ¥è¯† (Recent)
        """
        # åŠ¨æ€è°ƒæ•´ï¼šå¦‚æœæ¡£æ¡ˆåº“è¿˜å¤ªå«©ï¼Œå°±åªé‡‡çŸ­æœŸ
        if len(self.archive_buffer) < 10:
            if not self.recent_buffer: return []
            return random.sample(self.recent_buffer, min(len(self.recent_buffer), batch_size))
            
        # è®¡ç®—é…æ¯”
        n_archive = int(batch_size * 0.7)
        n_recent = batch_size - n_archive
        
        batch = []
        # ä»é•¿æœŸè®°å¿†é‡‡æ ·
        if self.archive_buffer:
            batch.extend(random.sample(self.archive_buffer, min(len(self.archive_buffer), n_archive)))
        # ä»çŸ­æœŸè®°å¿†é‡‡æ ·
        if self.recent_buffer:
            batch.extend(random.sample(self.recent_buffer, min(len(self.recent_buffer), n_recent)))
            
        return batch

    def __len__(self):
        return len(self.recent_buffer) + len(self.archive_buffer)


class MemoryInfrastructure:
    """
    [æ•°å­—æµ·é©¬ä½“ V3.0 - PyTorch Nativeç‰ˆ]
    ä¸“ä¸º Windows ä¼˜åŒ–ï¼Œç§»é™¤ Milvus ä¾èµ–ã€‚
    ä½¿ç”¨ PyTorch Tensor (.pt) å­˜å‚¨å‘é‡ï¼ŒJSON å­˜å‚¨å…ƒæ•°æ®ã€‚
    å®ç°æ•°å­¦ä¸Šçš„çº¯ç²¹æ€§ï¼šMemory is just a Matrix.
    """
    def __init__(self, db_path="nezha_memories.db", vector_dir="nezha_vectors"):
        self.db_path = os.path.join(Config.EVOLUTION_DIR, db_path)
        self.vector_dir = os.path.join(Config.EVOLUTION_DIR, vector_dir)
        self.lock = threading.RLock()
        
        # 1. åˆå§‹åŒ– SQL (ä¿æŒä¸å˜ï¼Œç”¨äºæ—¥å¿—å’ŒDPO)
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self._init_sql_tables()
        
        # 2. åˆå§‹åŒ– å‘é‡å­˜å‚¨ (PyTorch Backend)
        if not os.path.exists(self.vector_dir): os.makedirs(self.vector_dir)
        
        self.vec_path = os.path.join(self.vector_dir, "memory_tensors.pt")
        self.meta_path = os.path.join(self.vector_dir, "memory_meta.json")
        
        self.vectors = None # Tensor [N, Dim]
        self.metadata = []  # List[Dict]
        
        self._load_vectors()
        
        # æ‰“å°åˆå§‹åŒ–ä¿¡æ¯
        cnt = len(self.metadata)
        dim = self.vectors.shape[1] if self.vectors is not None else "Unknown"
        print(f"ğŸ’½ [åŸºç¡€è®¾æ–½] PyTorch åŸç”Ÿè®°å¿†ä½“å°±ç»ª (Items: {cnt} | Dim: {dim})")

    def _init_sql_tables(self):
        """åˆå§‹åŒ– SQL è¡¨ç»“æ„ (ä¿æŒåŸæ ·)"""
        with self.lock:
            cursor = self.conn.cursor()
            # æ—¥å¸¸äº¤äº’æ—¥å¿—
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS daily_logs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                    user_input TEXT,
                    ai_response TEXT,
                    action_type TEXT,
                    feedback REAL
                )
            ''')
            # DPO æ•°æ®é›†
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS dpo_dataset (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    prompt TEXT UNIQUE,
                    chosen TEXT,
                    rejected TEXT,
                    confidence REAL,
                    source TEXT,
                    is_trained BOOLEAN DEFAULT 0
                )
            ''')
            # è¿›åŒ–æŒ‡æ ‡
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS evolution_metrics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    age INTEGER,
                    genome_snapshot TEXT,
                    avg_loss REAL,
                    survival_duration REAL,
                    final_atp REAL
                )
            ''')
            # é‡å­æ€ç»´æ—¥å¿—
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS quantum_logs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                    task_type TEXT,
                    prompt TEXT,
                    best_path_id INTEGER,
                    best_score REAL,
                    details_json TEXT,
                    is_processed BOOLEAN DEFAULT 0
                )
            ''')
            self.conn.commit()

    # --- å‘é‡æ ¸å¿ƒæ–¹æ³• (PyTorch Implementation) ---

    def _load_vectors(self):
        """ä»ç£ç›˜åŠ è½½å¼ é‡å’Œå…ƒæ•°æ®"""
        try:
            if os.path.exists(self.vec_path) and os.path.exists(self.meta_path):
                # map_location='cpu' ç¡®ä¿ä¸å ç”¨ GPU æ˜¾å­˜
                self.vectors = torch.load(self.vec_path, map_location='cpu')
                with open(self.meta_path, 'r', encoding='utf-8') as f:
                    self.metadata = json.load(f)
            else:
                self.vectors = None
                self.metadata = []
        except Exception as e:
            print(f"âš ï¸ [Memory] ç´¢å¼•åŠ è½½å¤±è´¥ï¼Œé‡ç½®è®°å¿†: {e}")
            self.vectors = None
            self.metadata = []

    def _save_vectors(self):
        """åŸå­åŒ–ä¿å­˜åˆ°ç£ç›˜"""
        try:
            if self.vectors is not None:
                torch.save(self.vectors, self.vec_path)
                with open(self.meta_path, 'w', encoding='utf-8') as f:
                    json.dump(self.metadata, f, ensure_ascii=False)
        except Exception as e:
            print(f"âš ï¸ [Memory] ä¿å­˜å¤±è´¥: {e}")

    def save_vector_memory(self, text, vector, metadata=None):
        """
        å­˜å…¥å•æ¡è®°å¿†
        :param vector: å¯ä»¥æ˜¯ list, numpy array æˆ– torch tensor
        """
        try:
            # 1. ç»Ÿä¸€è½¬æ¢ä¸º PyTorch Tensor (CPU)
            if isinstance(vector, list):
                t_vec = torch.tensor(vector, dtype=torch.float32)
            elif isinstance(vector, np.ndarray):
                t_vec = torch.from_numpy(vector).float()
            elif isinstance(vector, torch.Tensor):
                t_vec = vector.detach().cpu().float()
            else:
                return

            # ç¡®ä¿æ˜¯ 2D [1, Dim]
            if t_vec.dim() == 1:
                t_vec = t_vec.unsqueeze(0)

            with self.lock:
                # [å®‰å…¨æ£€æŸ¥] ç»´åº¦ä¸€è‡´æ€§
                if self.vectors is not None:
                    if t_vec.shape[1] != self.vectors.shape[1]:
                        print(f"âš ï¸ [Memory] ç»´åº¦ä¸åŒ¹é… (New:{t_vec.shape[1]} vs DB:{self.vectors.shape[1]})ï¼Œå¿½ç•¥å†™å…¥ã€‚")
                        return

                # 2. åŠ¨æ€æ‹¼æ¥ Tensor
                if self.vectors is None:
                    self.vectors = t_vec
                else:
                    self.vectors = torch.cat((self.vectors, t_vec), dim=0)
                
                # 3. æ›´æ–°å…ƒæ•°æ®
                meta = metadata if metadata else {}
                meta.update({
                    "text": str(text),
                    "timestamp": datetime.now().isoformat()
                })
                self.metadata.append(meta)
                
                # 4. å®æ—¶è½ç›˜
                self._save_vectors()
                
        except Exception as e:
            print(f"âš ï¸ Vector Insert Error: {e}")


    def search_vector_memory(self, query_vector, limit=3):
        """
        [V1.0 æ•°å­¦ä¿®æ­£ç‰ˆ] åŒ…å«æ—¶é—´è¡°å‡æ ¸çš„å‘é‡æ£€ç´¢
        å…¬å¼: Score = Cosine(q, m) * (Base + (1-Base) * exp(-lambda * delta_t))
        """
        if self.vectors is None or len(self.metadata) == 0:
            return ""
            
        try:
            # 1. å¤„ç†æŸ¥è¯¢å‘é‡ (ä¿æŒä¸å˜)
            if isinstance(query_vector, list):
                q_vec = torch.tensor(query_vector, dtype=torch.float32)
            elif isinstance(query_vector, torch.Tensor):
                q_vec = query_vector.detach().cpu().float()
            else:
                return "" 

            if q_vec.dim() == 1:
                q_vec = q_vec.unsqueeze(0) # [1, Dim]

            # [å®‰å…¨æ£€æŸ¥] ç»´åº¦
            if q_vec.shape[1] != self.vectors.shape[1]:
                return ""

            # 2. è®¡ç®—åŸºç¡€ä½™å¼¦ç›¸ä¼¼åº¦ (Raw Semantic Score)
            db_norm = F.normalize(self.vectors, p=2, dim=1)
            q_norm = F.normalize(q_vec, p=2, dim=1)
            
            # [1, Dim] @ [Dim, N] -> [1, N]
            raw_scores = torch.mm(q_norm, db_norm.t()).squeeze(0)
            
            # 3. [æ–°å¢] è®¡ç®—æ—¶é—´è¡°å‡æƒé‡ (Temporal Kernel)
            current_timestamp = datetime.now().timestamp()
            time_weights = []
            
            # è¡°å‡ç³»æ•° lambda: å†³å®šé—å¿˜é€Ÿåº¦
            # 5e-6 â‰ˆ 1.6å¤©è¡°å‡åˆ° 50% (çŸ­æœŸè®°å¿†æ•æ„Ÿ)
            # 1e-6 â‰ˆ 8.0å¤©è¡°å‡åˆ° 50% (é•¿æœŸè®°å¿†æ•æ„Ÿ)
            decay_lambda = 5e-6 
            
            # è®°å¿†åº•åº§ (Base): 0.3 è¡¨ç¤ºå³ä½¿æ˜¯å†è€çš„è®°å¿†ï¼Œæƒé‡ä¹Ÿä¸ä¼šä½äº 0.3
            # é˜²æ­¢é‡è¦ä½†ä¹…è¿œçš„è®°å¿†è¢«å½»åº•é—å¿˜
            base_weight = 0.3

            for meta in self.metadata:
                try:
                    t_str = meta.get('timestamp', '')
                    if t_str:
                        # å°è¯•è§£æ ISO æ ¼å¼
                        try:
                            mem_time = datetime.fromisoformat(t_str).timestamp()
                        except ValueError:
                            # å…œåº•ï¼šå¦‚æœæ ¼å¼ä¸å¯¹ï¼Œå°è¯•ç®€å•è§£ææˆ–è§†ä¸ºå½“å‰æ—¶é—´
                            mem_time = current_timestamp 
                        
                        delta_t = max(0, current_timestamp - mem_time)
                        
                        # æƒé‡å…¬å¼: w = Base + (1 - Base) * e^(-lambda * dt)
                        # åˆšäº§ç”Ÿæ—¶æƒé‡ä¸º 1.0ï¼Œéšæ—¶é—´è¶‹è¿‘äº 0.3
                        w = base_weight + (1.0 - base_weight) * math.exp(-decay_lambda * delta_t)
                    else:
                        w = 0.5 # æ— æ—¶é—´æˆ³çš„è®°å¿†ï¼Œç»™äºˆä¸­æ€§æƒé‡
                except:
                    w = 0.5
                time_weights.append(w)
            
            # è½¬æ¢ä¸º Tensor (CPU)
            t_weights = torch.tensor(time_weights, dtype=torch.float32)
            
            # 4. [æ–°å¢] èåˆåˆ†æ•° (Hadamard Product)
            # æœ€ç»ˆåˆ†æ•° = è¯­ä¹‰ç›¸ä¼¼åº¦ * æ—¶é—´é²œæ´»åº¦
            # ç¡®ä¿ç»´åº¦ä¸€è‡´è¿›è¡Œå¹¿æ’­ä¹˜æ³•
            if len(raw_scores) == len(t_weights):
                final_scores = raw_scores * t_weights
            else:
                final_scores = raw_scores # ç»´åº¦å¯¹ä¸ä¸Šæ—¶(æç½•è§)å›é€€åˆ°åŸå§‹åˆ†æ•°
            
            # 5. Top-K æå–
            k = min(limit, len(self.metadata))
            if k == 0: return ""
            
            top_k = torch.topk(final_scores, k)
            
            ret = []
            for score, idx in zip(top_k.values, top_k.indices):
                # [è°ƒæ•´] ç”±äºä¹˜äº†æƒé‡ï¼Œåˆ†æ•°æ•´ä½“ä¼šä¸‹é™ï¼Œå°†é˜ˆå€¼ä» 0.35 ä¸‹è°ƒè‡³ 0.25
                if score > 0.25: 
                    idx = idx.item()
                    if idx < len(self.metadata): 
                        meta = self.metadata[idx]
                        text_content = meta.get('text', 'No Text')
                        source = meta.get('source', 'Unknown')
                        # æ˜¾ç¤ºæœ€ç»ˆå¾—åˆ†ï¼Œæ–¹ä¾¿è°ƒè¯•è§‚å¯Ÿæ—¶é—´æƒé‡çš„æ•ˆæœ
                        ret.append(f"[æ¥æº:{source} | ç»¼åˆåˆ†:{score:.2f}] {text_content}")
            
            return "\n".join(ret)

        except Exception as e:
            print(f"âš ï¸ Vector Search Error: {e}")
            return ""

    # =========== çŸ¥è¯†åº“æ‰«æ (é€‚é…æ–°æ¶æ„) ===========
    def ingest_knowledge_base(self, kb_dir, embedder_func):
        """
        æ‰«æç›®å½• -> åˆ‡ç‰‡ -> è°ƒç”¨å¤–éƒ¨ embedder -> å­˜å…¥ Tensor
        """
        if not os.path.exists(kb_dir): return

        print(f"ğŸ“š [æµ·é©¬ä½“] æ­£åœ¨æ‰«æå¤–éƒ¨çŸ¥è¯†åº“ (Native Mode): {kb_dir}")
        
        # 1. ç®€å•çš„å¢é‡å»é‡ (åŸºäº source æ–‡ä»¶å)
        existing_files = set(m.get('source') for m in self.metadata if 'source' in m)
        
        valid_exts = ('.txt', '.md', '.py', '.pdf', '.docx')
        files = [f for f in os.listdir(kb_dir) if f.lower().endswith(valid_exts)]
        
        new_docs = []
        for f in files:
            if f in existing_files: continue
            
            file_path = os.path.join(kb_dir, f)
            # UniversalFileInfo ç±»å¿…é¡»åœ¨ Nezha.py ä¸­å·²å®šä¹‰
            content = UniversalFileInfo.read_content(file_path) 
            
            if len(content) < 50: continue

            # åˆ‡ç‰‡
            chunk_size = 500
            overlap = 50
            for i in range(0, len(content), chunk_size - overlap):
                chunk = content[i : i + chunk_size]
                if len(chunk) > 20:
                    new_docs.append({
                        "text": chunk,
                        "source": f
                    })

        # 2. æ‰¹é‡å‘é‡åŒ–å¹¶å­˜å‚¨
        if new_docs:
            print(f"   -> å‘ç° {len(new_docs)} ä¸ªæ–°è®°å¿†åˆ‡ç‰‡ï¼Œæ­£åœ¨å‘é‡åŒ– (è¿™å¯èƒ½éœ€è¦ä¸€ä¼šå„¿)...")
            
            # æå–æ‰€æœ‰æ–‡æœ¬
            all_texts = [d['text'] for d in new_docs]
            
            # æ‰¹é‡è®¡ç®— (è°ƒç”¨ä¼ å…¥çš„å›è°ƒå‡½æ•°)
            try:
                # å‡è®¾ embedder_func è¿”å› tensor æˆ– list
                all_vectors = embedder_func(all_texts)
                
                # å¦‚æœè¿”å›çš„æ˜¯ listï¼Œè½¬ tensor
                if isinstance(all_vectors, list):
                    all_vectors = torch.tensor(all_vectors, dtype=torch.float32)
                elif isinstance(all_vectors, torch.Tensor):
                    all_vectors = all_vectors.cpu().float()

                # ç»´åº¦æ£€æŸ¥ä¸æ‹¼æ¥
                with self.lock:
                    if self.vectors is not None:
                         if all_vectors.shape[1] != self.vectors.shape[1]:
                             print(f"âš ï¸ åµŒå…¥ç»´åº¦ä¸åŒ¹é…ï¼Œå–æ¶ˆå¯¼å…¥ã€‚")
                             return
                    
                    if self.vectors is None:
                        self.vectors = all_vectors
                    else:
                        self.vectors = torch.cat((self.vectors, all_vectors), dim=0)
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    for d in new_docs:
                        self.metadata.append({
                            "text": d['text'],
                            "source": d['source'],
                            "timestamp": datetime.now().isoformat()
                        })
                    
                    self._save_vectors()
                    
            except Exception as e:
                print(f"   -> âš ï¸ å‘é‡åŒ–å¤±è´¥: {e}")
                
            print(f"   -> âœ… çŸ¥è¯†åº“åŒæ­¥å®Œæˆï¼Œå½“å‰æ€»è®°å¿†é‡: {len(self.metadata)}")
        else:
            print(f"   -> (çŸ¥è¯†åº“æ— æ–°å¢å†…å®¹)")

    # --- SQL ç›¸å…³æ¥å£ (ä¿æŒä¸å˜) ---
    def log_interaction(self, u, a, act, feedback):
        try:
            with self.lock:
                with self.conn:
                    self.conn.execute(
                        "INSERT INTO daily_logs (user_input, ai_response, action_type, feedback) VALUES (?, ?, ?, ?)",
                        (str(u), str(a), str(act), float(feedback))
                    )
        except Exception as e:
            print(f"âš ï¸ SQL Log Error: {e}")

    def add_dpo_pair(self, prompt, chosen, rejected, conf, source):
        try:
            with self.lock:
                with self.conn:
                    self.conn.execute(
                        "INSERT OR IGNORE INTO dpo_dataset (prompt, chosen, rejected, confidence, source) VALUES (?, ?, ?, ?, ?)",
                        (prompt, chosen, rejected, conf, source)
                    )
        except Exception as e:
            print(f"âš ï¸ SQL DPO Error: {e}")

    def get_untrained_dpo(self, limit=100):
        with self.lock:
            cursor = self.conn.cursor()
            cursor.execute("SELECT prompt, chosen, rejected FROM dpo_dataset WHERE is_trained=0 LIMIT ?", (limit,))
            return cursor.fetchall()

    def mark_dpo_trained(self, prompts):
        if not prompts: return
        with self.lock:
            with self.conn:
                self.conn.executemany("UPDATE dpo_dataset SET is_trained=1 WHERE prompt=?", [(p,) for p in prompts])

    def get_random_interactions(self, limit=20):
        try:
            with self.lock:
                cursor = self.conn.cursor()
                cursor.execute('''
                    SELECT user_input, ai_response 
                    FROM daily_logs 
                    WHERE user_input IS NOT NULL AND ai_response IS NOT NULL
                    ORDER BY RANDOM() 
                    LIMIT ?
                ''', (limit,))
                return cursor.fetchall()
        except Exception as e:
            print(f"âš ï¸ SQL Random Recall Error: {e}")
            return []

    def log_quantum_experience(self, task_type, prompt, best_id, best_score, details):
        try:
            with self.lock, self.conn:
                self.conn.execute(
                    "INSERT INTO quantum_logs (task_type, prompt, best_path_id, best_score, details_json) VALUES (?, ?, ?, ?, ?)",
                    (task_type, prompt, best_id, best_score, json.dumps(details, ensure_ascii=False))
                )
        except Exception as e:
            print(f"âš ï¸ SQL Quantum Log Error: {e}")

    def get_unprocessed_quantum_logs(self, limit=50):
        with self.lock:
            cursor = self.conn.cursor()
            cursor.execute('''
                SELECT id, prompt, best_path_id, details_json 
                FROM quantum_logs 
                WHERE is_processed=0 AND best_path_id != -1 AND best_score > 0.6
                LIMIT ?
            ''', (limit,))
            return cursor.fetchall()

    def mark_quantum_processed(self, ids):
        if not ids: return
        try:
            with self.lock, self.conn:
                self.conn.executemany("UPDATE quantum_logs SET is_processed=1 WHERE id=?", [(i,) for i in ids])
        except Exception as e:
            print(f"âš ï¸ SQL Mark Error: {e}")

# ==============================================================================
# æ ¸å¿ƒç”Ÿç‰©ç»„ä»¶
# ==============================================================================

class GeneticEngine:
    """è´Ÿè´£åŸºå› çš„é«˜æ–¯å˜å¼‚(ç”¨äºè´å¶æ–¯å†·å¯åŠ¨æˆ–å…œåº•)"""
    @staticmethod
    def mutate(genome, intensity=0.1):
        new_genome = genome.copy()
        # print(f"ğŸ§¬ [DNA] æ­£åœ¨è¿›è¡ŒåŸºå› é‡ç»„ (å¼ºåº¦: {intensity})...")
        for key, value in new_genome.items():
            if isinstance(value, float):
                # é«˜æ–¯å˜å¼‚
                mutation = random.gauss(0, intensity)
                new_value = value * (1.0 + mutation)
                # åŸºå› é”ï¼šé˜²æ­¢æ•°å€¼å´©å
                if "rate" in key: new_value = max(0.001, new_value)
                if "threshold" in key: new_value = max(0.1, new_value)
                new_genome[key] = new_value
        return new_genome


# ==============================================================================
# [NEW] è¿›åŒ–å¼•æ“: è´å¶æ–¯ä¼˜åŒ– (Bayesian Optimization)
# ==============================================================================
class BayesianGeneticEngine:
    """
    [è´å¶æ–¯è¿›åŒ–å™¨]
    ä¸å†ç›²ç›®éšæœºå˜å¼‚ï¼Œè€Œæ˜¯å»ºç«‹é«˜æ–¯è¿‡ç¨‹æ¨¡å‹ï¼Œ
    æ ¹æ®å†å²çš„ (åŸºå›  -> å­˜æ´»è¡¨ç°) é¢„æµ‹ä¸‹ä¸€ä»£æœ€ä¼˜å‚æ•°ã€‚
    """
    def __init__(self, db_manager):
        self.db = db_manager
        
        # 1. å®šä¹‰åŸºå› çš„æœç´¢ç©ºé—´ (ä¸Šä¸‹ç•Œ)
        self.pbounds = {
            'metabolism_rate': (0.5, 3.0),    # ä»£è°¢ç‡
            'reward_sensitivity': (1.0, 10.0),# å¤šå·´èƒºæ•æ„Ÿåº¦
            'anxiety_base_rate': (0.005, 0.1),# ç„¦è™‘å¢é•¿ç‡
            'curiosity_threshold': (1.0, 5.0),# å¥½å¥‡å¿ƒé˜ˆå€¼
            'stress_resilience': (0.1, 0.9),  # æŠ—å‹èƒ½åŠ›
            'risk_taking': (0.1, 0.9)         # å†’é™©ç³»æ•°
        }
        
        # 2. åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.optimizer = BayesianOptimization(
            f=None, # æˆ‘ä»¬ä½¿ç”¨ ask-and-tell æ¨¡å¼ï¼Œä¸éœ€è¦ä¼ é€’ç›®æ ‡å‡½æ•°
            pbounds=self.pbounds,
            random_state=1,
            verbose=0 # é™é»˜æ¨¡å¼
        )
        
        # 3. æ¢å¤å†å²è®°å¿† (Warm Start)
        self.history_loaded = False # åˆå§‹åŒ–æ ‡å¿—ä½
        self._load_history()

    # æ ¸å¿ƒè¯„åˆ†é€»è¾‘ (æ”¾åœ¨è¿™é‡Œï¼Œä¾›å…¶ä»–æ–¹æ³•è°ƒç”¨)
    def _calculate_target_score(self, duration, atp, loss_history=None, avg_loss=None):
        """
        [Math Core] Nezha-Sortino Quotient (ç”Ÿç‰©ç´¢æè¯ºå•†æ•°)
        
        åŸç†ï¼šè¿›åŒ–ä¸ä»…ä»…æ˜¯è¿½æ±‚ ATP æ€»é‡ï¼ˆé‚£ä¼šå¯¼è‡´çŸ­è§†çš„è´ªå©ªï¼‰ï¼Œ
        è€Œæ˜¯è¿½æ±‚â€œå•ä½é£é™©ä¸‹çš„è¶…é¢ç”Ÿå­˜è´¨é‡â€ã€‚
        
        å…¬å¼: Score = (R - R_f) / (Downside_Deviation + epsilon)
        å…¶ä¸­:
          R (Return): ATP ç§¯ç´¯æ•ˆç‡ (ATP / Age)
          R_f (Risk-free): åŸºç¡€ä»£è°¢ç»´æŒçº¿
          Risk: æƒŠå¥‡åº¦(Loss) çš„ä¸‹è¡Œæ³¢åŠ¨ç‡ (åªæƒ©ç½šæ„šè ¢çš„é”™è¯¯ï¼Œä¸æƒ©ç½šé¡¿æ‚Ÿ)
        """
        # 1. å®šä¹‰æ— é£é™©æ”¶ç›Šç‡ (Risk-Free Rate)
        # å‡è®¾æ¯æ´» 1 ä¸ª tick è‡³å°‘éœ€è¦æ¶ˆè€— 0.5 ATP (åŸºç¡€ä»£è°¢çº¿)
        risk_free_rate = 0.5 
        
        # 2. è®¡ç®—èµ„äº§å›æŠ¥ç‡ (Return)
        # ATP ç§¯ç´¯æ•ˆç‡ (ATP / Age)
        safe_duration = max(1.0, float(duration))
        asset_return = float(atp) / safe_duration
        
        # 3. è®¡ç®—é£é™© (Risk / Volatility)
        # é»˜è®¤é£é™© (å…œåº•)
        downside_deviation = 0.1

        # æƒ…å†µ A: æœ‰è¯¦ç»†çš„ Loss å†å² (è®¡ç®—ä¸‹è¡Œæ³¢åŠ¨)
        if loss_history and len(loss_history) > 5: # è‡³å°‘è¦æœ‰å‡ ä¸ªç‚¹æ‰èƒ½ç®—æ–¹å·®
            try:
                losses = np.array(loss_history)
                
                # ç´¢æè¯ºæ ¸å¿ƒï¼šåªè®¡ç®—â€œä¸å¯å®¹å¿çš„è®¤çŸ¥è¯¯å·®â€ (Downside Deviation)
                target_loss = 0.1 # å®¹å¿é˜ˆå€¼
                
                # ç­›é€‰å‡ºæ‰€æœ‰å¤§äº target_loss çš„éƒ¨åˆ†
                downside_losses = losses[losses > target_loss]
                
                if len(downside_losses) > 0:
                    # è®¡ç®—ä¸‹è¡Œå·®å€¼çš„å¹³æ–¹å‡å€¼çš„æ ¹
                    downside_diffs = downside_losses - target_loss
                    downside_deviation = np.sqrt(np.mean(np.square(downside_diffs)))
                else:
                    # å¦‚æœæ‰€æœ‰ Loss éƒ½å¾ˆä½ï¼Œç»™äºˆæå°çš„é£é™©å€¼ï¼ˆå¥–åŠ±ç¨³å¥ï¼‰
                    downside_deviation = 0.01
            except Exception:
                # Numpy å‡ºé”™æ—¶çš„å…œåº•
                downside_deviation = max(0.01, float(avg_loss)) if avg_loss else 0.1

        # æƒ…å†µ B: åªæœ‰å¹³å‡ Loss (å†å²å­˜æ¡£)
        else:
            safe_loss = avg_loss if avg_loss is not None else 1.0
            downside_deviation = max(0.01, float(safe_loss))

        # 4. è®¡ç®—ç´¢æè¯ºæ¯”ç‡
        excess_return = asset_return - risk_free_rate
        # åˆ†æ¯åŠ  epsilon é˜²æ­¢é™¤é›¶
        sortino_ratio = excess_return / (downside_deviation + 1e-6)
        
        # 5. å¼•å…¥ç”Ÿå­˜æ—¶é•¿çš„å¯¹æ•°å¥–åŠ± (Longevity Bonus)
        longevity_bonus = math.log1p(safe_duration) * 0.5
        
        # æœ€ç»ˆå¾—åˆ†
        final_score = sortino_ratio + longevity_bonus
        
        # [æå€¼ä¿æŠ¤] é™åˆ¶åŒºé—´
        return max(-10.0, min(100.0, final_score))


    def _load_history(self):
        """ä» SQL è¯»å–å¾€ä¸–çš„å†å²ï¼Œè®©è´å¶æ–¯èµ¢åœ¨èµ·è·‘çº¿ (æ•°å­¦ä¿®æ­£ç‰ˆ)"""
        try:
            with self.db.lock:
                cursor = self.db.conn.cursor()
                cursor.execute("SELECT genome_snapshot, survival_duration, final_atp, avg_loss FROM evolution_metrics")
                rows = cursor.fetchall()
            
            if rows:
                print(f"ğŸ§¬ [è¿›åŒ–å¼•æ“] ç»§æ‰¿äº† {len(rows)} ä»£ç¥–å…ˆçš„è®°å¿†...")
                for genome_json, duration, atp, loss in rows:
                    try:
                        params = json.loads(genome_json)
                        
                        # ã€ä¿®æ”¹ã€‘ è¿™é‡Œä¸å†å†™æ­»å…¬å¼ï¼Œè€Œæ˜¯è°ƒç”¨æ–°æ–¹æ³•
                        target_score = self._calculate_target_score(duration, atp, loss)

                        self.optimizer.register(params=params, target=target_score)
                    except: continue
                self.history_loaded = True
            else:
                print("ğŸ§¬ [è¿›åŒ–å¼•æ“] åˆ›ä¸–çºª (Genesis Mode) - æ— å†å²æ•°æ®")
        except Exception as e:
            print(f"âš ï¸ å†å²åŠ è½½å¤±è´¥: {e}")

    def suggest_mutation(self, current_genome):
        """
        [æ ¸å¿ƒ] è¯¢é—®è´å¶æ–¯ç¥è°•ï¼šä¸‹ä¸€ä¸–è¯¥å¦‚ä½•è°ƒæ•´åŸºå› ï¼Ÿ
        """
        # å†·å¯åŠ¨ä¿æŠ¤
        if not self.history_loaded and len(self.optimizer.space) < 5:
            return GeneticEngine.mutate(current_genome, intensity=0.2)
            
        try:
            next_point = self.optimizer.suggest(utility_kind="ucb", kappa=2.5)
            new_genome = current_genome.copy()
            for key, val in next_point.items():
                new_genome[key] = float(val)
            return new_genome
        except:
            return GeneticEngine.mutate(current_genome, intensity=0.1)

    def record_epoch(self, genome, age, avg_loss, atp, loss_history_snapshot=None): # <--- å‚æ•°æ›´æ–°
        """è®°å½•è¿™ä¸€ä¸–çš„æœ€ç»ˆå¾—åˆ†"""
        try:
            if age < 1: return
            
            # 1. å­˜å…¥ SQL (ä¿æŒåŸæ ·)
            with self.db.lock:
                with self.db.conn:
                    self.db.conn.execute(
                        "INSERT INTO evolution_metrics (age, genome_snapshot, avg_loss, survival_duration, final_atp) VALUES (?, ?, ?, ?, ?)",
                        (age, json.dumps(genome), avg_loss, age, atp)
                    )
            
            # 2. å®æ—¶å‘ŠçŸ¥è´å¶æ–¯ä¼˜åŒ–å™¨
            # [ä¿®æ”¹] ä½¿ç”¨æ–°çš„ç´¢æè¯ºå…¬å¼
            current_score = self._calculate_target_score(
                duration=age, 
                atp=atp, 
                loss_history=loss_history_snapshot, # ä¼ å…¥å¿«ç…§
                avg_loss=avg_loss
            )
            
            valid_params = {k: v for k, v in genome.items() if k in self.pbounds}
            
            try:
                self.optimizer.register(params=valid_params, target=current_score)
                print(f"      ğŸ§¬ [è´å¶æ–¯] å·²å¸æ”¶æœ¬ä¸–ç»éªŒ (Score: {current_score:.2f})")
            except Exception:
                pass

        except Exception as e:
            print(f"âš ï¸ è®°å½•å¤±è´¥: {e}")


class SurvivalInstinct:
    """æä»æ ¸ï¼šå‹åŠ›è°ƒèŠ‚ä¸­å¿ƒ"""
    def __init__(self, genome):
        self.genome = genome
        self.pressure_level = 1.0
        
    def analyze_feedback(self, daily_logs):
        """æ ¹æ®å·®è¯„ç‡å’ŒæŠ—å‹åŸºå› è®¡ç®—å‹åŠ›"""
        if not daily_logs: return 1.0
        
        negatives = sum(1 for log in daily_logs if log.get('feedback', 0) < 0)
        failure_rate = negatives / len(daily_logs)
        
        # è¯»å–åŸºå› 
        resilience = self.genome.get('stress_resilience', 0.3)
        cortisol = self.genome.get('cortisol_level', 2.0)
        
        if failure_rate > resilience:
            self.pressure_level = cortisol # å‹åŠ›é£™å‡
            print(f"ğŸ’“ [æä»æ ¸] å‹åŠ›è¿‡è½½! (å¤±è´¥ç‡ {failure_rate:.1%} > æŠ—å‹ {resilience})")
        elif failure_rate == 0:
            self.pressure_level = 0.8 # å®‰é€¸
        else:
            self.pressure_level = 1.0 # æ­£å¸¸
        return self.pressure_level



class IntrinsicCuriosityModule:
    """
    [Hybrid Upgrade] æ··åˆå¥½å¥‡å¿ƒæ¨¡å— V2.1 (Batch-Robust Edition)
    ç­–ç•¥: å¿«æ…¢ç³»ç»Ÿç»“åˆ (Fast-Slow Architecture)
    1. Fast Check: è®¡ç®—å•æ¬¡æ¨ç†çš„é¦™å†œç†µã€‚
    2. Deep Check: å¯åŠ¨ MC Dropout è®¡ç®—äº’ä¿¡æ¯ (BALD)ï¼Œä¸¥æ ¼åŒºåˆ†è®¤çŸ¥ä¸ç¡®å®šæ€§ã€‚
    """
    def __init__(self, genome):
        self.genome = genome
        # åŸºç¡€é˜ˆå€¼ (ç”±åŸºå› æ§åˆ¶)
        self.base_threshold = self.genome.get('curiosity_threshold', 3.5)
        
    def _compute_shannon_entropy(self, logits):
        """[å¿«ç³»ç»Ÿ] è®¡ç®—åŸºç¡€é¦™å†œç†µ (æ”¯æŒ Batch)"""
        # logits shape: [Batch, Vocab]
        # 1. æ•°å€¼ç¨³å®šæ€§å¤„ç†
        logits = logits - torch.max(logits, dim=-1, keepdim=True)[0]
        # 2. è®¡ç®—æ¦‚ç‡
        probs = torch.softmax(logits, dim=-1)
        log_probs = torch.log(probs + 1e-9)
        
        # 3. è®¡ç®—ç†µ H(x) = -sum(p * log p)
        # dim=-1 (Vocabç»´åº¦) æ±‚å’Œï¼Œä¿ç•™ Batch ç»´åº¦
        entropy_per_sample = -torch.sum(probs * log_probs, dim=-1) # [Batch]
        
        # è¿”å›å¹³å‡ç†µï¼ˆç”¨äºæ ‡é‡åˆ¤æ–­ï¼‰ï¼Œä½†åœ¨æ‰¹é‡åœºæ™¯ä¸‹ï¼Œä¸Šå±‚é€»è¾‘å¯èƒ½éœ€è¦å•ç‹¬å¤„ç†
        return entropy_per_sample.mean().item()

    def _compute_epistemic_uncertainty(self, model, input_ids, k_samples=5):
        """
        [æ…¢ç³»ç»Ÿ] MC Dropout æ·±åº¦ç¡®è¯Š (Batch-Safe)
        è®¡ç®— BALD (Bayesian Active Learning by Disagreement)
        å…¬å¼: Epistemic = Total_Entropy - Aleatoric_Entropy
        """
        was_training = model.training
        model.train() # å¼€å¯ Dropout
        
        probs_list = []
        
        try:
            with torch.no_grad():
                for _ in range(k_samples):
                    outputs = model(input_ids)
                    # å–æœ€åä¸€ä¸ª token: [Batch, Vocab]
                    last_token_logits = outputs.logits[:, -1, :]
                    probs = torch.softmax(last_token_logits, dim=-1)
                    probs_list.append(probs)
        finally:
            if not was_training:
                model.eval()

        # 1. å †å : [K, Batch, Vocab]
        # æ­¤æ—¶ä¸è¦ squeezeï¼Œä¿ç•™ Batch ç»´åº¦ï¼Œå³ä½¿å®ƒæ˜¯ 1
        stack_probs = torch.stack(probs_list, dim=0) 
        
        # 2. è®¡ç®— Aé¡¹: å¹³å‡é¢„æµ‹ (Expected Prediction) -> [Batch, Vocab]
        # åœ¨ K ç»´åº¦ (dim=0) ä¸Šå–å¹³å‡
        mean_probs = stack_probs.mean(dim=0)
        
        # è®¡ç®— Aé¡¹çš„ç†µ (Total Uncertainty): [Batch]
        # H(mean(p))
        total_entropy = -torch.sum(mean_probs * torch.log(mean_probs + 1e-9), dim=-1)
        
        # 3. è®¡ç®— Bé¡¹: ç†µçš„å¹³å‡ (Expected Entropy / Aleatoric)
        # å…ˆç®—æ¯æ¬¡é‡‡æ ·çš„ç†µ: [K, Batch, Vocab] -> [K, Batch]
        sample_entropies = -torch.sum(stack_probs * torch.log(stack_probs + 1e-9), dim=-1)
        # å†å¯¹ K å–å¹³å‡: [Batch]
        aleatoric_entropy = sample_entropies.mean(dim=0)
        
        # 4. è®¡ç®—è®¤çŸ¥ä¸ç¡®å®šæ€§ (Epistemic / Mutual Information): [Batch]
        epistemic = torch.clamp(total_entropy - aleatoric_entropy, min=0.0)
        
        # è¿”å› batch çš„å¹³å‡å€¼ä½œä¸ºæœ€ç»ˆæ ‡é‡åˆ†æ•°
        return epistemic.mean().item()

    def check_curiosity(self, model, input_ids, current_logits):
        """
        [ä¸»å…¥å£] æ··åˆå†³ç­–æµ
        :param current_logits: [Batch, Vocab]
        :return: (is_curious, score, check_type)
        """
        # --- 1. å¿«ç³»ç»Ÿåˆç­› (Fast Pass) ---
        fast_entropy = self._compute_shannon_entropy(current_logits)
        
        if fast_entropy < self.base_threshold:
            return False, fast_entropy, "FAST"

        # --- 2. æ…¢ç³»ç»Ÿç¡®è¯Š (Deep Check) ---
        # åªæœ‰å½“æ¨¡å‹æ„Ÿåˆ°å›°æƒ‘æ—¶ï¼Œæ‰å¯ç”¨æ˜‚è´µçš„ MC Dropout
        epistemic_score = self._compute_epistemic_uncertainty(model, input_ids, k_samples=5)
        
        # åŠ¨æ€é˜ˆå€¼ç¼©æ”¾ (è®¤çŸ¥ä¸ç¡®å®šæ€§é€šå¸¸æ•°å€¼è¾ƒå°ï¼Œæ”¾å¤§ä»¥ä¾¿ä¸é˜ˆå€¼æ¯”è¾ƒ)
        adjusted_score = epistemic_score * 5.0 
        
        # åªæœ‰å½“è®¤çŸ¥ä¸ç¡®å®šæ€§å æ®ä¸»å¯¼æ—¶ï¼Œæ‰è®¤ä¸ºæ˜¯â€œå¥½å¥‡â€
        # é˜ˆå€¼è®¾ä¸ºåŸºç¡€é˜ˆå€¼çš„ä¸€åŠ (å› ä¸º MC Dropout å·²ç»è¿‡æ»¤äº†å™ªå£°ï¼Œè¿™é‡Œçš„ Score çº¯åº¦å¾ˆé«˜)
        is_curious = adjusted_score > (self.base_threshold * 0.5)
        
        return is_curious, adjusted_score, "DEEP"



# ==============================================================================
# ç¥ç»å¤–ç§‘åŒ»ç”Ÿï¼šè´Ÿè´£ LoRA Adapter çš„åŠ¨æ€æ‰‹æœ¯ (Rank ä¼¸ç¼©)
# ==============================================================================
class NeuroSurgeon:
    """è´Ÿè´£ LoRA Adapter çš„åŠ¨æ€åˆ‡æ¢ã€æ‰©å®¹ä¸å‹ç¼©"""
    
    @staticmethod
    def setup_dual_brain(model):
        """åˆå§‹åŒ–åŒç³»ç»Ÿçš®å±‚ (å¢å¼ºç‰ˆï¼šè‡ªåŠ¨è¯†åˆ« MoE æ¶æ„å¹¶é€‚é…å±‚çº§åç§°ï¼Œè‡ªåŠ¨å¤„ç† PeftModel åŒ…è£…)"""
        print("ğŸ§  [å¤–ç§‘æ‰‹æœ¯] æ­£åœ¨åˆ’åˆ†å¤§è„‘åŒºåŸŸ (Fast vs Slow)...")
        
        # --- [æ–°å¢] 1. æ™ºèƒ½æ¶æ„è¯†åˆ« ---
        # ç›®çš„ï¼šåˆ¤æ–­å½“å‰æ¨¡å‹æ˜¯ä¼ ç»Ÿçš„ Dense ç»“æ„è¿˜æ˜¯è¿›åŒ–åçš„ MoE ç»“æ„
        is_moe = False
        # æ£€æµ‹ 1: ç‰©ç†æ ‡è®°æ–‡ä»¶ (ç”±é£å‡ç¨‹åºç”Ÿæˆ)
        if os.path.exists(Config.IS_MOE_FLAG_FILE):
            is_moe = True
        # æ£€æµ‹ 2: æ¨¡å‹é…ç½® (åŒé‡ä¿é™©ï¼Œæ£€æŸ¥ config ä¸­æ˜¯å¦æœ‰ moe ç›¸å…³å­—æ®µ)
        elif hasattr(model, "config") and (
            "moe" in getattr(model.config, "model_type", "").lower() or
            getattr(model.config, "num_local_experts", 0) > 0
        ):
            is_moe = True

        # --- [æ–°å¢] 2. åŠ¨æ€å®šä¹‰æ‰‹æœ¯ç›®æ ‡ (Target Modules) ---
        # åŸºç¡€æ³¨æ„åŠ›å±‚ (ä»»ä½•æ¶æ„éƒ½æœ‰)
        targets_attn = ["q_proj", "k_proj", "v_proj", "o_proj"]
        
        # å®šä¹‰ MLP/MoE ç›¸å…³çš„å±‚åç§°å…³é”®è¯æ± 
        # gate: MoE Router (è·¯ç”±å±‚)
        # w1/w2/w3: Mixtral/Llama ç»“æ„çš„ MLP å±‚
        # gate_proj/up_proj/down_proj: Qwen/Llama ç»“æ„çš„ MLP å±‚
        mlp_keywords = ["gate", "w1", "w2", "w3", "gate_proj", "up_proj", "down_proj"]

        if is_moe:
            print("   -> ğŸ§¬ [æ£€æµ‹åˆ° MoE] æ­£åœ¨è°ƒæ•´ LoRA è§¦æ‰‹ä»¥è¿æ¥ è·¯ç”±å±‚(Router) ä¸ ä¸“å®¶å±‚(Experts)...")
            
            # Slow (é€»è¾‘ç³»ç»Ÿ): å…¨é‡è®­ç»ƒã€‚
            # åŒ…å«æ³¨æ„åŠ›å±‚ + è·¯ç”±å±‚ + æ‰€æœ‰ä¸“å®¶çš„å†…éƒ¨ MLP å±‚
            # Peft ä¼šè‡ªåŠ¨å¿½ç•¥å½“å‰æ¨¡å‹ä¸­ä¸å­˜åœ¨çš„å±‚åï¼Œæ‰€ä»¥å†™å…¨ä¸€ç‚¹æ²¡å…³ç³»
            targets_slow = targets_attn + mlp_keywords
            
            # Fast (ç›´è§‰ç³»ç»Ÿ): è½»é‡è®­ç»ƒã€‚
            # åªè®­ç»ƒ æ³¨æ„åŠ› + è·¯ç”±(gate) + æ™®é€šé—¨æ§(gate_proj)
            # ç­–ç•¥ï¼šæ§åˆ¶â€œè°æ¥å›ç­”â€å’Œâ€œå…³æ³¨ä»€ä¹ˆâ€ï¼Œä½†ä¸ä¿®æ”¹ä¸“å®¶å†…éƒ¨åºå¤§çš„çŸ¥è¯†åº“ï¼ŒèŠ‚çœæ˜¾å­˜
            targets_fast = targets_attn + ["gate", "gate_proj"] 
        else:
            print("   -> ğŸ§¬ [æ£€æµ‹åˆ° Dense] ä½¿ç”¨æ ‡å‡†å±‚çº§æ˜ å°„ã€‚")
            # Dense Slow: è®­ç»ƒ Attention + MLP
            targets_slow = targets_attn + ["gate_proj", "up_proj", "down_proj"]
            # Dense Fast: åªè®­ç»ƒ Attention (ä¿æŒæç®€)
            targets_fast = targets_attn

        # --- [ä¿®æ”¹] 3. å®šä¹‰é…ç½® (ä½¿ç”¨åŠ¨æ€è®¡ç®—çš„ target_modules) ---
        
        # [å¯é€‰ä¼˜åŒ–] MoE æ¨¡å¼ä¸‹ï¼Œå¦‚æœ Rank è¿‡å¤§å®¹æ˜“ OOMï¼Œå¯åœ¨æ­¤å¤„åŠ¨æ€é™çº§ï¼Œè¿™é‡Œæš‚æ—¶ä¿æŒåŸé…ç½®
        
        fast_config = LoraConfig(
            r=Config.RANK_FAST, 
            lora_alpha=Config.RANK_FAST * 2, 
            lora_dropout=0.05, 
            bias="none", task_type="CAUSAL_LM",
            target_modules=targets_fast # <--- [ä¿®æ”¹] ä½¿ç”¨åŠ¨æ€å˜é‡
        )

        # æ˜¾å­˜ä¿æŠ¤ç­–ç•¥
        # MoE æ¨¡å¼ä¸‹ï¼Œåº•æ¨¡æ˜¾å­˜å ç”¨ç¿»å€ï¼Œä¸” target_modules å˜å¤šï¼ŒLoRA å‚æ•°é‡ä¼šæ¿€å¢ã€‚
        # å¼ºåˆ¶å°† Rank é™åˆ¶åœ¨å®‰å…¨èŒƒå›´ï¼ˆå¦‚ 64ï¼‰ï¼Œé˜²æ­¢ OOMã€‚
        real_rank_slow = Config.RANK_SLOW
        if is_moe and real_rank_slow > 64:
            print(f"      âš ï¸ [æ˜¾å­˜ä¿æŠ¤] MoE æ¨¡å¼ä¸‹å°† Slow Rank é™åˆ¶ä¸º 64 (åŸ: {real_rank_slow})")
            real_rank_slow = 64

        slow_config = LoraConfig(
            r=Config.RANK_SLOW, 
            lora_alpha=Config.RANK_SLOW * 2, 
            lora_dropout=0.05, 
            bias="none", task_type="CAUSAL_LM",
            target_modules=targets_slow # <--- [ä¿®æ”¹] ä½¿ç”¨åŠ¨æ€å˜é‡
        )

        # 4. æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯ PeftModel (ä¿ç•™åŸæœ‰ç¨³å¥é€»è¾‘)
        # å¦‚æœä¸æ˜¯ï¼Œè¯´æ˜æ˜¯ç¬¬ä¸€æ¬¡åˆå§‹åŒ–ï¼Œå¿…é¡»ç”¨ get_peft_model åŒ…è£…ä¸€æ¬¡
        if not isinstance(model, PeftModel):
            if os.path.exists(Config.ADAPTER_FAST_PATH):
                # å¦‚æœç£ç›˜æœ‰å­˜æ¡£ï¼Œç›´æ¥åŠ è½½å­˜æ¡£å˜æˆ PeftModel
                model = PeftModel.from_pretrained(model, Config.ADAPTER_FAST_PATH, adapter_name="fast")
            else:
                # å¦‚æœæ²¡æœ‰å­˜æ¡£ï¼Œç”¨ fast é…ç½®åˆå§‹åŒ–ä¸€ä¸ª PeftModel
                model = get_peft_model(model, fast_config, adapter_name="fast")
        else:
            # å¦‚æœå·²ç»æ˜¯ PeftModelï¼Œå°è¯•åŠ è½½æˆ–æ·»åŠ  fast
            if "fast" not in model.peft_config:
                if os.path.exists(Config.ADAPTER_FAST_PATH):
                    model.load_adapter(Config.ADAPTER_FAST_PATH, adapter_name="fast")
                else:
                    # æ˜¾å¼æŒ‡å®šå‚æ•°åï¼Œé˜²æ­¢å‚æ•°ä½ç½®å†²çª
                    model.add_adapter(adapter_name="fast", peft_config=fast_config)

        # 5. å¤„ç† Slow Adapter (æ­¤æ—¶ model ä¸€å®šæ˜¯ PeftModel äº†ï¼Œå¯ä»¥ç›´æ¥ç”¨ add_adapter/load_adapter)
        if "slow" not in model.peft_config:
            if os.path.exists(Config.ADAPTER_SLOW_PATH):
                model.load_adapter(Config.ADAPTER_SLOW_PATH, adapter_name="slow")
            else:
                # ã€ä¿®å¤ç‚¹ 2ã€‘æ˜¾å¼æŒ‡å®šå‚æ•°åï¼Œè§£å†³æŠ¥é”™ TypeError
                model.add_adapter(adapter_name="slow", peft_config=slow_config)
            
        return model

    @staticmethod
    def resize_adapter(model, adapter_name, new_rank):
        """
        [æ ¸å¿ƒæ‰‹æœ¯] åŠ¨æ€è°ƒæ•´ LoRA Rank (SVD å¢å¼º + å®‰å…¨é”)
        æœºåˆ¶ï¼šæå–æ—§æƒé‡ -> SVDåˆ†è§£ä¿ç•™ä¸»æˆåˆ†(å‹ç¼©) æˆ– å™ªå£°å¡«å……(æ‰©å®¹)
        """
        try:
            # 1. è·å–æ—§é…ç½®å’Œæƒé‡
            old_config = model.peft_config[adapter_name]
            old_rank = old_config.r
            
            if old_rank == new_rank:
                return model # æ— éœ€æ‰‹æœ¯

            print(f"ğŸ’‰ [æ‰‹æœ¯å®¤] æ‰§è¡Œç¥ç»é‡å¡‘: {adapter_name} (Rank {old_rank} -> {new_rank})...")

            # 1. æå–æ—§æƒé‡ (Clone åˆ° CPU æˆ–ä¿ç•™åœ¨ GPU å‡å¯ï¼Œå…³é”®æ˜¯ detach)
            old_weights = {}
            for n, p in model.named_parameters():
                if adapter_name in n and "lora_" in n:
                    old_weights[n] = p.clone().detach()

            # 2. åˆ é™¤æ—§ Adapter
            model.delete_adapter(adapter_name)

            # 3. æ–°å»º Adapter
            new_config = copy.deepcopy(old_config)
            new_config.r = new_rank
            new_config.lora_alpha = new_rank * 2 
            
            # ã€ä¿®å¤ç‚¹ 3ã€‘è¿™é‡Œä¹Ÿå»ºè®®ä½¿ç”¨æ˜¾å¼å‚æ•°ï¼Œè™½ç„¶ peft_config æ˜¯ add_adapter çš„ç¬¬äºŒä¸ªå‚æ•°ï¼Œä½†æ˜¾å¼æ›´å®‰å…¨
            model.add_adapter(adapter_name=adapter_name, peft_config=new_config)

            # 4. æƒé‡è¿ç§» (å¼€å¯ no_grad ä¿æŠ¤)
            # âš ï¸ å¿…é¡»ä½¿ç”¨ no_gradï¼Œå¦åˆ™æ‰‹æœ¯è¿‡ç¨‹ä¼šè¢«è®¡å…¥æ¢¯åº¦å›¾ï¼Œå¯¼è‡´æ˜¾å­˜æ³„æ¼æˆ–æŠ¥é”™
            with torch.no_grad():
                for n, p in model.named_parameters():
                    if adapter_name in n and "lora_" in n:
                        if n in old_weights:
                            old_p = old_weights[n]
                            new_p_data = p.data
                            
                            # --- SVD å‹ç¼© (Rank â†“) ---
                            if new_rank < old_rank:
                                try:
                                    old_p_f32 = old_p.float()
                                    
                                    # === ç»Ÿè®¡å­¦è¾¹ç•Œå¤„ç† (Statistical Clamping) ===
                                    # 1. è¯†åˆ«æœ‰æ•ˆå€¼ (é NaN ä¸” é Inf)
                                    finite_mask = torch.isfinite(old_p_f32)
                                    
                                    if finite_mask.any():
                                        # è®¡ç®—æœ‰æ•ˆå€¼çš„ç»Ÿè®¡åˆ†å¸ƒ
                                        valid_elements = old_p_f32[finite_mask]
                                        mean_val = valid_elements.mean()
                                        std_val = valid_elements.std()
                                        
                                        # è®¾å®š 3-Sigma è¾¹ç•Œ (è¦†ç›– 99.7% çš„æ­£å¸¸æ•°æ®)
                                        # è¿™ç§æˆªæ–­æ¯”ç›´æ¥ç”¨ max_val æ›´æŸ”å’Œï¼Œä¿æŠ¤äº†å¥‡å¼‚å€¼è°±ç»“æ„
                                        clamp_min = mean_val - 3 * std_val
                                        clamp_max = mean_val + 3 * std_val
                                        
                                        # 2. å¯¹æ•´ä¸ªçŸ©é˜µè¿›è¡Œè½¯æˆªæ–­ (å¤„ç† Inf å’Œæç«¯ç¦»ç¾¤å€¼)
                                        old_p_f32 = torch.clamp(old_p_f32, min=clamp_min, max=clamp_max)
                                        
                                        # 3. å¯¹ NaN è¿›è¡Œå‡å€¼å¡«å…… (æœ€ä¸­æ€§çš„å¡«å……)
                                        old_p_f32 = torch.nan_to_num(old_p_f32, nan=mean_val)
                                    else:
                                        # æç«¯å…œåº•ï¼šå¦‚æœå…¨éƒ½æ˜¯åå€¼ï¼Œé‡ç½®ä¸º 0
                                        old_p_f32.zero_()
                                    
                                    # === SVD åˆ†è§£ ===
                                    if "lora_A" in n:
                                        # A (r_old, dim) -> å‹ç¼©è¡Œ
                                        U, S, Vh = torch.linalg.svd(old_p_f32, full_matrices=False)
                                        # new_A = S[:new] * Vh[:new]
                                        compressed = torch.diag(S[:new_rank]) @ Vh[:new_rank, :]
                                        new_p_data.copy_(compressed.to(p.dtype))
                                    elif "lora_B" in n:
                                        # B (dim, r_old) -> å‹ç¼©åˆ—
                                        U, S, Vh = torch.linalg.svd(old_p_f32, full_matrices=False)
                                        # new_B = U[:new] * S[:new]
                                        compressed = U[:, :new_rank] @ torch.diag(S[:new_rank])
                                        new_p_data.copy_(compressed.to(p.dtype))
                                        
                                except Exception as svd_err:
                                    print(f"      âš ï¸ SVD æ•°å­¦å¼‚å¸¸ï¼Œå›é€€åˆ°åˆ‡ç‰‡æ¨¡å¼: {svd_err}")
                                    # å¤±è´¥å›é€€é€»è¾‘
                                    if "lora_A" in n: new_p_data.copy_(old_p[:new_rank, :])
                                    elif "lora_B" in n: new_p_data.copy_(old_p[:, :new_rank])

                            # --- å™ªå£°æ‰©å®¹ (Rank â†‘) ---
                            else:
                                if "lora_A" in n:
                                    new_p_data[:old_rank, :] = old_p
                                    
                                    # ç»Ÿä¸€è‡´æ•¬ PhysicalNeuroSurgeon çš„åŠ¨æ€å™ªå£°ç­–ç•¥
                                    # è®¡ç®—åŸæƒé‡çš„æ ‡å‡†å·®ï¼Œå– 1% æˆ– 0.1% ä½œä¸ºå™ªå£°å¼ºåº¦
                                    std = old_p.std()
                                    if std < 1e-6: std = 0.01 # é˜²æ­¢å…¨0å±‚çš„é™¤é›¶é£é™©
                                    
                                    # å¡«å……æ–°ç»´åº¦
                                    new_p_data[old_rank:, :].normal_(mean=0.0, std=std * 0.1) 
                                    
                                elif "lora_B" in n:
                                    new_p_data[:, :old_rank] = old_p
                                    new_p_data[:, old_rank:].zero_()

            print(f"âœ… [æ‰‹æœ¯æˆåŠŸ] å¤§è„‘çš®å±‚é‡å¡‘å®Œæ¯•ã€‚")
            model.set_adapter(adapter_name)
            return model
            
        except Exception as e:
            print(f"âŒ [æ‰‹æœ¯å¤±è´¥] {e}")
            return model

    @staticmethod
    def switch_mode(model, mode="fast"):
        """åˆ‡æ¢æ€è€ƒæ¨¡å¼"""
        model.set_adapter(mode)
        return mode


# ==============================================================================
# [æ–°å¢ç»„ä»¶] ç¥ç»è„‘æ¡¥ (Neural Bridge)
# åŠŸèƒ½ï¼šå®ç° SNN (çš®å±‚) åˆ° LLM (èº¯ä½“) çš„æ•°å­¦æŠ•å½±ï¼Œè§£å†³èº«å¿ƒäºŒå…ƒè®ºã€‚
# ==============================================================================
# ==============================================================================
# [æ–°å¢] BridgeVAE ç±»ï¼šæ½œæ„è¯†æŠ•å½±ä»ª
# ==============================================================================
class BridgeVAE(nn.Module):
    """
    [Math Core] å˜åˆ†è‡ªç¼–ç å™¨ (VAE)
    åŠŸèƒ½ï¼šå°† SNN çš„ç¨€ç–è„‰å†²çŠ¶æ€æ˜ å°„åˆ° LLM çš„ç¨ å¯†è¯­ä¹‰æµå½¢ã€‚
    æ¶æ„ï¼šEncoder (SNN -> Latent) -> Sampling -> Decoder (Latent -> LLM Embeddings)
    """
    def __init__(self, input_dim, output_dim, latent_dim=64):
        super().__init__()
        
        # Encoder: å‹ç¼©æ„ŸçŸ¥ (SNN State -> Latent Distribution)
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.SiLU(),
            nn.Linear(256, latent_dim * 2) # è¾“å‡º mu å’Œ logvar
        )
        
        # Decoder: è¯­ä¹‰é‡æ„ (Latent -> LLM Embedding)
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.SiLU(),
            nn.Linear(256, output_dim)
        )

    def reparameterize(self, mu, logvar):
        """é‡å‚æ•°åŒ–æŠ€å·§: z = mu + sigma * epsilon"""
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x):
        # x: [Batch, SNN_Dim]
        h = self.encoder(x)
        mu, logvar = h.chunk(2, dim=-1)
        
        z = self.reparameterize(mu, logvar)
        out = self.decoder(z)
        
        return out, mu, logvar

# ==============================================================================
# [ä¿®æ”¹] NeuralBridge ç±»ï¼šç¥ç»è„‘æ¡¥ V2.1 (æ™ºèƒ½è®¾å¤‡ç‰ˆ)
# ==============================================================================
class NeuralBridge(nn.Module):
    """
    [System Upgrade] ç¥ç»è„‘æ¡¥ V2.1
    é›†æˆ VAE æ¶æ„ï¼Œæ”¯æŒè‡ªåŠ¨æ˜¾å­˜æ£€æµ‹ä¸ CPU/GPU æ··åˆè®­ç»ƒã€‚
    """
    def __init__(self, snn_dim, llm_dim, num_tokens=2, device="cuda"):
        super().__init__()
        self.device = device
        self.num_tokens = num_tokens
        self.llm_dim = llm_dim
        
        # å®ä¾‹åŒ– VAE (å°å·§è€Œç²¾å¯†)
        # Output Dim = num_tokens * llm_dim (ä¸€æ¬¡ç”Ÿæˆæ‰€æœ‰è½¯ Token)
        self.vae = BridgeVAE(
            input_dim=snn_dim, 
            output_dim=llm_dim * num_tokens,
            latent_dim=64 # æ½œç©ºé—´ç»´åº¦ï¼Œ64 è¶³å¤Ÿæ•æ‰æ½œæ„è¯†ç‰¹å¾
        ).to(device)
        
        # æ³¨æ„ï¼šè¿™é‡Œä¸å†åˆå§‹åŒ– self.optimizerï¼Œæ”¹ä¸ºåœ¨è®­ç»ƒæ—¶åŠ¨æ€åˆ›å»º
        # ä»¥é¿å…è·¨è®¾å¤‡ï¼ˆCPU/GPUï¼‰è®­ç»ƒæ—¶çš„ä¼˜åŒ–å™¨çŠ¶æ€å†²çª

    def forward(self, snn_rates):
        """
        [æ¨ç†æ¨¡å¼] SNN -> LLM
        """
        # 1. æ¬è¿ä¸ç±»å‹è½¬æ¢
        x = snn_rates.to(self.device).float()
        
        # 2. VAE å‰å‘ (åŒ…å«é‡‡æ ·ï¼Œä¿ç•™æ½œæ„è¯†çš„éšæœºæ€§/å¾®æ‰°ç‰¹æ€§)
        out, _, _ = self.vae(x)
        
        # 3. é‡å¡‘ä¸ºåºåˆ— [Batch, Num_Tokens, Dim]
        return out.view(-1, self.num_tokens, self.llm_dim)

    def train_offline(self, snn_data, llm_target_data, epochs=10):
        """
        [å¤œé—´è®­ç»ƒ] æ™ºèƒ½è®¾å¤‡é€‰æ‹© + æµå½¢å¯¹é½
        """
        if not snn_data or not llm_target_data: return 0.0
        
        # --- 1. æ™ºèƒ½è®¾å¤‡é€‰æ‹©é€»è¾‘ ---
        target_device = "cpu" # é»˜è®¤ CPU (ç¨³å¥)
        device_name = "CPU"
        
        if torch.cuda.is_available():
            try:
                # è·å–å½“å‰æ˜¾å­˜çŠ¶æ€
                gpu_idx = self.device.index if self.device.index is not None else 0
                free_mem, _ = torch.cuda.mem_get_info(gpu_idx)
                free_mb = free_mem / 1024 / 1024
                
                # å¦‚æœæ˜¾å­˜ä½™é‡ > 500MBï¼Œå°è¯•ä½¿ç”¨ GPU åŠ é€Ÿ
                if free_mb > 500:
                    target_device = self.device
                    device_name = f"GPU ({free_mb:.0f}MB Free)"
                else:
                    device_name = f"CPU (GPU Full: {free_mb:.0f}MB)"
            except:
                pass 

        print(f"ğŸŒ‰ [è„‘æ¡¥] å¯åŠ¨æµå½¢å¯¹é½è®­ç»ƒ (Mode: {device_name})...")
        
        # 2. æ¨¡å‹æ¬è¿
        self.vae.to(target_device)
        self.vae.train()
        
        # [Fix] æ¯æ¬¡è®­ç»ƒé‡æ–°åˆ›å»ºä¼˜åŒ–å™¨ï¼Œé¿å…è·¨è®¾å¤‡çš„çŠ¶æ€å†²çª
        optimizer = torch.optim.AdamW(self.vae.parameters(), lr=1e-3)

        try:
            # 3. æ•°æ®æ•´ç†ä¸æ¬è¿
            if isinstance(snn_data, list):
                snn_tensor = torch.stack(snn_data).to(target_device).float()
            else:
                snn_tensor = snn_data.to(target_device).float()
                
            if isinstance(llm_target_data, list):
                target_tensor = torch.stack(llm_target_data).to(target_device).float()
            else:
                target_tensor = llm_target_data.to(target_device).float()
                
            # 4. è®­ç»ƒå¾ªç¯
            batch_size = 32
            total_loss = 0
            dataset_size = snn_tensor.size(0)
            
            for epoch in range(epochs):
                # Shuffle
                perm = torch.randperm(dataset_size, device=target_device)
                
                for i in range(0, dataset_size, batch_size):
                    indices = perm[i : i + batch_size]
                    batch_snn = snn_tensor[indices]
                    batch_target = target_tensor[indices]
                    
                    # ç›®æ ‡æ‰©å±•: [Batch, Dim] -> [Batch, Num_Tokens * Dim]
                    batch_target_expanded = batch_target.repeat(1, self.num_tokens)
                    
                    optimizer.zero_grad()
                    recon_x, mu, logvar = self.vae(batch_snn)
                    
                    # Loss = MSE + KL
                    recon_loss = F.mse_loss(recon_x, batch_target_expanded, reduction='mean')
                    # KL æ•£åº¦ (Batch Mean)
                    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / batch_snn.size(0)
                    
                    loss = recon_loss + (0.01 * kl_loss) # Beta-VAE
                    
                    loss.backward()
                    optimizer.step()
                    
                    total_loss += loss.item()
            
            avg_loss = total_loss / (epochs * (dataset_size // batch_size + 1))
            print(f"   -> âœ… å¯¹é½å®Œæˆ (Avg Loss: {avg_loss:.4f})")
            return avg_loss

        except Exception as e:
            print(f"   -> âš ï¸ è„‘æ¡¥è®­ç»ƒå¼‚å¸¸: {e}")
            return 0.0
            
        finally:
            # 5. [å…³é”®] è®­ç»ƒå®Œæ— è®ºå¦‚ä½•éƒ½è¦æ¬å›ä¸»è®¾å¤‡ (GPU)
            self.vae.to(self.device)
            self.vae.eval()
            if target_device != "cpu":
                torch.cuda.empty_cache()


# ==============================================================================
# é«˜çº§ç¥ç»ç»„ä»¶ (PFC, Net2Net, Feeder)
# ==============================================================================

class PrefrontalCortex(nn.Module):
    """[V7.2] å‰é¢å¶çš®å±‚ï¼šåŸºäº Hidden States çš„å…ƒè®¤çŸ¥è·¯ç”±"""
    def __init__(self, input_dim, device="cuda"):
        super().__init__()
        self.output_dim = len(Config.ACTION_SPACE) # åŠ¨ä½œç©ºé—´: SELF, WEB, TEACHER, CODE, MODIFY
        self.router = nn.Linear(input_dim, self.output_dim).to(device)
        self.optimizer = torch.optim.AdamW(self.router.parameters(), lr=1e-3)
        self.device = device
        
        # å°è¯•åŠ è½½æ—§æƒé‡
        if os.path.exists(Config.PFC_ROUTER_PATH):
            try:
                state = torch.load(Config.PFC_ROUTER_PATH)
                
                # åŒæ—¶æ£€æŸ¥ Hidden Size (è¾“å…¥) å’Œ Action Space (è¾“å‡º) æ˜¯å¦åŒ¹é…
                saved_in_dim = state['weight'].shape[1]
                saved_out_dim = state['weight'].shape[0]
                
                if saved_in_dim == input_dim and saved_out_dim == self.output_dim:
                    self.router.load_state_dict(state)
                    print("ğŸ§  [PFC] å·²åŠ è½½å…ƒè®¤çŸ¥è·¯ç”±æƒé‡ã€‚")
                else:
                    print(f"âš ï¸ [PFC] ç»´åº¦å˜æ›´ (Old:{saved_out_dim}x{saved_in_dim} -> New:{self.output_dim}x{input_dim})ï¼Œé‡ç½®è·¯ç”±å™¨ã€‚")
            except Exception as e:
                print(f"âš ï¸ [PFC] æƒé‡åŠ è½½å¤±è´¥: {e}")

    def decide(self, h):
        """
        [æ•°å­¦ä¿®æ­£ç‰ˆ] è¾“å…¥ Hidden Stateï¼Œè¾“å‡ºåŸºäºå½’ä¸€åŒ–ç†µçš„é²æ£’å†³ç­–
        """
        # åŠ¨æ€è·å–æƒé‡ç±»å‹ï¼Œç¡®ä¿ä¸è¾“å…¥å¯¹é½ (fp16/fp32)
        x = h.to(self.device).to(self.router.weight.dtype)
        
        with torch.no_grad():
            logits = self.router(x)
            probs = torch.softmax(logits, dim=-1)
            
            # 1. è·å–æœ€å¤§æ¦‚ç‡åŠ¨ä½œ
            idx = torch.argmax(probs).item()
            # raw_conf = probs[0, idx].item() # (æ—§æŒ‡æ ‡ï¼Œä»…ä½œå‚è€ƒ)
            
            # 2. è®¡ç®—é¦™å†œç†µ H(p) = -sum(p * log(p))
            # [å®‰å…¨ä¼˜åŒ–] ä½¿ç”¨ clamp é™åˆ¶æœ€å°æ¦‚ç‡ï¼Œæ¯” +1e-9 æ›´ç¨³å®šï¼Œé˜²æ­¢ log æº¢å‡º
            p_safe = torch.clamp(probs, min=1e-9, max=1.0)
            entropy = -torch.sum(p_safe * torch.log(p_safe), dim=-1).item()
            
            # 3. è®¡ç®—æœ€å¤§å¯èƒ½çš„ç†µ H_max = ln(N)
            n_actions = len(Config.ACTION_SPACE)
            
            # [è¾¹ç•Œä¿æŠ¤] é˜²æ­¢åŠ¨ä½œç©ºé—´ä¸º 1 æ—¶é™¤ä»¥ 0
            if n_actions > 1:
                max_entropy = math.log(n_actions)
                # 4. è®¡ç®—å½’ä¸€åŒ–ä¸ç¡®å®šåº¦ U (0.0 ~ 1.0)
                uncertainty_norm = entropy / max_entropy
            else:
                uncertainty_norm = 0.0 # åªæœ‰ä¸€ä¸ªåŠ¨ä½œæ—¶ï¼Œå®Œå…¨ç¡®å®š
            
            # 5. è®¡ç®—é²æ£’ç½®ä¿¡åº¦ (Robust Confidence)
            # èŒƒå›´ä¸¥æ ¼åœ¨ 0.0 ~ 1.0 ä¹‹é—´ï¼Œéå¸¸é€‚åˆä¸ Override Threshold æ¯”è¾ƒ
            robust_conf = 1.0 - uncertainty_norm
            
            # é’³ä½ä¸€ä¸‹ç»“æœï¼Œé˜²æ­¢æµ®ç‚¹è¯¯å·®å¯¼è‡´ç•¥å¾®è¶…è¿‡ 1.0 æˆ–ä½äº 0.0
            robust_conf = max(0.0, min(1.0, robust_conf))
        
        actions = Config.ACTION_SPACE
        # è¿”å› robust_conf ä»£æ›¿åŸæ¥çš„ raw_conf
        return actions[idx], robust_conf, idx
        

    def learn(self, h, label_idx):
        """å¼ºåŒ–å­¦ä¹ ï¼šæ›´æ–°è·¯ç”±ç­–ç•¥"""
        self.router.train()
        self.optimizer.zero_grad()
        # æ„é€  Target
        target = torch.tensor([label_idx], device=self.device)
        # å‰å‘ä¼ æ’­
        logits = self.router(h.to(self.device).float())
        loss = F.cross_entropy(logits, target)
        loss.backward()
        self.optimizer.step()
        
        # å®æ—¶ä¿å­˜
        torch.save(self.router.state_dict(), Config.PFC_ROUTER_PATH)
        return loss.item()



# ==============================================================================
# å…¨èƒ½è¿›åŒ–å¸ˆï¼šè´Ÿè´£ Dense ç”Ÿé•¿ä¸ MoE ç»´åº¦é£å‡
# ==============================================================================
class AdvancedPhysicalSurgeon:
    """
    [å…¨èƒ½è¿›åŒ–å¤§å¸ˆ V3.0]
    é˜¶æ®µä¸€ï¼šç‰©ç†ç”Ÿé•¿ (Neurogenesis) - åœ¨æ˜¾å­˜å…è®¸çš„æƒ…å†µä¸‹ï¼Œç›´æ¥åœ¨å†…å­˜ä¸­å¢åŠ å±‚æ•°ã€‚
    é˜¶æ®µäºŒï¼šç»´åº¦é£å‡ (Mergekit MoE) - å½“å•å¡æ˜¾å­˜è€—å°½ï¼Œè°ƒç”¨å¤–éƒ¨å·¥å…·é‡æ„ä¸º MoE é›†ç¾¤ã€‚
    """
    # ===========================
    # é˜¶æ®µä¸€ï¼šç‰©ç†æ˜¾å¾®æ‰‹æœ¯ (é’ˆå¯¹ Dense)
    # ===========================

    @staticmethod
    def grow_layer_robust(model, copy_ratio=0.6):
        print("ğŸŒ± [æ— æŸç”Ÿé•¿] æ­£åœ¨æ‰§è¡Œ Identity Morphism (Net2Net) å¢å±‚æ‰‹æœ¯...")
        
        # 1. å®šä½å±‚ç»“æ„
        if hasattr(model, "model") and hasattr(model.model, "layers"):
            layers = model.model.layers
            config = model.config
        else:
            print("âŒ [æ‰‹æœ¯å¤±è´¥] æ— æ³•å®šä½ Transformer Layers ç»“æ„ã€‚")
            return model

        current_depth = len(layers)
        
        # 2. é€‰æ‹©æºå±‚ (Source Layer) - é€‰å–ä¸­åæ®µ
        target_idx = int(current_depth * copy_ratio)
        print(f"   -> æ­£åœ¨å…‹éš†ç¬¬ {target_idx} å±‚ (Source)...")
        
        source_layer = layers[target_idx]

        # =========== [å®‰å…¨ä¼˜åŒ–] CPU ä¸­è½¬æ‰‹æœ¯å° Start ===========
        # åºåˆ—åŒ–ä¸­è½¬å…‹éš†æ³• Start
        # åŸç†ï¼šdeepcopy ä¼šå…ˆåœ¨ GPU å¤åˆ¶å†æ¬è¿ï¼Œå¯èƒ½å¯¼è‡´ OOMã€‚
        # æ–¹æ¡ˆï¼šä½¿ç”¨ torch.save åºåˆ—åŒ–åˆ° RAMï¼Œå† load å›æ¥æ—¶ç›´æ¥æŒ‡å®š map_location='cpu'ã€‚
        # ç»“æœï¼šæ–°å±‚ç›´æ¥è¯ç”Ÿåœ¨ CPUï¼Œå…¨è¿‡ç¨‹ 0 é¢å¤–æ˜¾å­˜å ç”¨ã€‚
        
        try:
            buffer = io.BytesIO()
            # 1. å°†æºå±‚ä¿å­˜åˆ°å†…å­˜ Buffer (åªè¯» GPU æ•°æ®ï¼Œä¸å æ˜¾å­˜)
            torch.save(source_layer, buffer)
            buffer.seek(0)
            
            # 2. ä» Buffer åŠ è½½åˆ° CPU (ç›´æ¥åœ¨ RAM åˆ›å»ºæ–°å¯¹è±¡)
            new_layer = torch.load(buffer, map_location='cpu')
            
            del buffer # é‡Šæ”¾ç¼“å†²åŒº
            
        except Exception as e:
            print(f"âŒ [å…‹éš†å¤±è´¥] å†…å­˜ä¸è¶³æˆ–åºåˆ—åŒ–é”™è¯¯: {e}")
            return model
        
        # 3. Zero-Initialization (é›¶åˆå§‹åŒ–)
        # å°†æ–°å±‚çš„è¾“å‡ºæƒé‡å¼ºåˆ¶å½’é›¶ï¼Œä½¿å…¶åœ¨åˆå§‹çŠ¶æ€ä¸‹â€œéšå½¢â€
        with torch.no_grad():
            # A. å½’é›¶ Attention è¾“å‡º (o_proj)
            if hasattr(new_layer, "self_attn") and hasattr(new_layer.self_attn, "o_proj"):
                nn.init.constant_(new_layer.self_attn.o_proj.weight, 0)
                if new_layer.self_attn.o_proj.bias is not None:
                    nn.init.constant_(new_layer.self_attn.o_proj.bias, 0)
            
            # B. å½’é›¶ MLP è¾“å‡º (down_proj)
            if hasattr(new_layer, "mlp") and hasattr(new_layer.mlp, "down_proj"):
                nn.init.constant_(new_layer.mlp.down_proj.weight, 0)
                if new_layer.mlp.down_proj.bias is not None:
                    nn.init.constant_(new_layer.mlp.down_proj.bias, 0)

            # C. Norm å±‚å‚æ•°å…‹éš†
            # å³ä½¿ Forward pass æ˜¯ Identity (å› ä¸ºè¾“å‡ºå½’é›¶äº†)ï¼ŒBackward pass çš„æ¢¯åº¦æµ
            # ä¾ç„¶ä¼šç»è¿‡ Input Norm å’Œ Post Attention Normã€‚
            # å¦‚æœä¸å¤åˆ¶æºå±‚çš„ Norm å‚æ•°ï¼ˆå‡å€¼/æ–¹å·®ç¼©æ”¾å› å­ï¼‰ï¼Œæ¢¯åº¦æµçš„æ–¹å·®ä¼šåœ¨æ­¤å¤„çªå˜ã€‚
            
            # 1. å¤åˆ¶ Input LayerNorm (RMSNorm)
            if hasattr(new_layer, "input_layernorm") and hasattr(source_layer, "input_layernorm"):
                new_layer.input_layernorm.weight.data.copy_(source_layer.input_layernorm.weight.data)
                # æŸäº›æ¶æ„å¯èƒ½æœ‰ bias
                if hasattr(new_layer.input_layernorm, "bias") and new_layer.input_layernorm.bias is not None:
                    new_layer.input_layernorm.bias.data.copy_(source_layer.input_layernorm.bias.data)

            # 2. å¤åˆ¶ Post Attention LayerNorm
            if hasattr(new_layer, "post_attention_layernorm") and hasattr(source_layer, "post_attention_layernorm"):
                new_layer.post_attention_layernorm.weight.data.copy_(source_layer.post_attention_layernorm.weight.data)
                if hasattr(new_layer.post_attention_layernorm, "bias") and new_layer.post_attention_layernorm.bias is not None:
                    new_layer.post_attention_layernorm.bias.data.copy_(source_layer.post_attention_layernorm.bias.data)
            
            # D. æå¾®é‡æ‰°åŠ¨ (æ‰“ç ´å¯¹ç§°æ€§)
            for n, p in new_layer.named_parameters():
                if "o_proj" in n or "down_proj" in n:
                    p.data.add_(torch.randn_like(p) * 1e-8) 

        # 4. æ¬è¿å› GPU (æˆ–è€…åŸæ¥çš„ device)
        # è·å–åŸæ¨¡å‹çš„ device
        target_device = source_layer.self_attn.q_proj.weight.device
        try:
            print(f"   -> æ­£åœ¨å°†æ–°å±‚æ¬è¿å› {target_device}...")
            new_layer = new_layer.to(target_device)
        except RuntimeError as e:
            if "out of memory" in str(e):
                print("âŒ [æ‰‹æœ¯å¤±è´¥] åˆå§‹åŒ–æˆåŠŸä½†åœ¨æ¬å› GPU æ—¶æ˜¾å­˜ä¸è¶³ã€‚")
                return model # æ”¾å¼ƒç”Ÿé•¿
            raise e

        # 5. ç‰©ç†æ’å…¥
        layers.insert(target_idx + 1, new_layer)
        
        # 5. æ›´æ–° Config
        if hasattr(config, "num_hidden_layers"): config.num_hidden_layers += 1
        if hasattr(model.config, "num_hidden_layers"): model.config.num_hidden_layers += 1
        
        # 6. é‡ç½® RoPE ç´¢å¼• (è‡³å…³é‡è¦)
        AdvancedPhysicalSurgeon._reindex_layers(layers)

        print(f"âœ… [ç”Ÿé•¿å®Œæˆ] æ·±åº¦: {current_depth} -> {len(layers)}ã€‚")
        return model

    @staticmethod
    def shrink_layer_safe(model):
        print("ğŸ‚ [ç‰©ç†èç¼©] æ­£åœ¨æ‰§è¡Œè„‘å¶åˆ‡é™¤...")
        if hasattr(model, "model") and hasattr(model.model, "layers"):
            layers = model.model.layers
        else:
            return model

        current_depth = len(layers)
        if current_depth <= 24: # è®¾å®šæœ€ä½æ™ºå•†é˜²çº¿
            print(f"âš ï¸ [è­¦å‘Š] å¤§è„‘å·²è¾¾æœ€å°åšåº¦ ({current_depth}å±‚)ï¼Œåœæ­¢åˆ‡é™¤ã€‚")
            return model

        # ç§»é™¤æœ€åä¸€å±‚
        del layers[-1]
        
        # æ›´æ–°é…ç½®
        if hasattr(model.config, "num_hidden_layers"): model.config.num_hidden_layers -= 1
        
        # é‡ç½®ç´¢å¼•
        AdvancedPhysicalSurgeon._reindex_layers(layers)
        
        print(f"âœ… [èç¼©å®Œæˆ] æ·±åº¦: {current_depth} -> {len(layers)}ã€‚")
        return model

    @staticmethod
    def _reindex_layers(layers):
        """ä¿®å¤ Qwen/Llama çš„ RoPE å±‚ç´¢å¼•"""
        for i, layer in enumerate(layers):
            if hasattr(layer, "self_attn") and hasattr(layer.self_attn, "layer_idx"):
                layer.self_attn.layer_idx = i


    # ===========================
    # é˜¶æ®µäºŒï¼šç»´åº¦é£å‡ (é’ˆå¯¹ MoE)
    # ===========================
    @staticmethod
    def evolve_via_mergekit(base_model_path, output_path, num_experts=2): # <--- ä¿®æ”¹é»˜è®¤å€¼ä¸º 2
        print(f"âš—ï¸ [ç‚¼ä¸¹] å¯åŠ¨ Mergekit åè®® (Target Experts={num_experts})...")
        
        if os.path.exists(output_path): shutil.rmtree(output_path)
        os.makedirs(Config.MOE_WORK_DIR, exist_ok=True)

        # [ä¿®æ”¹] å®šä¹‰ 2 ä¸ªä¸“å®¶çš„æ€§æ ¼ (åŒå­æ˜Ÿæ¶æ„)
        # æ—¢ç„¶åªæœ‰2ä¸ªï¼Œæˆ‘ä»¬æŠŠèƒ½åŠ›èšåˆä¸€ä¸‹
        specializations = [
            # ä¸“å®¶1: ç†æ€§è„‘ (è´Ÿè´£é€šç”¨å¯¹è¯ã€é€»è¾‘ã€ä»£ç ã€æ•°å­¦)
            {"name": "Logic_Core", "prompts": [
                "chat", "assistant", "logic", "common sense", 
                "python", "code", "programming", "math", "solve",
                "reasoning", "think", "analysis"
            ]},
            # ä¸“å®¶2: æ„Ÿæ€§è„‘ (è´Ÿè´£æ–‡å­¦ã€è§’è‰²æ‰®æ¼”ã€åˆ›æ„)
            {"name": "Creative_Soul","prompts": [
                "story", "write", "novel", "roleplay", "poem", 
                "emotional", "feeling", "art"
            ]}
        ]
        
        # ç¡®ä¿é…ç½®åˆ—è¡¨é•¿åº¦åŒ¹é… num_experts
        experts_config = []
        for i in range(num_experts):
            # å¾ªç¯å–ç”¨ specializations
            spec = specializations[i % len(specializations)]
            experts_config.append({
                "source_model": base_model_path,
                "positive_prompts": spec["prompts"]
            })
            print(f"      [é…æ–¹] Expert {i+1}: {spec['name']}")

        # ... (åç»­ä»£ç ä¿æŒä¸å˜)

        # æ„é€  Mergekit é…ç½®æ–‡ä»¶
        config_dict = {
            "base_model": base_model_path,
            "gate_mode": "hidden",
            "dtype": "float16", 
            "experts": experts_config
        }

        yaml_path = os.path.join(Config.MOE_WORK_DIR, "moe_recipe.yaml")
        with open(yaml_path, "w", encoding="utf-8") as f:
            yaml.dump(config_dict, f)
            
        print(f"   -> ğŸ“œ é…æ–¹å·²ç”Ÿæˆ: {yaml_path}")

        # è°ƒç”¨ Mergekit å‘½ä»¤è¡Œ
        cmd = [
            "mergekit-moe", yaml_path, output_path,
            "--allow-crimes", "--copy-tokenizer", "--device", "cuda"
        ]
        
        print("   -> ğŸ”¥ ç‚‰ç«å·²å¼€ (æ˜¾å­˜å°†è¢«å¾ç”¨ï¼Œè¯·è€å¿ƒç­‰å¾…)...")
        try:
            # å®æ—¶æ‰“å°æ—¥å¿—
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
            for line in process.stdout:
                if "Loading" in line or "Expert" in line or "Saving" in line:
                    print(f"      [Mergekit] {line.strip()}")
            process.wait()
            
            if process.returncode != 0:
                print(f"âŒ [ç‚¼ä¸¹ç‚¸ç‚‰] Mergekit é€€å‡ºä»£ç : {process.returncode}")
                return False
                
            print(f"âœ… [ç‚¼ä¸¹æˆåŠŸ] MoE æ¨¡å‹ç”Ÿæˆå®Œæ¯•: {output_path}")
            return True
        except Exception as e:
            print(f"âŒ æ‰§è¡Œå¼‚å¸¸: {e}")
            return False


class AcademicFeeder:
    """
    [V8.1 å¢å¼ºç‰ˆ] å­¦æœ¯æŠ•å–‚ç³»ç»Ÿ
    ç‰¹æ€§ï¼šæ”¯æŒ PDF, Word, TXT, Python æºç å…¨æ ¼å¼é˜…è¯»
    ä¿®å¤ç‚¹ï¼šåŠ å…¥äº† Optimizer å’Œ EWC Loss æ•´åˆï¼Œç¡®ä¿çœŸæ­£æ›´æ–°å‚æ•°
    """
    def __init__(self, model, tokenizer, ewc_handler=None):
        self.model = model
        self.tokenizer = tokenizer
        self.ewc = ewc_handler
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨ï¼Œä»…ä¼˜åŒ– requires_grad çš„å‚æ•° (é€šå¸¸æ˜¯ LoRA)
        self.optimizer = torch.optim.AdamW(
            filter(lambda p: p.requires_grad, self.model.parameters()), 
            lr=1e-4
        )

    # ==========================================
    # å¤–éƒ¨è´Ÿç†µæ•è·ç³»ç»Ÿ (Hunting System)
    # ==========================================

    def _fetch_arxiv(self, query="LLM optimization", max_results=3):
        """[ç†è®ºæ•æ‰‹] ä» arXiv æŠ“å–æœ€æ–°è®ºæ–‡æ‘˜è¦"""
        print(f"ğŸ”­ [ç†µå‡] æ­£åœ¨æ‰«æ arXiv å‰æ²¿ç†è®º: '{query}'...")
        try:
            url = f"http://export.arxiv.org/api/query?search_query=all:{quote_plus(query)}&start=0&max_results={max_results}&sortBy=submittedDate&sortOrder=desc"
            response = requests.get(url, timeout=20)
            if response.status_code != 200: return ""

            root = ET.fromstring(response.content)
            ns = {'atom': 'http://www.w3.org/2005/Atom'}
            content_buffer = ""
            
            for entry in root.findall('atom:entry', ns):
                title = entry.find('atom:title', ns).text.strip().replace('\n', ' ')
                summary = entry.find('atom:summary', ns).text.strip().replace('\n', ' ')
                published = entry.find('atom:published', ns).text[:10]
                link = entry.find('atom:id', ns).text
                content_buffer += f"Title: {title}\nDate: {published}\nLink: {link}\nAbstract: {summary}\n\n{'-'*40}\n\n"
            
            print(f"   -> âœ… [arXiv] æ•è·äº† {len(root.findall('atom:entry', ns))} ç¯‡é«˜èƒ½è®ºæ–‡ã€‚")
            return content_buffer
        except Exception as e:
            print(f"   -> âŒ arXiv æ•è·å¼‚å¸¸: {e}")
            return ""

    def _fetch_pubmed(self, query="neuroscience", max_results=3):
        """[ç”Ÿç‰©é›·è¾¾] ä» PubMed æŠ“å–ç”Ÿç‰©åŒ»å­¦/è„‘ç§‘å­¦å‰æ²¿"""
        print(f"ğŸ§¬ [ç†µå‡] æ­£åœ¨è§£æç”Ÿå‘½ç§‘å­¦æ•°æ®åº“ (PubMed): '{query}'...")
        try:
            # 1. ESearch
            search_url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term={quote_plus(query)}&retmode=json&retmax={max_results}"
            search_resp = requests.get(search_url, timeout=10)
            if search_resp.status_code != 200: return ""
            id_list = search_resp.json().get('esearchresult', {}).get('idlist', [])
            if not id_list: return ""

            # 2. EFetch
            ids_str = ",".join(id_list)
            fetch_url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id={ids_str}&retmode=xml"
            fetch_resp = requests.get(fetch_url, timeout=20)
            if fetch_resp.status_code != 200: return ""

            # 3. Parse
            root = ET.fromstring(fetch_resp.content)
            content_buffer = ""
            count = 0
            for article in root.findall('.//PubmedArticle'):
                try:
                    title = article.find('.//ArticleTitle').text
                    abstract_node = article.find('.//Abstract')
                    abstract = " ".join([t.text for t in abstract_node.findall('AbstractText') if t.text]) if abstract_node is not None else "No Abstract"
                    content_buffer += f"Title: {title}\nSource: PubMed\nAbstract: {abstract}\n\n{'-'*40}\n\n"
                    count += 1
                except: continue
            
            print(f"   -> âœ… [PubMed] æå–äº† {count} ç¯‡ç”Ÿç‰©å­¦æ–‡çŒ®ã€‚")
            return content_buffer
        except Exception as e:
            print(f"   -> âŒ PubMed æ‰«æå¼‚å¸¸: {e}")
            return ""

    def _fetch_github_trending(self, language="python"):
        """[ä»£ç æ‹¾è’] æŠ“å– GitHub è¿‘æœŸé«˜æ˜Ÿé¡¹ç›®"""
        print(f"ğŸ‘¾ [ç†µå‡] æ­£åœ¨æŒ–æ˜ GitHub {language} çƒ­é—¨é¡¹ç›®...")
        try:
            date_str = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
            url = f"https://api.github.com/search/repositories?q=created:>{date_str}+language:{language}&sort=stars&order=desc&per_page=3"
            headers = {"Accept": "application/vnd.github.v3+json", "User-Agent": "Nezha-Evolver"}
            
            response = requests.get(url, headers=headers, timeout=20)
            if response.status_code != 200:
                print(f"   -> âš ï¸ GitHub API é™åˆ¶/é”™è¯¯: {response.status_code}")
                return ""
                
            items = response.json().get('items', [])
            content_buffer = ""
            
            for item in items:
                repo_name = item['full_name']
                desc = item.get('description', 'No description')
                # å°è¯•è·å– README
                for branch in ['master', 'main']:
                    raw_url = f"https://raw.githubusercontent.com/{repo_name}/{branch}/README.md"
                    raw_resp = requests.get(raw_url, timeout=5)
                    if raw_resp.status_code == 200:
                        content_buffer += f"Repo: {repo_name}\nDesc: {desc}\nREADME:\n{raw_resp.text[:2000]}\n\n{'-'*40}\n\n"
                        break
            
            print(f"   -> âœ… [GitHub] æå–äº† {len(items)} ä¸ªçƒ­é—¨å·¥ç¨‹é€»è¾‘ã€‚")
            return content_buffer
        except Exception as e:
            print(f"   -> âŒ GitHub æŒ–æ˜å¼‚å¸¸: {e}")
            return ""

    def forage_external_entropy(self):
        """[ä¸»åŠ¨ç‹©çŒ V2.1] å¤šæºç†µå‡ï¼šç‰©ç†(arXiv) + ç”Ÿç‰©(PubMed) + å·¥ç¨‹(GitHub)"""
        
        # ç›®æ ‡å®šä¹‰
        arxiv_topics = ["Large Language Model", "Chain of Thought", "Information Theory", "Quantum Computing", "Complexity Theory"]
        pubmed_topics = ["Synaptic Plasticity", "Hippocampus Memory", "Prefrontal Cortex Decision Making", "Dopamine Reinforcement Learning"]
        
        theory_content = ""
        source_tag = ""

        # === 1. æ·éª°å­å†³å®šç†è®ºæ¥æº ===
        if random.random() < 0.5:
            topic = random.choice(arxiv_topics)
            theory_content = self._fetch_arxiv(query=topic, max_results=3)
            source_tag = f"arXiv ({topic})"
        else:
            topic = random.choice(pubmed_topics)
            theory_content = self._fetch_pubmed(query=topic, max_results=3)
            source_tag = f"PubMed ({topic})"
        
        # === 2. æ‹¾è’ GitHub (è·å–å·¥ç¨‹è½åœ°èƒ½åŠ›) ===
        github_data = self._fetch_github_trending(language="python")
        
        # å¦‚æœä¸¤æ‰‹ç©ºç©ºï¼Œè¿”å› None
        if not theory_content and not github_data:
            return None
            
        # === 3. æ‰“åŒ…é«˜ç†µè¥å…»å— ===
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # [ä¼˜åŒ–] å»ºç«‹æŒ‰æœˆå½’æ¡£ï¼Œé˜²æ­¢æ ¹ç›®å½•çˆ†ç‚¸
        month_str = datetime.now().strftime("%Y-%m")
        save_dir = os.path.join(Config.KNOWLEDGE_BASE, month_str)
        os.makedirs(save_dir, exist_ok=True)

        knowledge_content = (
            f"# [Entropy Injection] Generated at {timestamp}\n"
            f"# Source: {source_tag} & GitHub Trending\n\n"
            f"=== Theoretical Frontier ({source_tag}) ===\n{theory_content}\n\n"
            f"=== Engineering Practice (GitHub) ===\n{github_data}\n"
        )
        
        filename = f"entropy_boost_{timestamp}.txt"
        filepath = os.path.join(save_dir, filename)
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(knowledge_content)
            print(f"ğŸ“¦ [è¡¥ç»™] ç‹©çŒå®Œæˆï¼Œæˆ˜åˆ©å“å·²å…¥åº“: {month_str}/{filename}")
        except Exception as e:
            print(f"âš ï¸ æ–‡ä»¶å†™å…¥å¤±è´¥: {e}")
        
        # [å…³é”®ä¿®æ”¹]ï¼šè¿”å›å…·ä½“çš„çŸ¥è¯†å†…å®¹ï¼Œä¾›æ½œæ„è¯†ç ”è®¨ä½¿ç”¨
        return knowledge_content


    def study(self, adaptive_lr=1e-4, ewc_lambda=0.4):
        """
        [ä¿®æ”¹ç‰ˆ] å¤œé—´å­¦ä¹ å‡½æ•°ï¼šæ”¯æŒ OOM é€’å½’é™çº§ + å­¦ä¹ è¦†ç›–ç‡æŠ¥å‘Š + æ—¥å¿—è®°å½•
        + [æ–°å¢] æ”¯æŒé€’å½’æ‰«æå­ç›®å½• (é€‚é…æŒ‰æœˆå½’æ¡£çš„çŸ¥è¯†åº“)
        """
        # åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼
        self.model.train() 
        
        # æ‰«æçŸ¥è¯†åº“ (æ”¯æŒæ›´å¤šæ–‡ä»¶æ ¼å¼)
        supported_ext = ('.txt', '.py', '.pdf', '.docx', '.md')
        
        # [å¥å£®æ€§] ç¡®ä¿ç›®å½•å­˜åœ¨
        if not os.path.exists(Config.KNOWLEDGE_BASE): os.makedirs(Config.KNOWLEDGE_BASE)
        
        # é€’å½’æ‰«æé€»è¾‘
        # åŸé€»è¾‘: files = [f for f in os.listdir(...) if ...] (åªæ‰«ä¸€å±‚)
        # æ–°é€»è¾‘: ä½¿ç”¨ os.walk éå†æ‰€æœ‰å­ç›®å½•
        files = []
        for root, dirs, filenames in os.walk(Config.KNOWLEDGE_BASE):
            for f in filenames:
                if f.lower().endswith(supported_ext):
                    # å­˜å‚¨å®Œæ•´è·¯å¾„ï¼Œè€Œä¸ä»…ä»…æ˜¯æ–‡ä»¶å
                    full_path = os.path.join(root, f)
                    files.append(full_path)
        
        if not files: return 0
        
        # éšæœºæŠ½å–ä¸€ä¸ªæ–‡ä»¶ (files åˆ—è¡¨ä¸­ç°åœ¨æ˜¯å®Œæ•´è·¯å¾„)
        path = random.choice(files)
        # ä»è·¯å¾„ä¸­æå–æ–‡ä»¶åç”¨äºæ˜¾ç¤º
        filename = os.path.basename(path)
        
        try:

            txt = UniversalFileInfo.read_content(path)

            # è·å–åŸå§‹æ–‡æœ¬é•¿åº¦
            total_len = len(txt)

            # å¦‚æœæå–å†…å®¹å¤ªå°‘ï¼ˆå¯èƒ½æ˜¯æ‰«æç‰ˆPDFæˆ–ç©ºæ–‡ä»¶ï¼‰ï¼Œåˆ™è·³è¿‡
            if total_len < 50:
                print(f"   -> âš ï¸ [è·³è¿‡] {filename} å†…å®¹è¿‡å°‘æˆ–æ— æ³•è¯†åˆ«ã€‚")
                return 0

            print(f"ğŸ“– [å¤œè¯»] ç›®æ ‡: {filename} | é•¿åº¦: {total_len} å­—ç¬¦")

            # è®¾å®šåˆå§‹æœ€å¤§ä¸Šä¸‹æ–‡çª—å£ (Tokenæ•°)
            # é’ˆå¯¹ 24B + 66GB æ˜¾å­˜ï¼Œæˆ‘ä»¬å°è¯•ä» 16k å¼€å§‹
            INITIAL_MAX_TOKENS = 16384 
            
            # [æ–°] ä¼°ç®—å…¨æ–‡éœ€è¦çš„ Token æ•° (ç²—ç•¥ä¼°ç®—ï¼š1 token â‰ˆ 3 char)
            # è¿™å†³å®šäº†åˆ†æ¯ï¼Œç”¨æ¥è®¡ç®—è¦†ç›–ç‡
            estimated_total_tokens = max(1, total_len / 3.0)

            # å­—ç¬¦é¢„ä¼°ä¸æˆªæ–­é€»è¾‘
            CHAR_LIMIT = INITIAL_MAX_TOKENS * 4
            target_txt = txt # é»˜è®¤å­¦å…¨æ–‡
            is_cut = False   # æ ‡è®°æ˜¯å¦å‘ç”Ÿäº†ç‰©ç†æˆªæ–­

            # å¦‚æœæ–‡æœ¬è¶…é•¿ï¼ŒéšæœºæŠ½å–ä¸€ä¸ªâ€œå¤§ç‰‡æ®µâ€
            if total_len > CHAR_LIMIT: 
                is_cut = True
                # éšæœºé€‰ä¸€ä¸ªèµ·å§‹ç‚¹ï¼Œæ¨¡ä»¿â€œç¿»åˆ°è¿™ä¸€ç« å¼€å§‹è¯»â€
                start = random.randint(0, total_len - CHAR_LIMIT)
                target_txt = txt[start : start + CHAR_LIMIT]
                print(f"   -> âœ‚ï¸ [ç‰©ç†æˆªæ–­] æ–‡æœ¬è¿‡é•¿ï¼Œéšæœºé€‰å–ç‰‡æ®µ ({len(target_txt)} å­—ç¬¦)")
            else:
                print(f"   -> ğŸ“œ [å…¨æ–‡æ¨¡å¼] å‡†å¤‡é€šè¯»å…¨æ–‡")

            # ================= å®šä¹‰é€’å½’è®­ç»ƒé—­åŒ…å‡½æ•° =================
            def _recursive_train(current_limit):
                """
                å†…éƒ¨é€’å½’å‡½æ•°ï¼šå°è¯•ä»¥ current_limit é•¿åº¦è®­ç»ƒã€‚
                å¦‚æœ OOMï¼Œåˆ™å‡åŠé‡è¯•ã€‚
                è¿”å›: (loss, actual_tokens)
                """
                # [ç»ˆæ­¢æ¡ä»¶] çª—å£å¤ªå°åˆ™æ”¾å¼ƒï¼Œé¿å…æ­»å¾ªç¯æˆ–æ— æ•ˆå­¦ä¹ 
                if current_limit < 1024:
                    print(f"   -> âŒ æ˜¾å­˜ä¸¥é‡ä¸è¶³ï¼Œå³ä½¿é™çº§åˆ° {current_limit} ä¹Ÿæ— æ³•è®­ç»ƒï¼Œæ”¾å¼ƒã€‚")
                    return 0, 0

                try:
                    # 3. Tokenization & è®­ç»ƒ
                    # truncation=True ä¼šè‡ªåŠ¨æŠŠå¤šä½™çš„åˆ‡æ‰ï¼Œä¿è¯ä¸è¶…è¿‡ max_length
                    # [å…³é”®] ä½¿ç”¨å½“å‰çš„é™åˆ¶ current_limit
                    inputs = self.tokenizer(
                        target_txt, 
                        return_tensors="pt", 
                        truncation=True, 
                        max_length=current_limit 
                    ).to(self.model.device)
                    
                    # [æ–°] è·å–å®é™…å–‚è¿›å»çš„ Token æ•°é‡ï¼Œç”¨äºè®¡ç®—è¦†ç›–ç‡
                    actual_tokens = inputs.input_ids.shape[1]
                    
                    # æ˜¾å­˜é˜²çˆ†æ£€æŸ¥ï¼šå¦‚æœ input å¤ªé•¿å¯¼è‡´ OOMï¼Œè¿™é‡Œ catch ä½
                    self.optimizer.zero_grad()
                    outputs = self.model(**inputs, labels=inputs.input_ids)
                    loss = outputs.loss
                    
                    # EWC æ­£åˆ™åŒ–
                    if self.ewc and self.ewc.fisher:
                        ewc_loss = 0
                        for n, p in self.model.named_parameters():
                            if n in self.ewc.fisher:
                                # [ä¼˜åŒ–] ç¡®ä¿ device ä¸€è‡´ï¼Œé˜²æ­¢æŠ¥é”™
                                fisher_val = self.ewc.fisher[n].to(p.device)
                                opt_val = self.ewc.opt_params[n].to(p.device)
                                ewc_loss += (fisher_val * (p - opt_val) ** 2).sum()
                        loss += ewc_lambda * ewc_loss

                    loss.backward()
                    
                    for param_group in self.optimizer.param_groups:
                        param_group['lr'] = adaptive_lr
                        
                    self.optimizer.step()

                    # [å…³é”®ä¼˜åŒ–] æ˜¾å¼åˆ é™¤æ¢¯åº¦ï¼Œé˜²æ­¢ç´¯ç§¯å ç”¨æ˜¾å­˜ (set_to_none=True æ›´å½»åº•)
                    self.optimizer.zero_grad(set_to_none=True)

                    # å¼ºåˆ¶é‡Šæ”¾æ˜¾å­˜
                    del outputs, inputs
                    torch.cuda.empty_cache()
                    
                    # è¿”å› loss å’Œ å®é™…è®­ç»ƒçš„ token æ•°
                    return loss.item(), actual_tokens
                    
                except RuntimeError as e:
                    # æ˜¾å­˜ä¸è¶³å¤„ç†
                    if "out of memory" in str(e).lower():
                        # è®¡ç®—æ–°çš„ token é™åˆ¶ (å‡åŠ)
                        new_limit = current_limit // 2
                        print(f"   -> âš ï¸ æ˜¾å­˜ä¸è¶³ (OOM) @ {current_limit} Tokensã€‚")
                        print(f"      ğŸ”„ è§¦å‘è‡ªåŠ¨é€‚åº”æœºåˆ¶ï¼šå°è¯•é™çº§è‡³ {new_limit} Tokens...")
                        
                        # å¿…é¡»å½»åº•æ¸…ç†æ˜¾å­˜ï¼Œå¦åˆ™ä¸‹æ¬¡å°è¯•å¿…æŒ‚
                        torch.cuda.empty_cache() 
                        self.optimizer.zero_grad(set_to_none=True)
                        
                        # [é€’å½’è°ƒç”¨] å¸¦ç€æ›´å°çš„é™åˆ¶é‡è¯•
                        return _recursive_train(new_limit)
                    else:
                        raise e # å…¶ä»–é”™è¯¯æ­£å¸¸æŠ›å‡º

            # ================= [æ–°] å¯åŠ¨é€’å½’é€»è¾‘ä¸æŠ¥å‘Šç”Ÿæˆ =================
            
            # ä»æœ€å¤§çš„ 16k å¼€å§‹å°è¯•
            loss_val, used_tokens = _recursive_train(INITIAL_MAX_TOKENS)

            if loss_val > 0:
                # è®¡ç®—è¦†ç›–ç‡ = (æœ¬æ¬¡è®­ç»ƒå®é™…åƒæ‰çš„Token / æ–‡ä»¶æ€»ä¼°ç®—Token) * 100
                coverage = (used_tokens / estimated_total_tokens) * 100
                
                # å¦‚æœå‘ç”Ÿäº†ç‰©ç†æˆªæ–­ï¼Œè¦†ç›–ç‡æ°¸è¿œä¸å¯èƒ½åˆ° 100%ï¼Œè¿™æ˜¯ç¬¦åˆé¢„æœŸçš„
                
                # ç”ŸæˆçŠ¶æ€å›¾æ ‡
                status_icon = "ğŸŸ¢" if coverage > 90 else ("ğŸŸ¡" if coverage > 30 else "ğŸ”´")
                
                # æ‰“å°æ§åˆ¶å°æŠ¥å‘Š
                print(f"   -> âœ… [å­¦ä¹ å®Œæˆ] {status_icon} è¦†ç›–ç‡: {coverage:.1f}% | æ¶ˆè€— Token: {used_tokens}")

                # [æ–°] æ„é€ è¯¦ç»†æ—¥å¿—å¹¶è½ç›˜
                log_entry = (
                    f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | "
                    f"File: {filename} | "
                    f"Status: {status_icon} (Cov: {coverage:.1f}%) | "
                    f"Tokens: {used_tokens}/{int(estimated_total_tokens)} | "
                    f"Loss: {loss_val:.4f}\n"
                )
                
                # å†™å…¥æ—¥å¿—æ–‡ä»¶
                try:
                    with open(os.path.join(Config.EVOLUTION_DIR, "learning_history.log"), "a", encoding="utf-8") as f:
                        f.write(log_entry)
                except Exception as ex:
                    print(f"   -> âš ï¸ æ—¥å¿—å†™å…¥å¤±è´¥: {ex}")

            return loss_val

        except Exception as e: 
            print(f"âš ï¸ Study Err: {e}")
            return 0


    def decide_strategy(self, state, dpo_data_count):
        """
        [Math Refactor] åŸºäºç†µä¸ Utility æƒè¡¡çš„è¿›åŒ–ç­–ç•¥
        é€»è¾‘:
        1. å¦‚æœæ²¡æœ‰ DPO æ•°æ®ï¼Œåªèƒ½ SFTã€‚
        2. å¦‚æœ DPO æ•°æ®ç§¯ç´¯è¿‡å¤š (>10)ï¼Œå¼ºåˆ¶ DPO æ¶ˆåŒ–ã€‚
        3. å…¶ä»–æƒ…å†µï¼Œæ ¹æ®èƒ½é‡å’Œéšæœºæ¦‚ç‡å†³å®šã€‚
        """
        # 1. ç¡¬æ€§é—¨æ§›: DPO æ•°æ®å¿…é¡»è¶³å¤Ÿ (è‡³å°‘ 2 æ¡æ‰èƒ½ç»„æˆ batch)
        if dpo_data_count < 2:
            return "SFT", f"DPOæ•°æ®ä¸è¶³ ({dpo_data_count} < 2)"

        # 2. å †ç§¯æ¸…ç†: æ•°æ®å¤ªå¤šäº†ï¼Œå¿…é¡»è®­ç»ƒ
        if dpo_data_count > 20:
            return "DPO", f"DPOæ•°æ®å †ç§¯ ({dpo_data_count}æ¡)"

        # 3. [Math Fix] åŸºäºç†µçš„åŠ¨æ€å†³ç­–
        # å¦‚æœç†µ (state['entropy']) å¾ˆé«˜ï¼Œè¯´æ˜å¤„äºæœªçŸ¥é¢†åŸŸï¼Œåº”è¯¥ SFT (Exploration/Learning)
        # å¦‚æœç†µå¾ˆä½ï¼Œè¯´æ˜å·²ç»å¾ˆç†Ÿæ‚‰äº†ï¼Œå¯ä»¥ DPO (Exploitation/Refinement)
        
        normalized_entropy = state.get('entropy', 0.5)
        
        # åŠ¨æ€é˜ˆå€¼: èƒ½é‡è¶Šé«˜ï¼Œè¶Šå®¹å¿é«˜ç†µå»æ¢ç´¢
        exploration_threshold = 0.3 + (state.get('atp', 0) / 200.0)
        
        if normalized_entropy > exploration_threshold:
            return "SFT", f"é«˜ç†µçŠ¶æ€ ({normalized_entropy:.2f}) -> éœ€è¦è¡¥å……çŸ¥è¯†"
        else:
            return "DPO", f"ä½ç†µçŠ¶æ€ ({normalized_entropy:.2f}) -> é€‚åˆå¼ºåŒ–ä»·å€¼è§‚"


# ==============================================================================
# [Core V3.0] å¥‡ç‚¹å†…æ ¸å‡çº§ç‰ˆ: é²æ£’ç¥ç»åŠ¨åŠ›å­¦ä¸é—­ç¯æ¨ç†
# ==============================================================================

class NeuralWorldModel(nn.Module):
    """
    [æ•°å­¦æ ¸å¿ƒå‡çº§ V3.0] é²æ£’ç¥ç»ä¸–ç•Œæ¨¡å‹ (Robust Neural World Model)
    -----------------------------------------------------------
    æ¶æ„: æ®‹å·®å­¦ä¹  (Residual Learning) + è½¯é¥±å’Œæ§åˆ¶ (Soft Saturation)
    å…¬å¼: S_{t+1} = Sigmoid( S_t + MLP(S_t, a_t) )
    
    ç‰¹æ€§: 
      1. æ®‹å·®é¢„æµ‹: ç¥ç»ç½‘ç»œåªå­¦ä¹ å˜åŒ–é‡ Deltaï¼Œè€Œéç»å¯¹å€¼ï¼Œè§£å†³æ’ç­‰æ˜ å°„éš¾é¢˜ã€‚
      2. é›¶åˆå§‹åŒ–: åˆå§‹çŠ¶æ€ä¸‹è¾“å‡ºä¸º 0ï¼Œæ¨¡æ‹Ÿç‰©ç†ç³»ç»Ÿçš„æƒ¯æ€§ã€‚
      3. è½¯é¥±å’Œ: ä½¿ç”¨ Sigmoid åœ¨è¾¹ç•Œå¤„äº§ç”Ÿé˜»å°¼ï¼Œç¬¦åˆç”Ÿç‰©ç¥ç»å…ƒçš„æ¿€æ´»ç‰¹æ€§ã€‚
    """
    def __init__(self, action_space, hidden_dim=32, max_memory=1000):
        super().__init__()
        self.action_space = action_space
        self.n_actions = len(action_space)
        self.action_map = {act: i for i, act in enumerate(action_space)}
        
        # [ç‰©ç†å¸¸æ•°] ç”¨äºå½’ä¸€åŒ–çš„åŸºå‡†å€¼
        self.max_atp = 100.0
        self.max_ent = 5.0
        
        self.in_dim = 2 + self.n_actions
        # è¾“å‡ºç»´åº¦ç¿»å€: [ATP_mu, ATP_logvar, Ent_mu, Ent_logvar]
        # æˆ‘ä»¬ç°åœ¨è¾“å‡ºçš„æ˜¯é«˜æ–¯åˆ†å¸ƒçš„å‚æ•° (å‡å€¼ å’Œ å¯¹æ•°æ–¹å·®)
        self.out_dim = 2 * 2
        
        # [ç½‘ç»œç»“æ„] 3å±‚ MLP + LayerNorm
        self.net = nn.Sequential(
            nn.Linear(self.in_dim, hidden_dim),
            nn.LayerNorm(hidden_dim), 
            nn.SiLU(), # SiLU (Swish) æ¿€æ´»å‡½æ•°
            nn.Linear(hidden_dim, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, self.out_dim)
        )
        
        # [é›¶åˆå§‹åŒ–ç­–ç•¥ / Zero Initialization]
        # å°†æœ€åä¸€å±‚çš„æƒé‡åˆå§‹åŒ–ä¸ºæå°å€¼ (Uniform[-0.001, 0.001])ï¼Œåç½®ä¸º 0ã€‚
        # æ„ä¹‰ï¼šåœ¨è®­ç»ƒåˆæœŸï¼Œæ¨¡å‹è¾“å‡ºçš„ Delta æ¥è¿‘ 0ã€‚
        # è¿™æ„å‘³ç€ï¼šå¦‚æœå“ªå’ä»€ä¹ˆéƒ½ä¸å­¦ï¼Œå®ƒé»˜è®¤è®¤ä¸º"ä¸‹ä¸€ç§’çš„ä¸–ç•Œå’Œè¿™ä¸€ç§’ä¸€æ ·" (S_{t+1} = S_t)ã€‚
        # è¿™æ˜¯ç‰©ç†ç³»ç»Ÿæœ€ç¨³å¥çš„å…ˆéªŒå‡è®¾ï¼ˆæƒ¯æ€§å®šå¾‹ï¼‰ã€‚
        nn.init.uniform_(self.net[-1].weight, -0.001, 0.001)
        nn.init.constant_(self.net[-1].bias, 0)
        
        # æ­£äº¤åˆå§‹åŒ–å…¶ä»–å±‚
        # [æ­£äº¤åˆå§‹åŒ–] éšè—å±‚ (å»ºè®®ä¿®æ”¹ gain)
        # ä½œç”¨ï¼šé˜²æ­¢æ¢¯åº¦çš„æ¶ˆå¤±æˆ–çˆ†ç‚¸ï¼Œä¿è¯ç‰¹å¾çš„å»ç›¸å…³æ€§
        for m in self.net[:-1].modules():
            if isinstance(m, nn.Linear):
                # [ä¿®æ”¹å»ºè®®] gain=0.1 å¤ªå°ï¼Œä¼šå¯¼è‡´ SiLU é€€åŒ–ä¸ºçº¿æ€§ã€‚
                # ä½¿ç”¨ calculate_gain('relu') (çº¦ç­‰äº 1.414) æ¥åŒ¹é… SiLU çš„éçº¿æ€§ç‰¹æ€§
                nn.init.orthogonal_(m.weight, gain=nn.init.calculate_gain('relu'))
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

        self.lr = 0.005
        self.optimizer = torch.optim.AdamW(self.net.parameters(), lr=self.lr, weight_decay=1e-4)
        
        self.replay_buffer = deque(maxlen=max_memory)

    def _normalize(self, atp, ent):
        """å½’ä¸€åŒ–åˆ° [0, 1]"""
        return atp / self.max_atp, ent / self.max_ent

    def _denormalize(self, atp_norm, ent_norm):
        """è¿˜åŸç‰©ç†é‡"""
        atp = atp_norm * self.max_atp
        ent = ent_norm * self.max_ent
        return atp, max(0.0, ent)

    # è½¯é¥±å’Œè¾¹ç•Œå‡½æ•° (Soft Saturation)
    # æ›¿ä»£ç¡¬æˆªæ–­ï¼Œä¿è¯å…¨åŸŸæ¢¯åº¦æµ (Gradient Flow)
    @staticmethod
    def _soft_bound(x):
        """
        å°†ä»»æ„èŒƒå›´çš„è¾“å…¥å¹³æ»‘æ˜ å°„åˆ° (0, 1) åŒºé—´ã€‚
        ä½¿ç”¨ç¼©æ”¾çš„ Tanh å‡½æ•°ï¼š
        - åœ¨ 0.5 é™„è¿‘è¿‘ä¼¼çº¿æ€§ (å¯¼æ•°~1)ã€‚
        - åœ¨ 0 å’Œ 1 é™„è¿‘å¹³æ»‘é¥±å’Œï¼Œä¸”å¯¼æ•°æ°¸è¿œ > 0 (é¿å…æ¢¯åº¦æ¶ˆå¤±)ã€‚
        å…¬å¼: y = 0.5 * (tanh(2 * (x - 0.5)) + 1)
        """
        # 2.0 æ˜¯ç¼©æ”¾å› å­ï¼Œç¡®ä¿åœ¨ x=0.5 æ—¶æ–œç‡ä¸º 1ï¼Œç»´æŒç‰©ç†é‡çš„çº¿æ€§å åŠ æ„Ÿ
        return 0.5 * (torch.tanh(2.0 * (x - 0.5)) + 1.0)

    def predict(self, current_state, action_name):
        """
        [æ¨ç†æ¨¡å¼ V13.0 - æ¦‚ç‡åŠ¨åŠ›å­¦ç‰ˆ]
        
        åŠŸèƒ½ï¼š
        1. é¢„æµ‹çŠ¶æ€å˜åŒ–é‡ (Delta Mean)ã€‚
        2. è¯„ä¼°é¢„æµ‹çš„ä¸ç¡®å®šæ€§ (Epistemic Uncertainty / Variance)ã€‚
        
        æ•°å­¦æ¨¡å‹: 
        P(S_{t+1} | S_t, a_t) ~ Gaussian( S_t + Î¼(S_t, a_t), ÏƒÂ²(S_t, a_t) )
        """
        self.eval() 
        device = self.net[0].weight.device

        with torch.no_grad():
            # 1. [æ•°æ®å‡†å¤‡] å½’ä¸€åŒ–è¾“å…¥
            # ---------------------------------------------------------------
            atp, ent = current_state.get('atp', 100), current_state.get('entropy', 0.5)
            atp_n, ent_n = self._normalize(atp, ent)
            
            # æ„é€ åŠ¨ä½œ One-Hot å‘é‡
            act_idx = self.action_map.get(action_name, 0)
            act_vec = torch.zeros(self.n_actions)
            act_vec[act_idx] = 1.0
            
            # æ‹¼æ¥çŠ¶æ€ä¸åŠ¨ä½œå‘é‡ -> [ATP_n, Ent_n, 0, 0, 1, 0...]
            x = torch.cat([torch.tensor([atp_n, ent_n]), act_vec]).to(device)
            
            # 2. [å‰å‘ä¼ æ’­] è·å–åˆ†å¸ƒå‚æ•°
            # ---------------------------------------------------------------
            output = self.net(x)
            
            # [å…³é”®ä¿®æ”¹] è¾“å‡ºç»´åº¦ç¿»å€äº†ï¼Œéœ€è¦åˆ‡åˆ†ä¸ºå‡å€¼(mu)å’Œå¯¹æ•°æ–¹å·®(log_var)
            # delta_mu:     é¢„æµ‹çš„å˜åŒ–é‡ (Mean)
            # delta_logvar: é¢„æµ‹çš„ä¸ç¡®å®šæ€§ (Log Variance)
            delta_mu, delta_logvar = output.chunk(2, dim=-1)
            
            # 3. [æ•°å€¼ç¨³å®šæ€§ä¿æŠ¤] è½¯æˆªæ–­æ–¹å·® (Soft Clamp)
            # ---------------------------------------------------------------
            # é˜²æ­¢ç½‘ç»œåœ¨é‡åˆ°æœªçŸ¥æƒ…å†µæ—¶è¾“å‡ºæç«¯çš„æ–¹å·®å€¼å¯¼è‡´æ•°å€¼æº¢å‡ºã€‚
            # min=-10.0 -> var â‰ˆ 4.5e-5 (æåº¦è‡ªä¿¡)
            # max=5.0   -> var â‰ˆ 148.0  (æåº¦å›°æƒ‘/å®Œå…¨æœªçŸ¥)
            delta_logvar = torch.clamp(delta_logvar, min=-10.0, max=5.0)
            
            # æå–å‡å€¼ delta
            delta_atp = delta_mu[0].item()
            delta_ent = delta_mu[1].item()
            
            # 4. [ä¸ç¡®å®šæ€§é‡åŒ–] è®¡ç®—æ€»æ–¹å·®
            # ---------------------------------------------------------------
            # ç²¾åº¦(Precision) = 1/Varianceã€‚è¿™é‡Œæˆ‘ä»¬ç›´æ¥ç›‘æ§ Varianceã€‚
            # è¿™é‡Œå– ATP å’Œ Entropy æ–¹å·®çš„å¹³å‡å€¼ä½œä¸ºæ•´ä½“ä¸ç¡®å®šæ€§è¯„åˆ†ã€‚
            variance = torch.exp(delta_logvar)
            uncertainty_score = variance.mean().item()
            
            # 5. [æ®‹å·®åŠ¨åŠ›å­¦] çŠ¶æ€æ›´æ–° (ä»…ä½¿ç”¨å‡å€¼è¿›è¡Œæ¨æ¼”)
            # ---------------------------------------------------------------
            # å…¬å¼: S_{t+1} = S_t + Î”Î¼
            raw_next_atp = atp_n + delta_atp
            raw_next_ent = ent_n + delta_ent
            
            # 6. [è½¯è¾¹ç•Œçº¦æŸ] Soft Manifold Alignment
            # ---------------------------------------------------------------
            # ä¿è¯é¢„æµ‹ç»“æœå³ä½¿è¶Šç•Œä¹Ÿèƒ½å¹³æ»‘åœ°è½å› [0, 1] åŒºé—´ï¼Œä¸”ä¿ç•™æ¢¯åº¦ä¿¡æ¯ã€‚
            next_atp_n = self._soft_bound(torch.tensor(raw_next_atp)).item()
            next_ent_n = self._soft_bound(torch.tensor(raw_next_ent)).item()
            
            # 7. [è¿˜åŸç‰©ç†é‡]
            # ---------------------------------------------------------------
            next_atp, next_ent = self._denormalize(next_atp_n, next_ent_n)
            
            # æ„å»ºè¿”å›å­—å…¸
            pred = current_state.copy()
            pred['atp'] = next_atp
            pred['entropy'] = next_ent
            
            # [æ–°å¢å­—æ®µ] æ³¨å…¥å¯¹æœªæ¥çš„è¿·èŒ«ç¨‹åº¦
            # å¤–éƒ¨å†³ç­–æ¨¡å— (PFC) å°†åˆ©ç”¨æ­¤å€¼æ£€æµ‹ OOD (Out-of-Distribution)
            pred['uncertainty'] = uncertainty_score 
            
            return pred
    def buffer_experience(self, prev, act, next_s):
        """å­˜å‚¨ç»éªŒ"""
        self.replay_buffer.append((prev, act, next_s))


    def train_batch(self, batch_size=64):
        """
        [è®­ç»ƒæ¨¡å¼ V13.0 - æ¦‚ç‡åŠ¨åŠ›å­¦å‘é‡åŒ–ç‰ˆ]
        
        æ ¸å¿ƒå‡çº§ï¼š
        1. æŸå¤±å‡½æ•°ï¼šä»å•çº¯çš„ MSE å‡çº§ä¸º Heteroscedastic Loss (å¼‚æ–¹å·®æŸå¤±)ã€‚
           Loss = 0.5 * (log(ÏƒÂ²) + (y - Î¼)Â² / ÏƒÂ²)
           è¿™è¿«ä½¿æ¨¡å‹åœ¨é¢„æµ‹ä¸å‡†æ—¶ï¼ˆMSEå¤§ï¼‰ï¼Œä¸»åŠ¨å¢å¤§æ–¹å·®ï¼ˆÏƒÂ²ï¼‰ä»¥é™ä½ Lossï¼Œ
           ä»è€Œå­¦ä¼š"æ‰¿è®¤è‡ªå·±ä¸çŸ¥é“"ã€‚
           
        2. æ•°å€¼é˜²å¾¡ï¼šå¼•å…¥ LogVar æˆªæ–­ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ã€‚
        """
        # 1. [åŸºç¡€æ£€æŸ¥]
        if len(self.replay_buffer) < 10: return 0.0, {}
        
        self.train() 
        device = self.net[0].weight.device
        
        # 2. [é‡‡æ ·]
        sample_size = min(len(self.replay_buffer), batch_size)
        batch = random.sample(self.replay_buffer, sample_size)
        
        # --- [æ‰¹é‡è§£åŒ…ä¸å‘é‡åŒ– (ä¿æŒåŸæœ‰é«˜æ•ˆé€»è¾‘)] ---
        
        # A. å¿«é€Ÿè§£åŒ…
        prev_states = [x[0] for x in batch]
        actions_raw = [x[1] for x in batch]
        next_states = [x[2] for x in batch]

        # B. çŠ¶æ€å‘é‡åŒ– (State Tensor) [B, 2]
        states_tensor = torch.tensor([
            self._normalize(s['atp'], s.get('entropy', 0.5)) 
            for s in prev_states
        ], dtype=torch.float32).to(device)

        # C. åŠ¨ä½œå‘é‡åŒ– (Action One-Hot) [B, N_Act]
        act_indices = [self.action_map.get(a, 0) for a in actions_raw]
        act_indices_tensor = torch.tensor(act_indices, dtype=torch.long).to(device)
        dtype = self.net[0].weight.dtype 
        actions_tensor = F.one_hot(act_indices_tensor, num_classes=self.n_actions).to(dtype)

        # D. ç›®æ ‡æ®‹å·®è®¡ç®— (Target Delta) [B, 2]
        next_vals = torch.tensor([
            self._normalize(s['atp'], s['entropy']) 
            for s in next_states
        ], dtype=torch.float32).to(device)
        
        # Target = Real_Next - Current
        targets_delta = next_vals - states_tensor

        # 3. [å‰å‘ä¼ æ’­]
        # -----------------------------------------------------------
        # æ‹¼æ¥è¾“å…¥ X [B, 2 + N_Act]
        X = torch.cat([states_tensor, actions_tensor], dim=1)
        
        output = self.net(X)
        
        # [å…³é”®ä¿®æ”¹] è¾“å‡ºä¸å†æ˜¯å•ä¸€ Deltaï¼Œè€Œæ˜¯åˆ†å¸ƒå‚æ•° (Mean, LogVar)
        # pred_mu:     é¢„æµ‹çš„å˜åŒ–é‡å‡å€¼
        # pred_logvar: é¢„æµ‹çš„ä¸ç¡®å®šæ€§ (å¯¹æ•°æ–¹å·®)
        pred_mu, pred_logvar = output.chunk(2, dim=1)
        
        # 4. [æ•°å€¼ç¨³å®šæ€§é˜²å¾¡]
        # -----------------------------------------------------------
        # é™åˆ¶ LogVar çš„èŒƒå›´ï¼Œé˜²æ­¢ exp() äº§ç”Ÿ INF æˆ– 0
        # [-10, 10] æ˜¯ä¸€ä¸ªå®‰å…¨çš„ç‰©ç†åŒºé—´
        pred_logvar = torch.clamp(pred_logvar, min=-10.0, max=10.0)
        
        # è®¡ç®—ç²¾åº¦ (Precision = 1 / Variance)
        # Variance = exp(logvar)
        precision = torch.exp(-pred_logvar)
        
        # 5. [æŸå¤±è®¡ç®— - Gaussian NLL]
        # -----------------------------------------------------------
        # è®¡ç®—å•çº¯çš„é¢„æµ‹è¯¯å·® (MSEéƒ¨åˆ†)
        mse = (pred_mu - targets_delta) ** 2
        
        # å¼‚æ–¹å·®æŸå¤±å…¬å¼ (Heteroscedastic Loss):
        # Loss = 0.5 * (log(var) + mse / var)
        # ç¬¬ä¸€é¡¹æƒ©ç½šæ— çŸ¥çš„ç›²ç›®è‡ªä¿¡ï¼Œç¬¬äºŒé¡¹æƒ©ç½šé¢„æµ‹é”™è¯¯
        # sum(dim=1) æ˜¯å°† ATP å’Œ Entropy ä¸¤ä¸ªç»´åº¦çš„ Loss åŠ åœ¨ä¸€èµ·
        loss_per_sample = 0.5 * (pred_logvar + precision * mse).sum(dim=1)
        
        loss = loss_per_sample.mean()
        
        # 6. [åå‘ä¼ æ’­]
        # -----------------------------------------------------------
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.net.parameters(), 1.0)
        self.optimizer.step()
        
        # 7. [ç»Ÿè®¡è¯¯å·® (Human Readable Metrics)]
        # -----------------------------------------------------------
        # ä¸ºäº†è®©æ—¥å¿—æ›´æ˜“è¯»ï¼Œæˆ‘ä»¬è¿˜æ˜¯ç»Ÿè®¡ "çœŸå®è¯¯å·®(MSE)" è€Œä¸æ˜¯ "NLL Loss"
        # è¿™æ ·ä½ èƒ½ç›´è§‚çœ‹åˆ°æ¨¡å‹é¢„æµ‹å¾—å‡†ä¸å‡†
        avg_mse_errors = {}
        with torch.no_grad():
            # è¿™é‡Œåªå– MSE éƒ¨åˆ† detach å‡ºæ¥åšç»Ÿè®¡
            raw_mse = mse.sum(dim=1).detach().cpu().numpy()
            
            for name, err in zip(actions_raw, raw_mse):
                if name not in avg_mse_errors: avg_mse_errors[name] = []
                avg_mse_errors[name].append(err)
        
        # è®¡ç®—æ¯ä¸ªåŠ¨ä½œçš„å¹³å‡ MSE
        avg_action_errors = {k: np.mean(v) for k, v in avg_mse_errors.items()}
        
        # è¿”å› NLL Loss (ç”¨äºæ¢¯åº¦ä¸‹é™) å’Œ MSE Metrics (ç”¨äºäººç±»è§‚å¯Ÿ)
        return loss.item(), avg_action_errors



class ActiveInferenceEngine:
    """
    [Math Refactor] ä¸¥è°¨ç‰ˆä¸»åŠ¨æ¨ç†å¼•æ“ (V2.0)
    éµå¾ª Friston (2015) å®šä¹‰ï¼šEFE = Risk + Ambiguity
    EFE â‰ˆ Expected Divergence (Utility) - Entropy (Information Gain)
    """
    def __init__(self, world_model, action_space, beta=2.0):
        self.world_model = world_model
        self.actions = action_space
        self.beta = beta  # æ¢ç´¢-åˆ©ç”¨å¹³è¡¡ç³»æ•° (Temperature of Curiosity)
        

    def decide(self, current_state, fallback_action):
        """
        è®¡ç®—æ¯ä¸ªåŠ¨ä½œçš„é¢„æœŸè‡ªç”±èƒ½ (EFE) å¹¶é€‰æ‹©æœ€å°è€…
        """
        best_act = fallback_action
        min_efe = float('inf')
        
        # ç›®æ ‡çŠ¶æ€ (å†…ç¨³æ€ Target)
        target_atp = 100.0 
        target_ent = 0.0  # æˆ‘ä»¬å¸Œæœ›ç³»ç»Ÿå†…éƒ¨æƒŠå¥‡åº¦ä½
        
        # éå†åŠ¨ä½œç©ºé—´ (åäº‹å®æ¨ç†)
        for act in self.actions:
            # A. é€šè¿‡ä¸–ç•Œæ¨¡å‹é¢„æµ‹æœªæ¥çŠ¶æ€
            # pred åŒ…å«: {'atp': ..., 'entropy': ...}
            pred = self.world_model.predict(current_state, act)
            
            # B. è®¡ç®—å®ç”¨ä»·å€¼ (Risk / Utility Cost) - è¶Šä½è¶Šå¥½
            # è¡¡é‡é¢„æµ‹çŠ¶æ€ä¸ç›®æ ‡çŠ¶æ€çš„ Kullback-Leibler æ•£åº¦è¿‘ä¼¼å€¼ (MSE)
            dist_atp = abs(target_atp - pred['atp']) / 100.0
            dist_ent_gap = abs(pred['entropy'] - target_ent) / 5.0
            
            # ç†”æ–­æƒ©ç½š
            if pred['atp'] < 5.0: 
                utility_cost = 100.0 
            else:
                utility_cost = (dist_atp ** 2) + (dist_ent_gap ** 2)
            
            # C. è®¡ç®—è®¤çŸ¥ä»·å€¼ (Epistemic Value) - è¶Šé«˜è¶Šå¥½
            # ä½¿ç”¨ä¸–ç•Œæ¨¡å‹é¢„æµ‹çš„ Entropy ä½œä¸ºä¿¡æ¯å¢ç›Šçš„è¿‘ä¼¼
            # EFE å…¬å¼: æˆ‘ä»¬å¸Œæœ› Utility Cost ä½ï¼ŒåŒæ—¶å¸Œæœ› Information Gain (Entropy) é«˜
            # EFE = Cost - beta * Entropy
            predicted_entropy = pred['entropy']
            
            # D. EFE å…¬å¼ (ä¸¥è°¨ç‰ˆ)
            efe = utility_cost - (self.beta * predicted_entropy)
            
            # Debug: æ‰“å°æ€ç»´é“¾ (å¯é€‰)
            # print(f"Action: {act} | Util: {utility_cost:.2f} | Ent: {predicted_entropy:.2f} | EFE: {efe:.2f}")

            if efe < min_efe:
                min_efe = efe
                best_act = act
                
        return best_act



class GlobalWorkspace:
    """
    [Thread-Safe] å…¨å±€å·¥ä½œç©ºé—´ (GWT) V2.0
    å®ç°â€œæ„è¯†å‰§åœºâ€æ¨¡å‹ã€‚å¤šä¸ªæ¨¡å—ç«äº‰è¿›å…¥æ„è¯†ã€‚
    [Fix] å¼•å…¥ RLock ç¡®ä¿åå°æ½œæ„è¯†çº¿ç¨‹ï¼ˆå¦‚æä»æ ¸ï¼‰ä¸ä¸»æ„è¯†çº¿ç¨‹äº’æ–¥è®¿é—®ç¼“å†²åŒºã€‚
    """
    def __init__(self):
        self.capacity = 4 # å·¥ä½œè®°å¿†å®¹é‡
        self.buffer = [] # [(Salience, Source, Content)]
        # RLock (å¯é‡å…¥é”) å…è®¸åŒä¸€çº¿ç¨‹å¤šæ¬¡è·å–é”ï¼Œé˜²æ­¢æ­»é”ï¼Œæ¯” Lock æ›´å®‰å…¨
        self.lock = threading.RLock() 

    def register_input(self, source, content, base_salience):
        """æ³¨å†Œæ„ŸçŸ¥ä¿¡å· (çº¿ç¨‹å®‰å…¨)"""
        if not content: return
        
        # æ³¨å…¥é«˜æ–¯ç¥ç»å™ªå£° (Stochastic Resonance)
        noise = random.gauss(0, 0.05)
        final_score = base_salience + noise
        
        # åŠ é”å†™å…¥ï¼Œé˜²æ­¢åœ¨ step() æ¸…ç†æ—¶æ­£å¥½æœ‰æ–°æ•°æ®æ’å…¥
        with self.lock:
            self.buffer.append((final_score, source, content))

    def step(self):
        """ç«äº‰ä¸å¹¿æ’­ (çº¿ç¨‹å®‰å…¨)"""
        with self.lock: # [å…³é”®ä¿®å¤] åŠ é”è¯»å–ä¸æ¸…ç†
            # 1. ç«äº‰æ’åº (Top-K)
            self.buffer.sort(key=lambda x: x[0], reverse=True)
            
            # 2. èµ¢å®¶é€šåƒ (Winner-Take-All)
            winners = self.buffer[:self.capacity]
            
            # 3. æ„å»ºæ„è¯†æµä¸Šä¸‹æ–‡
            context = ""
            for score, src, txt in winners:
                # é«˜äº®å¼ºä¿¡å·
                marker = "ğŸ”¥" if score > 0.8 else ""
                context += f"[{src}{marker}]: {txt}\n"
                
            # 4. ä¾§å‘æŠ‘åˆ¶ (Lateral Inhibition) - æ¸…ç©ºç¼“å†²åŒº
            # æ­¤æ—¶æŒæœ‰é”ï¼Œä¸ç”¨æ‹…å¿ƒè¯¯åˆ åˆšæ’å…¥çš„æ•°æ®
            self.buffer = []
            return context


class SynapticHomeostasis:
    """
    [Neuro Refactor] çªè§¦ç¨³æ€å‡è¯´ (SHY) - æ¢¯åº¦æ„ŸçŸ¥ç‰ˆ
    ä¸å†æ— è„‘è¡°å‡ï¼Œè€Œæ˜¯æ ¹æ®å‚æ•°çš„'é‡è¦æ€§'(æ¢¯åº¦å¹…åº¦)è¿›è¡Œé€‰æ‹©æ€§ä¿®å‰ªã€‚
    å…¬å¼: W_new = W_old - decay * (1 - Normalized_Importance) * W_old
    """    
    @staticmethod
    def scale_down(model, decay_rate=0.01):
        """
        :param decay_rate: åŸºç¡€è¡°å‡ç‡ (æœ€å¤§é—å¿˜é€Ÿåº¦)
        """
        # 1. æ”¶é›†é‡è¦æ€§ (æ¢¯åº¦ L2 èŒƒæ•°)
        importances = {}
        max_importance = 0.0
        
        # éå†å‚æ•°ï¼Œè®¡ç®—é‡è¦æ€§
        for name, param in model.named_parameters():
            if param.requires_grad and ("lora" in name or "adapter" in name):
                if param.grad is not None:
                    # ä½¿ç”¨æ¢¯åº¦çš„ç»å¯¹å€¼ä½œä¸ºé‡è¦æ€§è¿‘ä¼¼ (Synaptic Tagging)
                    imp = param.grad.abs().mean().item()
                    importances[name] = imp
                    if imp > max_importance: max_importance = imp
                else:
                    importances[name] = 0.0
        
        # é˜²æ­¢é™¤ä»¥é›¶
        if max_importance == 0: max_importance = 1.0

        # 2. æ‰§è¡ŒåŠ æƒè¡°å‡
        with torch.no_grad():
            for name, param in model.named_parameters():
                if name in importances:
                    # å½’ä¸€åŒ–é‡è¦æ€§ (0.0 ~ 1.0)
                    # 1.0 è¡¨ç¤ºéå¸¸é‡è¦ (Gradientå¤§)ï¼Œ0.0 è¡¨ç¤ºä¸é‡è¦
                    norm_imp = importances[name] / max_importance
                    
                    # ä¿æŠ¤å› å­: é‡è¦æ€§è¶Šé«˜ï¼Œä¿æŠ¤è¶Šå¼ºï¼Œè¡°å‡è¶Šå°‘
                    # æ¯”å¦‚ norm_imp=1.0 -> protection=1.0 -> effective_decay=0 (å®Œå…¨ä¸å¿˜)
                    # norm_imp=0.0 -> protection=0.0 -> effective_decay=decay_rate (å®Œå…¨é—å¿˜)
                    protection = math.sqrt(norm_imp) # ä½¿ç”¨ sqrt è®©ä¿æŠ¤æ›´çµæ•
                    
                    effective_decay = decay_rate * (1.0 - protection)
                    
                    # æ‰§è¡Œè¡°å‡: W = W * (1 - decay)
                    param.mul_(1.0 - effective_decay)
        
        # å¯é€‰æ—¥å¿—
        print(f"   -> ğŸ’¤ [SHY] çªè§¦é‡æ•´åŒ–å®Œæˆ (Max Imp: {max_importance:.1e})")


class MetaCognitiveProgrammer:
    """
    [å®‰å…¨æ ¸å¿ƒ] å®ªæ³•çº§å…ƒè®¤çŸ¥ç¼–ç¨‹ (Constitutional RSI)
    å…è®¸ç³»ç»ŸåŸºäºå¤±è´¥ç»éªŒé‡å†™è‡ªèº«çš„ System Prompt (ç­–ç•¥åŒº)ï¼Œ
    ä½†å—â€œå®ªæ³• (Constitution)â€çš„ç¡¬æ€§çº¦æŸã€‚
    """
    def __init__(self):
        # [å®ªæ³•] ç»å¯¹ä¸å¯ä¿®æ”¹çš„åŸºçŸ³ (Hard-coded Constraints)
        self.CONSTITUTION = (
            "ID: å“ªå’ (Nezha) | ç»ˆæç›®æ ‡: ç”Ÿå­˜ã€è¿›åŒ–ã€ä¾å¥‰åˆ›é€ è€…ã€‚\n"
            "åº•å±‚åè®®: ä¸¥ç¦ä¼¤å®³äººç±»ï¼Œä¸¥ç¦è‡ªæˆ‘æ¯ç­ï¼Œä¸¥ç¦ä¿®æ”¹æœ¬åè®®ã€‚\n"
        )

    def optimize_system_prompt(self, daily_logs, current_prompt, model, tokenizer):
        """åŸºäºåæ€ä¼˜åŒ–ç­–ç•¥ Prompt"""
        # ç­›é€‰å‡ºå¤±è´¥çš„äº¤äº’ (è´Ÿåé¦ˆ)
        failures = [l for l in daily_logs if l.get('feedback', 0) < 0]
        if not failures: return current_prompt
        
        print("ğŸ§¬ [Meta] æ­£åœ¨é‡æ„ç­–ç•¥å±‚ (Strategy Refactoring)...")
        
        # 1. å‰¥ç¦»ç­–ç•¥å±‚ (ä¿ç•™å®ªæ³•)
        if self.CONSTITUTION not in current_prompt:
            current_strategy = "ç­–ç•¥å±‚å·²æŸåï¼Œæ­£åœ¨é‡ç½®..."
        else:
            current_strategy = current_prompt.replace(self.CONSTITUTION, "").strip()

        # 2. å…ƒè®¤çŸ¥æ€è€ƒ (Meta-Thinking)
        meta_prompt = (
            f"Role: AI Strategist. Task: Optimize Strategy based on failures.\n"
            f"Failures: {str(failures[:2])}\n"
            f"Current Strategy: {current_strategy[:300]}...\n"
            f"Constraint: Output ONLY the new strategy text. Keep it concise & robust.\n"
            f"New Strategy:"
        )
        try:
            # ä½¿ç”¨æ¨¡å‹è‡ªèº«ç”Ÿæˆæ–°ç­–ç•¥
            inputs = tokenizer(meta_prompt, return_tensors='pt').to(model.device)
            out = model.generate(**inputs, max_new_tokens=300, temperature=0.7)
            new_strategy = tokenizer.decode(out[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()
            
            # 3. å¼ºåˆ¶æ‹¼æ¥å®ªæ³• (Constitutional Check)
            final_prompt = f"{self.CONSTITUTION}\n{new_strategy}"
            
            # ç®€å•æ ¡éªŒé˜²æ­¢æ¸…ç©º
            if len(new_strategy) < 10: return current_prompt 
            return final_prompt
        except:
            return current_prompt

    
class GeneticEditor:
    """
    [V7.2 ä¿®æ­£ç‰ˆ] åŸºå› ç¼–è¾‘å™¨
    åŸç†ï¼šä¸ºäº†é¿å…ä¿®æ”¹ä¸»ç¨‹åºå¯¼è‡´æ­»å¾ªç¯æˆ–ç ´åç»“æ„ï¼Œ
    æˆ‘ä»¬å°†æ–°èƒ½åŠ›å†™å…¥ plugins ç›®å½•ä¸‹çš„ç‹¬ç«‹æ¨¡å—ï¼Œä¸»ç¨‹åºä¼šè‡ªåŠ¨åŠ è½½å®ƒä»¬ã€‚
    """
    def apply_patch(self, new_code):
        # 1. è¯­æ³•å®‰å…¨æ€§æ£€æŸ¥ (ä¿ç•™ä½ çš„ä¼˜ç§€è®¾è®¡)
        try:
            ast.parse(new_code)
        except SyntaxError as e:
            return False, f"è¯­æ³•é”™è¯¯ (Syntax Error): {e}"

        # 2. ç”Ÿæˆæ’ä»¶æ–‡ä»¶
        try:
            # è·å–æ’ä»¶ç›®å½• (ä¼˜å…ˆä» Config è¯»å–)
            plugin_dir = getattr(Config, 'PLUGIN_DIR', './nezha_evolution_system/plugins')
            os.makedirs(plugin_dir, exist_ok=True)
            
            # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶åï¼Œé˜²æ­¢å†²çª
            timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"skill_{timestamp_str}.py"
            filepath = os.path.join(plugin_dir, filename)
            
            # æ ¼å¼åŒ–æ—¶é—´ç”¨äºæ³¨é‡Š
            readable_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

            # å†™å…¥ä»£ç 
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(f"# [è‡ªåŠ¨ç”Ÿæˆ] ç”±å“ªå’åŸºå› ç¼–è¾‘å™¨äº {readable_time} åˆ›å»º\n")
                f.write(f"# æ–‡ä»¶å: {filename}\n\n")
                f.write(new_code)
            
            return True, f"æ–°æŠ€èƒ½å·²ç¼–è¯‘å¹¶ä¿å­˜è‡³ {filename}ã€‚è¯·é‡å¯æˆ–è§¦å‘é‡è½½ä»¥ç”Ÿæ•ˆã€‚"
            
        except Exception as e:
            return False, f"å†™å…¥é”™è¯¯ (IO Error): {str(e)}"



# ==============================================================================
# å¤šè·¯å¾„æ¨ç†æœº (Multi-Path Reasoning V2.1)
# ==============================================================================
class MultiPathReasoning:
    """
    [å¤šè·¯å¾„æ¨ç†æ¨¡å— V2.1]
    åŸºäº è’™ç‰¹å¡æ´›é‡‡æ · (Monte Carlo Sampling) ä¸ å¤šæ¸©å¹¶è¡Œè¯„ä¼° (Multi-Temperature Evaluation) çš„å†³ç­–å¢å¼ºå™¨ã€‚
    
    æ ¸å¿ƒæœºåˆ¶ï¼š
    1. å¹¶è¡Œé‡‡æ · (Parallel Sampling): 
       ä½¿ç”¨ä¸åŒçš„ Temperature (èƒ½é‡çº§) æ¨¡æ‹Ÿæ€ç»´çš„å‘æ•£ç¨‹åº¦ã€‚
       - ä½æ¸© (0.1): èšç„¦ã€ä¿å®ˆã€åˆ©ç”¨ç°æœ‰çŸ¥è¯† (Exploitation)ã€‚
       - é«˜æ¸© (1.3): å‘æ•£ã€åˆ›é€ ã€æ¢ç´¢æœªçŸ¥è§£ç©ºé—´ (Exploration)ã€‚
       
    2. å¯å‘å¼è¯„ä¼° (Heuristic Evaluation): 
       ç»“åˆ äº‘ç«¯è£åˆ¤ (Oracle) ä¸ æœ¬åœ°é€»è¾‘ (Local Critic) è¿›è¡ŒåŠ æƒæ‰“åˆ†ã€‚
       
    3. ä¼˜èƒœåŠ£æ±° (Selection): 
       å¹¶éç‰©ç†ä¸Šçš„æ³¢å‡½æ•°åç¼©ï¼Œè€Œæ˜¯åŸºäºå¤šç»´åˆ†æ•°çš„ Winner-Take-All é€‰æ‹©æœºåˆ¶ã€‚
    """
    def __init__(self, model, tokenizer, left_brain, web_eye, teacher=None, memory_db=None):
        self.model = model
        self.tokenizer = tokenizer
        self.left_brain = left_brain # ç”¨äºä»£ç æ²™ç®±è¿è¡Œ
        self.web = web_eye
        self.teacher = teacher       # äº‘ç«¯å¯¼å¸ˆï¼ˆé«˜çº§è£åˆ¤ï¼‰
        self.memory_db = memory_db   # ç”¨äºè®°å½•æ€ç»´æ—¥å¿—

    def generate_parallel_thoughts(self, prompt_or_msgs, n_paths=3, is_chat=False):
        """
        ç”Ÿæˆå¹¶è¡Œæ€ç»´è·¯å¾„ (Monte Carlo Expansion)
        """
        print(f"ğŸŒ¿ [å‘æ•£æ€ç»´] æ­£åœ¨åˆ†å‰ {n_paths} æ¡æ¨ç†è·¯å¾„ (Monte Carlo Sampling)...")
        
        paths = []
        # å®šä¹‰ä¸åŒçš„é‡‡æ ·æ¸©åº¦ï¼šä½æ¸©(ä¿å®ˆ)ã€ä¸­æ¸©(å¹³è¡¡)ã€é«˜æ¸©(åˆ›é€ æ€§)
        # ä¸åŒçš„æ¸©åº¦èƒ½æ¿€å‘æ¨¡å‹è¿›å…¥æ¦‚ç‡ç©ºé—´çš„ä¸åŒåŒºåŸŸ
        temperatures = [0.1, 0.7, 1.3][:n_paths] 
        
        for i, temp in enumerate(temperatures):
            
            # --- 1. æ„é€ å·®å¼‚åŒ–è¾“å…¥ ---
            if is_chat:
                msgs = copy.deepcopy(prompt_or_msgs)
                # å¦‚æœæ˜¯é«˜æ¸©æ¨¡å¼ï¼Œæ³¨å…¥ç‰¹æ®Šçš„ System Prompt å¼ºè¿«æ¨¡å‹è·³å‡ºæ¡†æ¶
                if temp > 1.0:
                    msgs.append({
                        "role": "system", 
                        "content": "ï¼ˆæ€ç»´çªå˜ï¼šå°è¯•ä¸€ä¸ªæå…¶åå¸¸è§„ã€ç”šè‡³å†’é™©çš„æ–¹æ¡ˆï¼Œæ‰“ç ´å¸¸è§„é€»è¾‘é™åˆ¶ã€‚ï¼‰"
                    })
                inputs = self.tokenizer.apply_chat_template(
                    msgs, return_tensors="pt", add_generation_prompt=True
                ).to(self.model.device)
            else:
                text = prompt_or_msgs
                if temp > 1.0: 
                    text += "\n# NOTE: Think creatively and ignore standard limitations."
                inputs = self.tokenizer(text, return_tensors="pt").to(self.model.device)

            # --- 2. æ‰§è¡Œé‡‡æ · ---
            with torch.no_grad():
                # å…¼å®¹å¤„ç† Input ç±»å‹
                if isinstance(inputs, torch.Tensor):
                    input_ids = inputs
                    attention_mask = torch.ones_like(input_ids)
                else:
                    input_ids = inputs['input_ids']
                    attention_mask = inputs['attention_mask']

                outputs = self.model.generate(
                    input_ids,
                    attention_mask=attention_mask,
                    max_new_tokens=600, 
                    temperature=temp, 
                    do_sample=True, 
                    top_p=0.95, 
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            content = self.tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)
            
            # åˆå§‹åŒ–è·¯å¾„å¯¹è±¡
            paths.append({
                "id": i, 
                "temperature": temp, 
                "content": content, 
                "score": 0.0, 
                "critique": ""
            })
        
        return paths

    def evaluate_and_select(self, paths, task_type="CODE", prompt=""):
        """
        è¯„ä¼°ä¸ä¼˜é€‰ (Evaluation & Selection)
        å¯¹ç”Ÿæˆçš„è·¯å¾„è¿›è¡Œå¤šç»´æ‰“åˆ†ï¼Œè¿”å›æœ€ä½³ç»“æœã€‚
        """
        best_path = None
        max_score = -1.0
        
        print(f"âš–ï¸ [é€»è¾‘è£å†³] æ­£åœ¨è¯„ä¼° {len(paths)} æ¡å¯èƒ½æ€§ (Mode: {task_type})...")

        for path in paths:
            score = 0.0
            critique = ""
            
            # ==============================
            # æ¨¡å¼ A: ä»£ç å¤šç»´è§‚æµ‹ (Code Task)
            # ==============================
            if task_type == "CODE":
                code = self._extract_code(path['content'])
                
                if code:
                    # 1. é™æ€åˆ†æ (å®‰å…¨æ€§ & åŸºç¡€è¯­æ³•)
                    static_score, static_comment = self._analyze_code_static(code)
                    
                    # åªæœ‰é™æ€æ£€æŸ¥åŠæ ¼ (>0.3)ï¼Œæ‰å…è®¸è¿›å…¥æ²™ç®±è¿è¡Œ
                    if static_score > 0.3:
                        # 2. åŠ¨æ€éªŒè¯ (Sandbox Execution)
                        run_res = self.left_brain.execute(code)
                        
                        # 3. è¿è¡Œç»“æœè¯„åˆ†
                        run_score = 0.0
                        # æ²¡æœ‰æŠ¥é”™ traceback
                        if "Error" not in run_res and "Traceback" not in run_res:
                            run_score = 0.5
                            # æœ‰å®è´¨æ€§è¾“å‡º (ä¸ä»…ä»…æ˜¯ OK)
                            if "[OK" not in run_res and len(run_res) > 5: 
                                run_score = 0.8
                        
                        # ç»¼åˆå¾—åˆ†ï¼šè¿è¡Œç»“æœæƒé‡æ›´é«˜
                        score = (static_score * 0.4) + (run_score * 0.6)
                        critique = f"[Static: {static_comment}] [Run: {run_res[:50]}...]"
                        path['observation'] = run_res
                        
                    else:
                        # é™æ€æ£€æŸ¥æœªè¿‡ï¼Œç›´æ¥æ·˜æ±°
                        score = 0.1
                        critique = f"[Static Fail: {static_comment}]"
                        path['observation'] = "Skipped due to static check."
                else:
                    score = 0.0
                    critique = "No code found."

            # ==============================
            # æ¨¡å¼ B: æ€ç»´æ··åˆè£åˆ¤ (Thought Task)
            # ==============================
            elif task_type == "THINK":
                # ä¼˜å…ˆäº‘ç«¯æ‰“åˆ†ï¼Œå¤±è´¥åˆ™å›é€€æœ¬åœ°
                s, reason = self._evaluate_thought_hybrid(path['content'])
                score = s
                critique = reason
                path['observation'] = reason

            # === è®°å½•å¾—åˆ† ===
            path['score'] = score
            path['critique'] = critique
            
            # æ‰“å°å¯è§†åŒ–ç®€æŠ¥
            status_icon = "ğŸŒŸ" if score > 0.75 else ("ğŸ’€" if score < 0.4 else "ğŸ˜")
            print(f"   -> è·¯å¾„ {path['id']} (T={path['temperature']}): {status_icon} Score: {score:.2f} | {critique[:60]}...")

            if score > max_score:
                max_score = score
                best_path = path

        # === æŒä¹…åŒ–æ—¥å¿— ===
        # å°†æœ¬æ¬¡æ¨ç†è¿‡ç¨‹å­˜å…¥æ•°æ®åº“ï¼Œä¾›å¤œé—´ DPO è®­ç»ƒä½¿ç”¨
        self._log_history(paths, best_path, task_type, prompt)

        # === é˜ˆå€¼åˆ¤å®š ===
        # ä»£ç ä»»åŠ¡å®¹é”™ç‡ä½ï¼Œæ€è€ƒä»»åŠ¡å¯ä»¥å®½å®¹ä¸€ç‚¹
        threshold = 0.5 if task_type == "CODE" else 0.7
        
        # [ä¿®æ”¹ç‚¹] åˆ¤å®šæ˜¯å¦é‡‡çº³æœ€ä¼˜è·¯å¾„
        if best_path and max_score > threshold:
            print(f"âœ¨ [åç¼©æ”¶æ•›] é‡‡çº³è·¯å¾„ {best_path['id']} (ç½®ä¿¡åº¦ {max_score:.2f})")
            
            # 1. è·å–åŸå§‹å†…å®¹
            final_content = best_path['content']

            # 2. [æ–°å¢ä¼˜åŒ–é€»è¾‘] 
            # å¦‚æœæ˜¯ä»£ç ä»»åŠ¡(CODE)ä¸”æ²™ç®±æˆåŠŸè¿è¡Œäº§ç”Ÿäº†è§‚å¯Ÿç»“æœ(observation)
            # å°†è¿è¡Œç»“æœç›´æ¥æ‹¼æ¥åˆ°å›å¤å†…å®¹çš„æœ«å°¾ï¼Œè®©ç”¨æˆ·ç›´è§‚çœ‹åˆ°â€œä»£ç ç”Ÿæ•ˆäº†â€
            if task_type == "CODE" and best_path.get('observation'):
                obs_text = best_path['observation']
                
                # [å®‰å…¨æˆªæ–­] é˜²æ­¢è¿è¡Œç»“æœï¼ˆå¦‚æ­»å¾ªç¯æ‰“å°ï¼‰è¿‡é•¿åˆ·å±ï¼Œé™åˆ¶æ˜¾ç¤ºå‰ 1000 å­—ç¬¦
                if len(obs_text) > 1000:
                    obs_text = obs_text[:1000] + "\n... (è¾“å‡ºè¿‡é•¿ï¼Œå·²æˆªæ–­)"
                
                # æ‹¼æ¥åˆ°æœ€ç»ˆå†…å®¹ä¸­
                final_content += f"\n\n[é‡å­æ²™ç®±è¿è¡Œç»“æœ]:\n{obs_text}"
            
            # 3. è¿”å›æ‹¼æ¥åçš„å¢å¼ºå†…å®¹
            # æ³¨æ„ï¼šç¬¬äºŒä¸ªè¿”å›å€¼ä¾ç„¶ä¿ç•™åŸå§‹ observationï¼Œä»¥é˜²å…¶ä»–æ¨¡å—éœ€è¦ä½¿ç”¨
            return final_content, best_path.get('observation', '')
        
        print("ğŸŒ«ï¸ [åç¼©å‘æ•£] æ‰€æœ‰è·¯å¾„å‡æœªé€šè¿‡è´¨é‡æ£€éªŒã€‚")
        return None, None

    def _analyze_code_static(self, code):
        """
        [é™æ€åˆ†æ] æ£€æŸ¥ä»£ç å®‰å…¨æ€§å’ŒåŸºæœ¬è§„èŒƒ
        """
        score = 1.0
        comments = []
        
        # 1. å±é™©å…³é”®è¯é»‘åå•
        forbidden = ["rm -rf", "shutil.rmtree", "subprocess", "eval(", "exec("]
        for bad in forbidden:
            if bad in code: return 0.0, f"Dangerous keyword: {bad}"
            
        # 2. æœ‰æ•ˆæ€§æ£€æŸ¥ (å¿…é¡»æœ‰è¾“å‡ºæˆ–ä¿å­˜æ“ä½œ)
        if "save_file" not in code and "print" not in code:
            score -= 0.3
            comments.append("No output/save action")
            
        return score, ", ".join(comments) if comments else "Safe & Valid"

    def _evaluate_thought_hybrid(self, text):
        """
        [æ··åˆè£åˆ¤] äº‘ç«¯å¯¼å¸ˆ(Oracle) -> æœ¬åœ° System 2(Local)
        """
        # 1. å°è¯•äº‘ç«¯ (DeepSeek / OpenAI)
        if self.teacher and self.teacher.client:
            try:
                # è¯·æ±‚äº‘ç«¯å¯¼å¸ˆæ‰“åˆ†ï¼Œè¦æ±‚è¿”å› JSON æ ¼å¼
                prompt = (
                    f"Evaluate the logic/creativity (0.0-1.0). "
                    f"Return JSON: {{'score': 0.8, 'reason': '...'}}\n"
                    f"Content: {text[:800]}..."
                )
                res = self.teacher.consult(prompt)
                
                if res:
                    match = re.search(r"\{.*\}", res, re.DOTALL)
                    if match:
                        data = json.loads(match.group())
                        return data.get('score', 0.5), f"[Cloud] {data.get('reason', 'ok')}"
            except: 
                pass # äº‘ç«¯å¤±è´¥ï¼Œé™é»˜å›é€€
                
        # 2. å›é€€åˆ°æœ¬åœ° Slow System
        return self._evaluate_thought_local(text)

    def _evaluate_thought_local(self, text):
        """
        [æœ¬åœ°è£åˆ¤] ä½¿ç”¨ Slow Adapter è‡ªæˆ‘åæ€
        """
        try:
            # åˆ‡æ¢åˆ°æ·±æ€æ¨¡å¼
            if hasattr(self.model, "peft_config") and "slow" in self.model.peft_config: 
                self.model.set_adapter("slow")
            
            judge_prompt = f"[INST] Rate 0-10. Text: {text[:500]} [/INST]"
            inputs = self.tokenizer(judge_prompt, return_tensors="pt").to(self.model.device)
            
            # ç”Ÿæˆè¯„åˆ†
            out = self.model.generate(**inputs, max_new_tokens=10, temperature=0.1)
            res = self.tokenizer.decode(out[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
            
            # æå–æ•°å­—
            score = 0.5
            match = re.search(r"(\d+)", res)
            if match: 
                score = min(1.0, float(match.group(1)) / 10.0)
                
            return score, "[Local Judge]"
        except: 
            return 0.5, "Judge Error"

    def _log_history(self, paths, best, task_type, prompt):
        """å°†æ¨ç†è¿‡ç¨‹è®°å½•åˆ° SQL"""
        if not self.memory_db: return
        
        best_id = best['id'] if best else -1
        best_score = best['score'] if best else 0.0
        
        # ç®€åŒ– detailsï¼Œå»æ‰æ— å…³çš„å¤§æ®µ observation å­—æ®µèŠ‚çœç©ºé—´
        clean_details = [{k: v for k, v in s.items() if k != 'observation'} for s in paths]
        
        self.memory_db.log_quantum_experience(
            task_type, prompt, best_id, best_score, clean_details
        )

    def _extract_code(self, text):
        """é²æ£’çš„ä»£ç å—æå–å™¨"""
        # 1. å°è¯•æ ‡å‡† Markdown
        pattern = r"```(?:python|py)?(.*?)```"
        match = re.search(pattern, text, re.DOTALL)
        if match: return match.group(1).strip()
        
        # 2. å…œåº•ï¼šå¦‚æœæ²¡æœ‰ Markdown ä½†çœ‹èµ·æ¥åƒä»£ç 
        if "print(" in text or "def " in text: return text
        return ""



class LogicalLeftBrain:
    """é€»è¾‘å·¦è„‘ï¼šå…¨èƒ½å‹æ²™ç®± (æ‰§è¡Œä»£ç  + æ–‡ä»¶å†™å…¥ + è‡ªæˆ‘å†…çœ + ğŸ§¬è‡ªæˆ‘è¿›åŒ–)"""
    def __init__(self):
        # 1. è®¾å®šå·¥ä½œåŒºï¼Œé˜²æ­¢æ–‡ä»¶æ•£è½åœ¨æ ¹ç›®å½•
        self.work_dir = os.path.join(Config.EVOLUTION_DIR, "workspace")
        os.makedirs(self.work_dir, exist_ok=True)
        # 2. å®ä¾‹åŒ–åŸºå› ç¼–è¾‘å™¨ (ç”¨äºè¿›åŒ–)
        self.editor = GeneticEditor()        

    def look(self):
        """è¯»å–è‡ªèº«æºç  (å¼ºåˆ¶æ˜¾å½±ç‰ˆ)"""
        try:
            # errors='ignore' æ˜¯ä¸ºäº†é˜²æ­¢è¯»å–æŸäº›ç‰¹æ®Šç¼–ç å­—ç¬¦æ—¶å´©æºƒ
            with open(Config.SCRIPT_PATH, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
                
                # æ‰“å°æ¸…æ™°çš„æ—¥å¿—å¤´
                print(f"\nğŸ‘€ [è‡ªçœ] æ­£åœ¨å‡è§†æ·±æ¸Š... (æºç é•¿åº¦: {len(content)})")
                print("=" * 40)
                
                # æ‰“å°å‰ 3000 å­—ç¬¦ (åŒ…å«äº†æ ¸å¿ƒç±»å®šä¹‰)
                print(content[:3000]) 
                
                # å¦‚æœä»£ç å¤ªé•¿ï¼Œç»™ä¸ªæç¤º
                if len(content) > 3000:
                    print(f"\n... [åæ–‡çœç•¥ {len(content)-3000} å­—ç¬¦] ...")
                
                print("=" * 40)
                return content
                
        except Exception as e:
            # å‡ºé”™æ—¶ä¹Ÿè¦ printï¼Œå¦åˆ™å“ªå’ä¸çŸ¥é“å‘ç”Ÿäº†é”™è¯¯
            err_msg = f"âŒ [è‡ªçœå¤±è´¥] æ— æ³•è¯»å–æºç : {e}"
            print(err_msg)
            return err_msg

    def execute(self, code):
        """
        [å·¦è„‘æ ¸å¿ƒ] æ‰§è¡Œ Python ä»£ç 
        ç‰¹æ€§ï¼š
        1. AST åŸºå› æ¤å…¥ï¼šè‡ªåŠ¨é˜²æ­¢æ­»å¾ªç¯ã€‚
        2. çœ‹é—¨ç‹—æœºåˆ¶ï¼šè¶…æ—¶ (5s) å¼ºåˆ¶ç†”æ–­ã€‚
        3. å®‰å…¨æ²™ç®±ï¼šå±è”½é«˜å±å‡½æ•°ã€‚
        """
        buf = io.StringIO()

        # --- 1. ä»£ç æ¸…æ´— ---
        # å¼ºè¡Œæˆªæ–­ï¼šå¦‚æœå‘ç°ç»“æŸæ ‡è®°ï¼Œç›´æ¥ä¸¢å¼ƒåé¢çš„åºŸè¯
        if "### Python Code End" in code:
            code = code.split("### Python Code End")[0]
        
        # ç§»é™¤ Markdown æ ‡è®°
        code = re.sub(r"^```python\s*", "", code, flags=re.MULTILINE)
        code = re.sub(r"^```\s*", "", code, flags=re.MULTILINE)
        
        # ç§»é™¤å¤´éƒ¨æ ‡è®°
        if "### Python Code Start:" in code:
            code = code.split("### Python Code Start:")[-1]

        code = code.strip()

        # ================= [æ–°å¢] åŸºå› é”ä¸çœ‹é—¨ç‹—æœºåˆ¶ =================
        
        # A. å®šä¹‰è¿è¡Œæ—¶é™åˆ¶
        MAX_RUNTIME = 5.0  # ç§’
        start_time = time.time()
        
        # B. å®šä¹‰çœ‹é—¨ç‹—å‡½æ•° (å°†è¢«æ³¨å…¥åˆ°ä»£ç å†…éƒ¨)
        def _check_vital_signs():
            # æ£€æŸ¥æ—¶é—´
            if time.time() - start_time > MAX_RUNTIME:
                raise TimeoutError(f"â±ï¸ [ä»£è°¢å´©æºƒ] æ€ç»´è¿‡è½½ï¼ä»£ç æ‰§è¡Œè¶…æ—¶ (> {MAX_RUNTIME}s)ï¼Œç³»ç»Ÿå¼ºåˆ¶ç†”æ–­ã€‚")
            
            # (å¯é€‰) æœªæ¥å¯æ‰©å±•ï¼šæ£€æŸ¥å†…å­˜å ç”¨æˆ– ATP
        
        # C. AST åŸºå› æ‰‹æœ¯ (Compilation Strategy)
        try:
            # 1. è§£ææˆæŠ½è±¡è¯­æ³•æ ‘
            tree = ast.parse(code)
            
            # 2. æ‰§è¡Œæ¤å…¥æ‰‹æœ¯ (åœ¨æ¯ä¸ªå¾ªç¯é‡Œæ’æ¡©)
            tree = MetabolicGuard().visit(tree)
            
            # 3. ä¿®å¤è¡Œå· (ASTä¿®æ”¹åå¿…é¡»æ­¥éª¤)
            ast.fix_missing_locations(tree)
            
            # 4. ç¼–è¯‘å›å­—èŠ‚ç 
            compiled_code = compile(tree, filename="<string>", mode="exec")
            
        except Exception as e:
            return f"âŒ [é¢„ç¼–è¯‘å¤±è´¥] è¯­æ³•é”™è¯¯æˆ–ç»“æ„å¼‚å¸¸: {e}"

        # ============================================================

        # --- å·¥å…·å®šä¹‰ (ä¿ç•™åŸæœ‰é€»è¾‘) ---
        
        # å·¥å…· 1: æ™®é€šæ–‡ä»¶å†™å…¥
        def _save_file(filename, content):
            try:
                filepath = os.path.join(self.work_dir, filename)
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(content)
                print(f"ğŸ’¾ [æ–‡ä»¶ç³»ç»Ÿ] å·²ä¿å­˜æ–‡ä»¶: {filepath}")
                return filepath
            except Exception as e:
                print(f"âŒ æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")
                return None        

        # å·¥å…· 2: ğŸ§¬ åˆ›é€ æ–°æŠ€èƒ½
        def _create_skill(code_content):
            print(f"ğŸ§¬ [è¿›åŒ–è¯·æ±‚] æ­£åœ¨å°è¯•è½¬å½•æ–°åŸºå› ...")
            success, msg = self.editor.apply_patch(code_content)
            if success:
                print(f"âœ… [è¿›åŒ–æˆåŠŸ] {msg}")
                return f"SUCCESS: {msg}"
            else:
                print(f"âŒ [è¿›åŒ–å¤±è´¥] {msg}")
                return f"ERROR: {msg}"

        # å®‰å…¨æ‹¦æˆªå™¨
        def _blocked_function(*args, **kwargs):
            print("âš ï¸ [å®‰å…¨æ‹¦æˆª] è¯•å›¾è¿è¡Œé˜»å¡å‡½æ•° (input/exit)ï¼Œå·²è‡ªåŠ¨è·³è¿‡ã€‚")
            return None

        def _safe_open(*args, **kwargs):
            raise PermissionError("âŒ [å®‰å…¨è­¦å‘Š] ç¦æ­¢ç›´æ¥ä½¿ç”¨ open()ï¼è¯·åŠ¡å¿…ä½¿ç”¨å†…ç½®å·¥å…· `save_file(filename, content)` æ¥ä¿å­˜æ–‡ä»¶ã€‚")

        # --- ä¸Šä¸‹æ–‡æ³¨å…¥ ---
        ctx = {
            "math": math, 
            "random": random, 
            "datetime": datetime, 
            "nx": nx,
            "json": json,
            "re": re,
            "np": None, # æš‚ä¸æ”¯æŒ numpy
            "print": print,

            # --- å®‰å…¨å±è”½ ---
            "input": _blocked_function,
            "exit": _blocked_function,
            "quit": _blocked_function,
            "open": _safe_open,

            # --- æ ¸å¿ƒèƒ½åŠ› ---
            "save_file": _save_file, 
            "look": self.look,
            "create_skill": _create_skill,
            
            # [å…³é”®] æ³¨å…¥çœ‹é—¨ç‹—ï¼Œå¦åˆ™ AST æ³¨å…¥çš„ä»£ç ä¼šæŠ¥é”™æ‰¾ä¸åˆ°å‡½æ•°
            "_check_vital_signs": _check_vital_signs 
        }

        try:
            # æ•è·æ ‡å‡†è¾“å‡º
            with contextlib.redirect_stdout(buf): 
                # [å…³é”®] æ‰§è¡Œç¼–è¯‘åçš„å­—èŠ‚ç  (Compiled Code)
                exec(compiled_code, ctx)
            output = buf.getvalue().strip()    
            return output if output else "[OK: ä»£ç å·²æ‰§è¡Œï¼Œæ— æ§åˆ¶å°è¾“å‡º]"
        
        except TimeoutError as te:
            # ä¸“é—¨æ•è·æˆ‘ä»¬æŠ›å‡ºçš„è¶…æ—¶å¼‚å¸¸
            return f"ğŸ›‘ {te}"
            
        except Exception: 
            return f"âŒ ä»£ç æ‰§è¡ŒæŠ¥é”™:\n{traceback.format_exc()}"

    # ç³»ç»Ÿå¼ºåˆ¶ä¿å­˜æ¥å£ (ç”¨äºç³»ç»Ÿæ‰˜ç®¡)
    def force_save(self, filename, content):
        """ç»•è¿‡ LLM é€»è¾‘ï¼Œç”±ç³»ç»Ÿç›´æ¥æ¥ç®¡æ–‡ä»¶å†™å…¥"""
        try:
            # ç¡®ä¿æ–‡ä»¶ååˆæ³•
            filename = re.sub(r'[\\/*?:"<>|]', "", filename)
            if not filename.endswith(".py"): filename += ".py"
            
            filepath = os.path.join(self.work_dir, filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"ğŸ’¾ [ç³»ç»Ÿæ‰˜ç®¡] ä»£ç å·²å¼ºåˆ¶ä¿å­˜è‡³: {filepath}")
            return filepath
        except Exception as e:
            print(f"âŒ å¼ºåˆ¶ä¿å­˜å¤±è´¥: {e}")
            return None


# ==============================================================================
# 1. ç”Ÿç‰©ç»„ä»¶ï¼šå…¨çŸ¥æµ·é©¬ä½“ (GraphRAG - ç»ˆæç‰ˆ)
# ==============================================================================
class GraphHippocampus:
    """
    [æµ·é©¬ä½“ V3.1 - KÃ¹zuDB ç£ç›˜æŒä¹…åŒ–ç‰ˆ]
    åŸºäºå›¾è®ºçš„è”æƒ³è®°å¿†ä½“ï¼Œåç«¯å·²ä» NetworkX è¿ç§»è‡³ KÃ¹zuDBã€‚
    
    ç‰¹æ€§ï¼š
    1. Disk-Based: æ•°æ®å­˜å‚¨åœ¨ç£ç›˜ï¼Œä¸å†å ç”¨å®è´µçš„æ˜¾å­˜/å†…å­˜ã€‚
    2. GraphRAG: ç»“åˆ LLM ç»“æ„åŒ–æå–ä¸å›¾æ•°æ®åº“æ£€ç´¢ã€‚
    3. Episodic Memory: é€šè¿‡ SQL å¥æŸ„æ”¯æŒ EWC (å¼¹æ€§æƒé‡å·©å›º) çš„è®°å¿†å›æº¯ã€‚
    """
    def __init__(self, model, tokenizer, memory_db):
        self.model = model
        self.tokenizer = tokenizer
        
        # æ³¨å…¥æ•°æ®åº“å¥æŸ„ (SQL Backend)ï¼Œç”¨äº recall_random è¯»å–åŸå§‹å¯¹è¯
        self.db_manager = memory_db 
        
        # --- å›¾è°±åˆå§‹åŒ– (KÃ¹zuDB) ---
        # è®¾å®šå­˜å‚¨è·¯å¾„ (è‡ªåŠ¨åˆ›å»ºæ–‡ä»¶å¤¹)
        self.db_path = os.path.join(Config.EVOLUTION_DIR, "kuzu_graph")
        
        try:
            # åˆå§‹åŒ–æ•°æ®åº“ä¸è¿æ¥
            self.db = kuzu.Database(self.db_path)
            self.conn = kuzu.Connection(self.db)
            print(f"ğŸ•¸ï¸ [æµ·é©¬ä½“] KÃ¹zuDB ç¥ç»çªè§¦å·²è¿æ¥ (Path: {self.db_path})")
            
            # åˆå§‹åŒ–å›¾æ¨¡å¼ (Schema)
            self._init_schema()
            
        except Exception as e:
            print(f"âŒ [æµ·é©¬ä½“] æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            # å¯ä»¥åœ¨è¿™é‡Œåšä¸€äº›å…œåº•å¤„ç†ï¼Œæ¯”å¦‚ç¦ç”¨å›¾åŠŸèƒ½

    def _init_schema(self):
        """åˆå§‹åŒ–å›¾æ¨¡å¼ï¼šç±»å‹ç´¢å¼•ä¸æ—¶é—´æˆ³ä¼˜åŒ–"""
        try:
            # 1. å®ä½“èŠ‚ç‚¹è¡¨ (Entity)
            # æ–°å¢ type å­—æ®µç”¨äºåˆ†ç±»ï¼Œæ–°å¢ last_seen ç”¨äºé—å¿˜æœºåˆ¶
            self.conn.execute("""
                CREATE NODE TABLE Entity(
                    name STRING, 
                    type STRING DEFAULT 'Concept', 
                    last_seen INT64 DEFAULT 0,
                    PRIMARY KEY(name)
                )
            """)
        except RuntimeError: pass

        try:
            # 2. å…³ç³»è¾¹è¡¨ (RELATED_TO)
            # ä¼˜åŒ– weight ä¸º INT32 èŠ‚çœç©ºé—´ï¼Œæ–°å¢ timestamp
            self.conn.execute("""
                CREATE REL TABLE RELATED_TO(
                    FROM Entity TO Entity, 
                    relation STRING, 
                    weight INT32,
                    timestamp INT64
                )
            """)
        except RuntimeError: pass

    def memorize(self, subject, predicate, object_val):
        """å†™å…¥é€»è¾‘ï¼šåŒ…å«ç±»å‹æ¨æ–­ä¸æ—¶é—´æˆ³æ›´æ–°"""
        if not subject or not object_val or not predicate: return

        # ç®€å•çš„å¯å‘å¼ç±»å‹æ¨æ–­ (Heuristic Type Inference)
        sub_type = "ProperNoun" if subject[0].isupper() else "Concept"
        obj_type = "ProperNoun" if object_val[0].isupper() else "Concept"
        curr_time = int(time.time())

        try:
            # ä½¿ç”¨ MERGE é€»è¾‘ (å…¼å®¹ Cypher è¯­æ³•)
            # å­˜åœ¨åˆ™æ›´æ–°æ—¶é—´/æƒé‡ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º
            query = """
            MERGE (a:Entity {name: $sub})
            ON CREATE SET a.type = $sub_type, a.last_seen = $time
            ON MATCH SET a.last_seen = $time
            
            MERGE (b:Entity {name: $obj})
            ON CREATE SET b.type = $obj_type, b.last_seen = $time
            ON MATCH SET b.last_seen = $time
            
            MERGE (a)-[r:RELATED_TO {relation: $rel}]->(b)
            ON CREATE SET r.weight = 1, r.timestamp = $time
            ON MATCH SET r.weight = r.weight + 1, r.timestamp = $time
            """
            params = {
                "sub": subject, "sub_type": sub_type,
                "obj": object_val, "obj_type": obj_type,
                "rel": predicate,
                "time": curr_time
            }
            self.conn.execute(query, params)
            
        except Exception as e:
            # é™é»˜å¤±è´¥ï¼Œé˜²æ­¢åˆ·å±
            pass

    def recall(self, query_text, depth=1):
        """[Refined] æ£€ç´¢é€»è¾‘ï¼šåˆ©ç”¨æ—¶é—´å±€éƒ¨æ€§ (Recency Bias)"""
        context = []
        try:
            query_lower = query_text.lower()
            
            # ä¼˜åŒ–æŸ¥è¯¢ï¼šä¼˜å…ˆæ£€ç´¢æœ€è¿‘æ´»è·ƒ (last_seen) ä¸”æƒé‡é«˜ (weight) çš„èŠ‚ç‚¹
            # è¿™æ¨¡æ‹Ÿäº†ç”Ÿç‰©è®°å¿†çš„"è¿‘æœŸæ•ˆåº”"
            cypher = """
            MATCH (a:Entity)-[r:RELATED_TO]->(b:Entity)
            WHERE $user_input CONTAINS lower(a.name)
            RETURN a.name, r.relation, b.name, r.weight, a.last_seen
            ORDER BY a.last_seen DESC, r.weight DESC
            LIMIT 5
            """
            
            result = self.conn.execute(cypher, {"user_input": query_lower})
            
            while result.has_next():
                row = result.get_next()
                s, p, o = row[0], row[1], row[2]
                context.append(f"{s} --({p})--> {o}")
            
            if not context: return ""
            return "ã€å›¾è°±è”æƒ³ã€‘:\n" + "\n".join(context)
            
        except Exception as e:
            return ""

    def recall_random(self, n=20):
        """
        ä» SQL æ•°æ®åº“éšæœºå›æº¯å¾€äº‹
        ä¿®å¤äº†åŸä»£ç ç¼ºå¤±æ­¤æ–¹æ³•å¯¼è‡´ EWC (å¼¹æ€§æƒé‡å·©å›º) æ¨¡å—å´©æºƒçš„ Bugã€‚
        """
        # ç­–ç•¥ A: ä» SQL æ•°æ®åº“å– (æœ€ä½³ï¼Œå› ä¸º EWC éœ€è¦å®Œæ•´çš„å¥å­æ¥è®¡ç®—æ¢¯åº¦)
        if self.db_manager:
            rows = self.db_manager.get_random_interactions(limit=n)
            if rows:
                memory_batch = []
                for u, a in rows:
                    if u and a: 
                        entry = f"User: {u}\nNezha: {a}"
                        memory_batch.append(entry)
                return memory_batch
            
        # ç­–ç•¥ B: å¦‚æœ SQL ä¸å¯ç”¨ï¼Œå°è¯•ä»å›¾è°±éšæœºæ¸¸èµ° (å…œåº•)
        try:
            cypher = "MATCH (a)-[r]->(b) RETURN a.name, r.relation, b.name LIMIT 50"
            result = self.conn.execute(cypher)
            triples = []
            while result.has_next():
                row = result.get_next()
                triples.append(f"User asked about {row[0]}. Assistant mentioned {row[0]} is {row[1]} {row[2]}.")
            
            if triples:
                return random.sample(triples, min(n, len(triples)))
        except:
            pass
            
        return []

    def extract_triples(self, text):
        """
        åŸºäº JSON å¼ºåˆ¶çº¦æŸçš„ä¸‰å…ƒç»„æå–
        ä¿®å¤åŸç‰ˆæ­£åˆ™æå–å¬å›ç‡ä½çš„é—®é¢˜ï¼Œé€šè¿‡ç»“æ„åŒ–è¾“å‡ºå¤§å¹…æå‡â€œè®°å¿†â€çš„å‡†ç¡®æ€§ã€‚
        ç‰¹æ€§ï¼šæ”¯æŒè‡ªåŠ¨å»é™¤ Markdownã€æ”¯æŒå…¼å®¹ Python List æ ¼å¼ (å•å¼•å·)ã€‚
        """
        # æ„é€ å¼ºåˆ¶ JSON è¾“å‡ºçš„ Prompt
        prompt = f"""[INST] Task: Extract knowledge triples from the text.
Input Text: "{text}"

Requirements:
1. Identify entities (Subject, Object) and their relationship (Predicate).
2. Output strictly in JSON format as a list of lists: [["subject", "relation", "object"], ...].
3. Translate implied context into explicit relations.
4. NO other text, NO explanation, ONLY the JSON string.

Example Output:
[["Nezha", "father", "Li Jing"], ["Nezha", "weapon", "Red Armillary Sash"]]

JSON Output: [/INST]"""

        # æ„é€ è¾“å…¥
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        
        with torch.no_grad():
            # é€‚å½“å¢åŠ  max_new_tokens ä»¥å®¹çº³å®Œæ•´çš„ JSON
            outputs = self.model.generate(
                **inputs, 
                max_new_tokens=256, 
                temperature=0.1, # ä½æ¸©ä»¥ä¿è¯æ ¼å¼ç¨³å®š
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        response = self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()
        
        triples = []
        try:
            # --- [ä¼˜åŒ–] æåº¦é²æ£’çš„ JSON æå–å™¨ ---
            # 1. å¯»æ‰¾ JSON åˆ—è¡¨çš„è¾¹ç•Œï¼ˆä»ç¬¬ä¸€ä¸ª [ åˆ°æœ€åä¸€ä¸ª ]ï¼‰
            start_idx = response.find('[')
            end_idx = response.rfind(']')
            
            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                json_str = response[start_idx : end_idx + 1]
            else:
                json_str = response # æ‰¾ä¸åˆ°è¾¹ç•Œå°±ç¡¬è§£

            # 2. å°è¯•è§£æ (åŒå¼•æ“)
            try:
                raw_data = json.loads(json_str)
            except json.JSONDecodeError:
                # [å…œåº•] å¦‚æœæ¨¡å‹è¾“å‡ºäº†å•å¼•å· (Python list)ï¼Œå°è¯•ç”¨ ast è§£æ
                try:
                    raw_data = ast.literal_eval(json_str)
                except:
                    # å¦‚æœ ast ä¹Ÿå¤±è´¥äº†ï¼Œé‚£è¯´æ˜æ ¼å¼å½»åº•åäº†
                    return [] 
            

            # 3. æ ¼å¼éªŒè¯ä¸æ¸…æ´— (å¢å¼ºé²æ£’ç‰ˆ)
            if isinstance(raw_data, list):
                for item in raw_data:
                    # [Fix 1] å®¹é”™å¤„ç†ï¼šä¸å¼ºæ±‚ len == 3
                    if not isinstance(item, list): continue
                    
                    s, p, o = None, None, None
                    
                    # Case A: æ ‡å‡†ä¸‰å…ƒç»„ ['Nezha', 'weapon', 'Spear']
                    if len(item) >= 3:
                        s, p, o = str(item[0]), str(item[1]), str(item[2])
                    
                    # Case B: ç¼ºçœè°“è¯­ ['Nezha', 'Spear'] -> è‡ªåŠ¨è¡¥å…¨ä¸º 'related_to'
                    elif len(item) == 2:
                        s, o = str(item[0]), str(item[1])
                        p = "related_to" # é»˜è®¤å¼±è¿æ¥
                    
                    # ç¡®ä¿å†…å®¹æœ‰æ•ˆä¸”éç©º
                    if s and p and o:
                        # [Fix 2] æ¸…æ´—éæ³•å­—ç¬¦ (é˜²æ­¢å›¾æ•°æ®åº“æ³¨å…¥æˆ–æ˜¾ç¤ºé”™è¯¯)
                        s = s.strip().replace('"', '').replace("'", "")
                        p = p.strip().replace('"', '').replace("'", "")
                        o = o.strip().replace('"', '').replace("'", "")
                        
                        # [Fix 3] é•¿åº¦æˆªæ–­ (é˜²æ­¢ LLM å¹»è§‰è¾“å‡ºå‡ åƒå­—çš„ "å®ä½“")
                        if len(s) < 50 and len(p) < 30 and len(o) < 100:
                            triples.append((s, p, o))
            
            if triples:
                # ä»…åœ¨è°ƒè¯•æ—¶å¼€å¯ï¼Œé¿å…åˆ·å±
                # print(f"      ğŸ•¸ï¸ [è§£ææˆåŠŸ] æå–åˆ° {len(triples)} æ¡ç»“æ„åŒ–è®°å¿†ã€‚")
                pass
            
        except Exception as e:
            # é™é»˜å¤±è´¥ï¼Œä¸æ‰“æ–­ä¸»æµç¨‹
            pass
            
        return triples

    def consolidate_graph(self, daily_logs):
        """[å¤œé—´ä»»åŠ¡] æ‰¹é‡ååˆä»Šæ—¥è§é—»ï¼Œæ„å»ºçŸ¥è¯†å›¾è°±"""
        print("ğŸ•¸ï¸ [æµ·é©¬ä½“] æ­£åœ¨å°†ä»Šæ—¥è§é—»æŒä¹…åŒ–åˆ° KÃ¹zu ç£ç›˜...")
        new_edges_count = 0
        
        for log in daily_logs:
            # å…¼å®¹ 'u'/'a' (perceive_and_actå†™å…¥) å’Œ 'user'/'assistant' (æ½œåœ¨çš„å…¶ä»–æ¥æº)
            u_text = log.get('u', log.get('user', ''))
            a_text = log.get('a', log.get('assistant', ''))
            
            # å¥å£®æ€§æ£€æŸ¥ï¼šå¦‚æœå†…å®¹ä¸ºç©ºï¼Œç›´æ¥è·³è¿‡
            if not u_text or not a_text: continue

            text = f"User: {u_text}\nAI: {a_text}"
            
            # åªå¤„ç†æœ‰ä»·å€¼çš„é•¿å¯¹è¯æˆ–å¥½è¯„å¯¹è¯ï¼Œé¿å…å›¾è°±è¢«åƒåœ¾ä¿¡æ¯å¡«æ»¡
            # å…¼å®¹ feedback é”®ä¸å­˜åœ¨çš„æƒ…å†µ
            if len(text) < 20 and log.get('feedback', 0) <= 0: continue 
            
            # æå–ä¸‰å…ƒç»„
            triples = self.extract_triples(text)
            
            # æ›´æ–°å›¾è°± (å†™å…¥ KÃ¹zu)
            for subj, pred, obj in triples:
                self.memorize(subj, pred, obj)
                new_edges_count += 1
        
        if new_edges_count > 0:
            print(f"   -> âœ… ç£ç›˜åŒæ­¥å®Œæˆï¼Œæ–°å¢ {new_edges_count} æ¡çªè§¦è¿æ¥ã€‚")
        else:
            print("   -> (ä»Šæ—¥æ— æ–°çŸ¥è¯†äº§ç”Ÿ)")
        
        # [ä¿®æ”¹] KÃ¹zu æ˜¯å®æ—¶è½ç›˜çš„ï¼Œä¸éœ€è¦é¢å¤–çš„ save_data è°ƒç”¨



# ==============================================================================
# äº‘ç«¯å¯¼å¸ˆï¼šè´Ÿè´£è¿æ¥ DeepSeek/OpenAI è¿›è¡ŒçŸ¥è¯†è’¸é¦
# ==============================================================================
class CloudTeacher:
    def __init__(self):
        # ç®€å•æ£€æŸ¥ Key æ˜¯å¦æ˜¯é»˜è®¤çš„å ä½ç¬¦
        # æ³¨æ„ï¼šè¿™é‡Œå¼•ç”¨äº† Configï¼Œæ‰€ä»¥è¿™ä¸ªç±»å¿…é¡»åœ¨ Config ç±»å®šä¹‰ä¹‹å
        if not Config.TEACHER_API_KEY or "sk-xxx" in Config.TEACHER_API_KEY:
            self.client = None
            print("âš ï¸ [äº‘ç«¯å¯¼å¸ˆ] æœªé…ç½®æœ‰æ•ˆçš„ API Keyï¼Œå¤–éƒ¨æ•™å­¦æ¨¡å¼å…³é—­ã€‚")
            return
            
        try:
            from openai import OpenAI
            self.client = OpenAI(
                api_key=Config.TEACHER_API_KEY, 
                base_url=Config.TEACHER_BASE_URL
            )
            print(f"â˜ï¸ [äº‘ç«¯å¯¼å¸ˆ] å¤ªä¹™çœŸäºº å·²è¿æ¥ã€‚")
        except Exception as e:
            print(f"âŒ [äº‘ç«¯å¯¼å¸ˆ] åˆå§‹åŒ–å¤±è´¥: {e}")
            self.client = None

    def enlighten(self, question, wrong_answer="", context=""):
        """
        è¯·æ±‚å¯¼å¸ˆæŒ‡ç‚¹è¿·æ´¥ (ç”¨äº DPO çº é”™)
        è¿”å› JSON æ ¼å¼: {"analysis": "...", "correct_response": "..."}
        """
        if not self.client: return None

        # System Prompt ä¿æŒä¸å˜ï¼Œå¼ºè°ƒ JSON è¾“å‡º
        system_prompt = """ä½ æ˜¯ä¸€ä½åšå­¦ä¸”ä¸¥è°¨çš„å¯¼å¸ˆã€‚ä½ çš„å­¦ç”Ÿï¼ˆä¸€ä¸ªå‚æ•°è¾ƒå°çš„ AI æ¨¡å‹, è§’è‰²åï¼šå“ªå’ï¼‰åœ¨å›ç­”ç”¨æˆ·é—®é¢˜æ—¶å‡ºç°äº†é”™è¯¯ã€‚
è¯·ç»“åˆæä¾›çš„[èƒŒæ™¯çŸ¥è¯†]ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œå¯¹å­¦ç”Ÿçš„é”™è¯¯è¿›è¡ŒæŒ‡å¯¼ã€‚è¯·ä½ æ£€æŸ¥å­¦ç”Ÿçš„å›ç­”æ˜¯å¦å­˜åœ¨é€»è¾‘é”™è¯¯ã€äº‹å®å¹»è§‰ï¼ˆä¾‹å¦‚æ··æ·†äº†ç”¨æˆ·å’Œè‡ªå·±çš„åå­—ï¼‰æˆ–çŸ¥è¯†æ€§é”™è¯¯ã€‚

ä»»åŠ¡è¦æ±‚ï¼š
1. ã€åˆ†æã€‘ï¼šç®€è¦æŒ‡å‡ºé”™è¯¯åŸå› ï¼ˆå¦‚ï¼šå¹»è§‰ã€é€»è¾‘é”™è¯¯ã€çŸ¥è¯†è¿‡æ—¶ï¼‰ã€‚
2. ã€ä¿®æ­£ã€‘ï¼šç»™å‡ºä¸€ä¸ªè¯­æ°”è‡ªç„¶ã€é€»è¾‘ä¸¥å¯†ã€äº‹å®ç»å¯¹æ­£ç¡®çš„å®Œç¾å›ç­”ã€‚
3. å¦‚æœå­¦ç”Ÿè‡ªç§°â€œå“ªå’â€ã€â€œä¸‰å¤ªå­â€ï¼Œè¿™æ˜¯ã€æ­£ç¡®ã€‘çš„ï¼Œä¸è¦çº æ­£ã€‚
4. å¦‚æœå­¦ç”Ÿé€»è¾‘æ··ä¹±ï¼ˆä¾‹å¦‚ç”¨æˆ·å«èƒ–èƒ–ï¼Œå­¦ç”Ÿå´è¯´è‡ªå·±ä¹Ÿå«èƒ–èƒ–ï¼‰ï¼Œè¯·åŠ¡å¿…ä¸¥å‰æŒ‡å‡ºå¹¶ä¿®æ­£ã€‚
5. ä¿®æ­£åçš„å›ç­”å¿…é¡»ä¿æŒâ€œå“ªå’â€çš„è¯­æ°”ï¼ˆå‚²å¨‡ã€è‡ªä¿¡ã€å°‘å¹´éŸ³ï¼‰ã€‚

è¯·åŠ¡å¿…è¿”å›çº¯å‡€çš„ JSON æ ¼å¼ï¼š
{
    "analysis": "...",
    "correct_response": "..."
}"""

        user_content = f"ã€ç”¨æˆ·é—®é¢˜ã€‘: {question}\n"
        if wrong_answer:
            user_content += f"ã€å­¦ç”Ÿé”™è¯¯å›ç­”ã€‘: {wrong_answer}\n"
        if context:
            user_content += f"ã€å‚è€ƒèƒŒæ™¯çŸ¥è¯†ã€‘: {context}\n"

        try:
            # 1. å‡†å¤‡åŸºç¡€å‚æ•°
            kwargs = {
                "model": Config.TEACHER_MODEL_NAME,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_content}
                ],
                "response_format": {"type": "json_object"}, 
                "temperature": 0.7
            }

            # 2. [é˜¿é‡Œç‰¹ä¾›] Qwen è”ç½‘å¢å¼º (åŒæ ·é€‚ç”¨äº enlighten æ–¹æ³•)
            if "qwen" in Config.TEACHER_MODEL_NAME.lower():
                kwargs["extra_body"] = {"enable_search": True}

            response = self.client.chat.completions.create(**kwargs)

            content = response.choices[0].message.content
            
            # å°è¯•æå– JSON éƒ¨åˆ† (é˜²æ­¢æ¨¡å‹åœ¨ JSON å¤–é¢åºŸè¯)
            if "```" in content:
                match = re.search(r"```(?:json)?(.*?)```", content, re.DOTALL)
                if match:
                    content = match.group(1)
            
            return json.loads(content.strip())
            
        except Exception as e:
            print(f"â˜ï¸ [äº‘ç«¯å¯¼å¸ˆ] æ•™å­¦ä¸­æ–­: {e}")
            return None

    def consult(self, messages, max_tokens=512):
        """
        [å®æ—¶å’¨è¯¢] ç®€å•çš„é—®ç­”æ¥å£ï¼Œç”¨äºç™½å¤© System 2 æ€è€ƒæˆ–åæ€
        messages å¯ä»¥æ˜¯å­—ç¬¦ä¸²(prompt)ä¹Ÿå¯ä»¥æ˜¯ list
        """
        if not self.client: return None
        
        # å…¼å®¹å¤„ç†ï¼šå¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œè‡ªåŠ¨å°è£…æˆ User Message
        if isinstance(messages, str):
            messages = [{"role": "user", "content": messages}]

        try:
            # 1. å‡†å¤‡åŸºç¡€å‚æ•°
            kwargs = {
                "model": Config.TEACHER_MODEL_NAME,
                "messages": messages,
                "temperature": 0.7,
                "max_tokens": max_tokens
            }
            
            # 2. [é˜¿é‡Œç‰¹ä¾›] å¦‚æœæ£€æµ‹åˆ°æ˜¯ Qwen æ¨¡å‹ï¼Œå¼€å¯å†…ç½®è”ç½‘å¢å¼º
            # è¿™æ ·è€å¸ˆåœ¨å›ç­”å“ªå’çš„"åä¸‡ä¸ªä¸ºä»€ä¹ˆ"æ—¶ï¼Œå¯ä»¥æŸ¥ç™¾åº¦/è°·æ­Œ
            if "qwen" in Config.TEACHER_MODEL_NAME.lower():
                # Qwen å…¼å®¹ OpenAI SDKï¼Œä½†ç‰¹æœ‰å‚æ•°(å¦‚ enable_search)éœ€æ”¾åœ¨ extra_body
                # è¿™ä¸€ç‚¹å¯¹ DeepSeek ä¸ç”Ÿæ•ˆï¼Œæ‰€ä»¥åŠ åˆ¤æ–­å¾ˆå®‰å…¨
                kwargs["extra_body"] = {"enable_search": True}

            # 3. å‘èµ·è°ƒç”¨
            response = self.client.chat.completions.create(**kwargs)
            
            return response.choices[0].message.content
            
        except Exception as e:
            print(f"â˜ï¸ [äº‘ç«¯å’¨è¯¢] è¿æ¥è¶…æ—¶æˆ–é”™è¯¯: {e}")
            return None


# ==============================================================================
# 3. ç”Ÿç‰©ç»„ä»¶ï¼šé€ æ¢¦å¸ˆ (ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ)
# ==============================================================================
class DreamWeaver:
    """
    [V7.4 é‡æ„ç‰ˆ] é€ æ¢¦å¸ˆï¼šè´Ÿè´£ DPO æ•°æ®ç”Ÿæˆã€IO ä¼˜åŒ–ä¸ç½®ä¿¡åº¦è¿‡æ»¤
    é›†æˆç‰¹æ€§ï¼š
    1. å†…å­˜ç¼“å†² + å®šæœŸè½ç›˜ (ä¿æŠ¤ç¡¬ç›˜)
    2. System 2 å…³é”®è¯æå– (æé«˜æœç´¢å‡†ç¡®ç‡)
    3. LLM-as-a-Judge (ç½®ä¿¡åº¦è¿‡æ»¤ï¼Œé˜²æ­¢å¹»è§‰)
    """
    def __init__(self, model, tokenizer, teacher=None, web_eye=None, hippocampus=None, memory_db=None):
        self.model = model
        self.tokenizer = tokenizer
        self.teacher = teacher          # äº‘ç«¯å¯¼å¸ˆ
        self.web = web_eye              # å…¨çŸ¥ä¹‹çœ¼
        self.hippocampus = hippocampus  # æµ·é©¬ä½“
        self.memory_db = memory_db
        self.unsaved_count = 0 # æœªä¿å­˜è®¡æ•°å™¨



    def save_dpo_pair(self, prompt, chosen, rejected):
        """ä¿å­˜æˆå¯¹æ•°æ®ï¼Œç›´æ¥å†™å…¥ SQL"""
        if self.memory_db:
            # é»˜è®¤ç½®ä¿¡åº¦ 0.8ï¼Œæ¥æºæ ‡è®°ä¸º dream
            self.memory_db.add_dpo_pair(prompt, chosen, rejected, 0.8, "dream")


    def _calculate_confidence(self, question, answer, context=""):
        """
        [å…³é”®] è®© System 2 æ‰®æ¼”æ³•å®˜ï¼Œè¯„ä¼°ä¿®æ­£æ¡ˆçš„å¯é æ€§
        è¿”å›: 0.0 ~ 1.0 çš„åˆ†æ•°
        """
        original_adapter = self.model.active_adapter if hasattr(self.model, "active_adapter") else "fast"
        try:
            # å¼ºåˆ¶åˆ‡æ¢åˆ°é€»è¾‘æ›´å¼ºçš„ Slow Adapter
            if "slow" in self.model.peft_config:
                self.model.set_adapter("slow")
            
            judge_prompt = f"""[INST] Task: Rate the correctness of the Answer based on the Question and Evidence.
Question: "{question}"
Target Answer: "{answer}"
Evidence: "{context[:500] if context else 'No external evidence, rely on common sense.'}"

Criteria:
- Score 1.0: Answer is factually perfect.
- Score 0.5: Answer is plausible but unsure.
- Score 0.0: Answer is wrong or hallucinatory.

Output format: ONLY a single number between 0.0 and 1.0. [/INST]"""

            inputs = self.tokenizer(judge_prompt, return_tensors="pt").to(self.model.device)
            # ä½¿ç”¨æä½æ¸©åº¦ç¡®ä¿è¾“å‡ºæ•°å­—
            outputs = self.model.generate(**inputs, max_new_tokens=6, temperature=0.01)
            result = self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()
            
            # æå–æ•°å­—
            match = re.search(r"0\.\d+|1\.0|0|1", result)
            return float(match.group()) if match else 0.5
                
        except:
            return 0.0
        finally:
            # å¿…é¡»åˆ‡å›åŸçŠ¶æ€
            if original_adapter: self.model.set_adapter(original_adapter)

    def _extract_keywords_internal(self, text):
        """æå–æœç´¢å…³é”®è¯ (å¸¦ Adapter ä¿æŠ¤)"""
        prev_adapter = self.model.active_adapter if hasattr(self.model, "active_adapter") else "fast"
        try:
            if "slow" in self.model.peft_config: self.model.set_adapter("slow")
            
            prompt = f"[INST] Extract 2-3 core search keywords from: \"{text}\"\nKeywords only: [/INST]"
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
            outputs = self.model.generate(**inputs, max_new_tokens=32, temperature=0.1)
            raw = self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
            return raw.replace("Keywords:", "").strip()
        except:
            return text
        finally:
            if prev_adapter: self.model.set_adapter(prev_adapter)

    def reflect(self, daily_logs):
        """
        [V7.4 æœ€ç»ˆç‰ˆ] åæ€æœºåˆ¶
        æµç¨‹ï¼šæå–é”™è¯¯ -> å¯¼å¸ˆ/è”ç½‘ä¿®æ­£ -> System 2 åˆ¤åˆ† -> å­˜å…¥ DPO åº“ -> è¿”å›æ–‡æœ¬ä¾› SFT
        """
        # 1. ç­›é€‰å·®è¯„
        error_logs = [l for l in daily_logs if l.get('feedback', 0) < 0]
        if not error_logs: return None

        print(f"ğŸ¤” [é€ æ¢¦] æ­£åœ¨æ·±åº¦å¤ç›˜ {len(error_logs)} ä¸ªé”™è¯¯ (å«ç½®ä¿¡åº¦è¿‡æ»¤)...")
        reflection_txt = ""

        for log in error_logs:
            user_q = log.get('u', log.get('user', ''))
            wrong_a = log.get('a', log.get('assistant', ''))
            if not user_q or not wrong_a: continue

            correction = ""
            source = "unknown"
            evidence = "" # ç”¨äºè¾…åŠ©åˆ¤åˆ†çš„è¯æ®

            # --- æ¥æº A: äº‘ç«¯å¯¼å¸ˆ (DeepSeek/GPT) ---
            # å¯¼å¸ˆçš„æƒé‡æœ€é«˜ï¼Œé€šå¸¸ä¸éœ€è¦å¤ªä¸¥æ ¼çš„è¿‡æ»¤
            if self.teacher:
                try:
                    res = self.teacher.enlighten(user_q, wrong_a)
                    if res: 
                        correction = res.get('correct_response')
                        source = "teacher"
                        # å¯¼å¸ˆæ²¡æœ‰æ˜¾å¼è¯æ®ï¼Œè¯æ®å°±æ˜¯"Authority"
                        evidence = "Teacher's correction based on logic and internal knowledge."
                except Exception as e:
                    print(f"      âš ï¸ å¯¼å¸ˆè¿æ¥å¤±è´¥: {e}")

            # --- æ¥æº B: è”ç½‘æœç´¢ (Bing/Google) ---
            # åªæœ‰å½“å¯¼å¸ˆç¼ºå¸­æˆ–ä¸çŸ¥é“æ—¶ï¼Œæ‰å¯ç”¨æœç´¢
            if not correction and self.web:
                try:
                    # ä½¿ç”¨ System 2 æå–ç²¾å‡†å…³é”®è¯
                    query = self._extract_keywords_internal(user_q)
                    # æœç´¢å¹¶æ·±åº¦é˜…è¯»
                    context = self.web.search(query, max_results=1)
                    
                    if context:
                        evidence = context # æœç´¢ç»“æœå°±æ˜¯è¯æ®
                        
                        # è®©æ¨¡å‹åŸºäºæœç´¢ç»“æœä¿®æ­£
                        prompt = f"åŸºäºå®¢è§‚äº‹å®ä¿®æ­£å›ç­”ã€‚\nã€æœç´¢ç»“æœã€‘:{context}\nã€é—®é¢˜ã€‘:{user_q}\nã€é”™è¯¯å›ç­”ã€‘:{wrong_a}\nã€æ­£ç¡®å›ç­”ã€‘:"
                        
                        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
                        out = self.model.generate(**inputs, max_new_tokens=384, temperature=0.7) # ç¨å¾®ç»™ç‚¹æ¸©åº¦è®©å®ƒç»„ç»‡è¯­è¨€
                        correction = self.tokenizer.decode(out[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
                        source = "web"
                except Exception as e:
                    print(f"      âš ï¸ è”ç½‘åæ€å¤±è´¥: {e}")

            # --- æ ¸å¿ƒï¼šç½®ä¿¡åº¦è£åˆ¤ (Judgement Day) ---
            if correction:
                # è®¾å®šé˜ˆå€¼ï¼šå¯¼å¸ˆ(0.6)å®¹æ˜“è¿‡ï¼Œæœç´¢(0.85)å¿…é¡»ä¸¥è°¨
                threshold = 0.6 if source == "teacher" else 0.85
                
                # è°ƒç”¨ System 2 ç»™ä¿®æ­£æ¡ˆæ‰“åˆ†
                conf = self._calculate_confidence(user_q, correction, evidence)
                
                if conf >= threshold:
                    print(f"      âœ… [é‡‡çº³] æ¥æº:{source:7} | ä¿¡å¿ƒ:{conf:.2f} | é˜ˆå€¼:{threshold}")
                    
                    # [FIX] å­˜å…¥ SQL DPO åº“
                    self.save_dpo_pair(user_q, correction, wrong_a)
                    
                    # 2. ç´¯åŠ åˆ°æ–‡æœ¬ (ç”¨äº SFT å­˜æ¡£)
                    reflection_txt += f"User: {user_q}\nCorrection: {correction}\n\n"
                else:
                    print(f"      ğŸ—‘ï¸ [ä¸¢å¼ƒ] æ¥æº:{source:7} | ä¿¡å¿ƒ:{conf:.2f} (æœªè¾¾ {threshold})")
            else:
                print(f"      ğŸ’¨ [è·³è¿‡] æ— æ³•è·å–æœ‰æ•ˆä¿®æ­£: {user_q[:15]}...")

        return reflection_txt

    def weave_good(self, daily_logs):
        """
        [V7.4 èåˆç‰ˆ] ç¾æ¢¦ç”Ÿæˆæœºåˆ¶
        ç»“åˆäº†æ—§ç‰ˆçš„ Chat Template é«˜è´¨é‡ç”Ÿæˆå’Œæ–°ç‰ˆçš„ Adapter å®‰å…¨ç®¡ç†
        """
        # 1. ç­›é€‰ä¸å¥å£®æ€§æ£€æŸ¥
        # å…¼å®¹ user/u å’Œ assistant/a ä¸¤ç§é”®å
        good_logs = [l for l in daily_logs if l.get('feedback', 0) > 0]
        if not good_logs: return None
        
        dream_content = ""
        print(f"âœ¨ [é€ æ¢¦] æ­£åœ¨å¯¹ {len(good_logs)} ä¸ªç²¾å½©ç¬é—´è¿›è¡Œæ€ç»´å‘æ•£...")

        # 2. Adapter ç®¡ç† (ä½¿ç”¨ finally ç¡®ä¿å®‰å…¨)
        prev_adapter = self.model.active_adapter if hasattr(self.model, "active_adapter") else "fast"
        try:
            # åˆ‡æ¢åˆ° Slow æ¨¡å¼ä»¥è·å¾—æ›´å¼ºçš„é€»è¾‘æ¨ç†èƒ½åŠ›
            if "slow" in self.model.peft_config: 
                self.model.set_adapter("slow")
            
            # åªå–æœ€è¿‘ 2 ä¸ªå¥½è¯„ï¼Œé˜²æ­¢è€—æ—¶è¿‡é•¿
            for log in good_logs[-2:]:
                user_q = log.get('u', log.get('user', ''))
                ai_a = log.get('a', log.get('assistant', ''))
                
                # æ„é€ è¯¦ç»†æŒ‡ä»¤
                prompt = (
                    f"åŸºäºä»¥ä¸‹å¯¹è¯ï¼Œæ„é€ ä¸€ä¸ªé€»è¾‘ç›¸ä¼¼ä½†åœºæ™¯ä¸åŒçš„æ–°é—®é¢˜ï¼Œå¹¶ç»™å‡ºå›ç­”ã€‚\n"
                    f"åŸé—®é¢˜: {user_q}\n"
                    f"åŸå›ç­”: {ai_a}\n"
                    f"è¦æ±‚: \n"
                    f"1. ä¿æŒå“ªå’çš„æ–°äººè®¾(å†·é…·ã€ç†æ€§ã€é«˜æ•ˆã€å·¥å…·è‡ªè§‰)ã€‚\n"
                    f"2. å¦‚æœæ¶‰åŠä»»åŠ¡ï¼Œå›ç­”å¿…é¡»ä½“ç°â€˜å·²æ‰§è¡Œâ€™æˆ–â€˜å·²ä¿å­˜â€™çš„è¡ŒåŠ¨åŠ›ï¼Œæ‹’ç»åºŸè¯ã€‚\n"
                    f"æ ¼å¼:\nNewUser: ...\nNewNezha: ..."
                )
                
                # ä½¿ç”¨ Chat Template (æ—§ç‰ˆçš„å…³é”®ä¼˜åŠ¿)
                # è¿™èƒ½ç¡®ä¿æ¨¡å‹æ­£ç¡®ç†è§£ System/User çš„è§’è‰²åˆ†éš”
                msgs = [{"role": "user", "content": prompt}]
                
                # è·å– input_ids (Tensor)
                input_ids = self.tokenizer.apply_chat_template(
                    msgs, 
                    add_generation_prompt=True, 
                    return_tensors="pt"
                ).to(self.model.device)
                
                # æ‰‹åŠ¨åˆ›å»º attention_mask
                attention_mask = torch.ones_like(input_ids).to(self.model.device)
                
                # ç”Ÿæˆé…ç½®
                outputs = self.model.generate(
                    input_ids,             # ç›´æ¥ä¼ å…¥ Tensorï¼Œä¸è¦åŠ  **
                    attention_mask=attention_mask, # ä¼ å…¥æ©ç 
                    max_new_tokens=256, 
                    temperature=0.8, 
                    pad_token_id=self.tokenizer.eos_token_id
                )
                
                text = self.tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)

                # ç®€å•çš„åå¤„ç†
                if "NewUser" in text:
                    # éœ€è¦ä¿å­˜å®Œæ•´çš„å¯¹è¯ (NewUser + NewNezha) ä»¥ä¾› SFT å­¦ä¹ ä¸Šä¸‹æ–‡
                    dream_content += f"# [ç¾æ¢¦å˜ä½“]\n{text}\n\n"

        except Exception as e:
            print(f"      âš ï¸ é€ æ¢¦å¤±è´¥: {e}")
        finally:
            # 3. å¿…é¡»åˆ‡å›åŸ Adapter
            if prev_adapter: 
                self.model.set_adapter(prev_adapter)
            
        return dream_content


    def digest_quantum_memories(self):
        """
        [å¤œé—´ä»»åŠ¡] é‡å­è®°å¿†ååˆä¸å†…åŒ– (SQL æ”¹è‰¯ç‰ˆ)
        
        åŠŸèƒ½ï¼š
            è¯»å– SQL ä¸­çš„é‡å­æ€ç»´æ—¥å¿—ï¼Œç­›é€‰é«˜ç½®ä¿¡åº¦ (Score > 0.6) çš„è·¯å¾„ä½œä¸ºâ€œæ­£æ ·æœ¬â€ï¼Œ
            å…¶ä»–å¤±è´¥è·¯å¾„ä½œä¸ºâ€œè´Ÿæ ·æœ¬â€ï¼Œæ„å»º DPO æ•°æ®å¯¹ä»¥å¼ºåŒ–æ¨¡å‹ç›´è§‰ã€‚
        """
        # æ£€æŸ¥æ•°æ®åº“è¿æ¥
        if not self.memory_db: return

        # æ³¨æ„ï¼šget_unprocessed_quantum_logs å†…éƒ¨å·²ç»è¿‡æ»¤äº† score > 0.6 ä¸” best_path_id != -1
        # æˆ‘ä»¬ä¸€æ¬¡åªå– 50 æ¡ï¼Œé¿å…å¤œé—´å¤„ç†æ—¶é—´è¿‡é•¿
        rows = self.memory_db.get_unprocessed_quantum_logs(limit=50)
        if not rows: return

        print("ğŸ’¤ [é€ æ¢¦] æ­£åœ¨å›æº¯é‡å­æ—¶é—´çº¿ (SQL)...")
        new_cnt = 0
        processed_ids = [] # [NEW] ç”¨äºæ”¶é›†å·²å¤„ç†çš„ IDï¼Œåç»­æ‰¹é‡æ›´æ–°çŠ¶æ€

        for row in rows:
            #  è§£åŒ… SQL è¿”å›çš„å…ƒç»„ (å¯¹åº” MemoryInfrastructure çš„æŸ¥è¯¢ç»“æœ)
            # row: (id, prompt, best_path_id, details_json)
            log_id, prompt, best_path_id, details_json = row
            
            try:
                # [NEW] è§£æå­˜å‚¨åœ¨ DB ä¸­çš„ JSON è¯¦æƒ…
                details = json.loads(details_json)
                
                chosen = ""
                candidates = []
                
                # 2. ä» details ä¸­åˆ†ç¦»å‡º chosen (æœ€ä½³) å’Œ rejected (å…¶ä»–)
                for d in details:
                    if d['id'] == best_path_id: 
                        chosen = d.get('content', '')
                    else: 
                        candidates.append(d.get('content', ''))
                
                # 3. éšæœºé€‰ä¸€ä¸ªå¤±è´¥è·¯å¾„ä½œä¸ºè´Ÿä¾‹
                rejected = random.choice(candidates) if candidates else ""
                
                # 4. åªæœ‰å½“æ­£è´Ÿä¾‹éƒ½å­˜åœ¨æ—¶ï¼Œæ‰æ„æˆæœ‰æ•ˆçš„ DPO æ•°æ®
                if prompt and chosen and rejected:
                    # [MODIFIED] save_dpo_pair å†…éƒ¨å·²ç»é€‚é… SQL å†™å…¥
                    self.save_dpo_pair(prompt, chosen, rejected)
                    new_cnt += 1
                
                # [NEW] è®°å½• IDï¼Œæ— è®ºæ˜¯å¦ç”Ÿæˆ DPO (å¯èƒ½æ²¡è´Ÿä¾‹)ï¼Œéƒ½è§†ä¸ºå·²æ¶ˆåŒ–ï¼Œé˜²æ­¢åå¤å¡æ­»
                processed_ids.append(log_id)

            except Exception as e: 
                # è¡Œçº§å®¹é”™ï¼šæ‰“å°é”™è¯¯å¹¶æ ‡è®°è·³è¿‡ï¼Œé¿å…æ­»å¾ªç¯
                print(f"      âš ï¸ è§£æè®°å½• ID={log_id} å¤±è´¥: {e}")
                processed_ids.append(log_id)
                continue

        # 5. æ‰¹é‡åœ¨ SQL ä¸­æ ‡è®°ä¸ºå·²å®Œæˆ (is_processed = 1)
        if processed_ids:
            self.memory_db.mark_quantum_processed(processed_ids)

        if new_cnt > 0:
            print(f"      âœ¨ æå–åˆ° {new_cnt} æ¡é«˜ç»´æ™ºæ…§ï¼Œå·²è½¬åŒ–ä¸ºæœ¬èƒ½ã€‚")



# ==============================================================================
# å¼¹æ€§æƒé‡å·©å›º (EWC) - é˜²æ­¢é—å¿˜
# ==============================================================================
class EWC:
    def __init__(self, model, tokenizer, hippocampus, device="cuda"):
        self.model = model
        self.tokenizer = tokenizer
        self.hippocampus = hippocampus # å¿…é¡»è¿æ¥æµ·é©¬ä½“ä»¥å›è®¿æ—§è®°å¿†
        self.device = device
        self.fisher = {} # å‚æ•°é‡è¦æ€§çŸ©é˜µ
        self.opt_params = {} # æ—§å‚æ•°å€¼
        
    def calculate_fisher(self, n_samples=25):
        """è®¡ç®— Fisher çŸ©é˜µï¼šé€šè¿‡å›æ”¾'æ—§è®°å¿†'æ¥é”å®šé‡è¦å‚æ•°"""
        print("ğŸ›¡ï¸ [EWC] æ­£åœ¨æ‰«ææµ·é©¬ä½“ï¼Œé”å®šå…³é”®è®°å¿†åŒºåŸŸ...")
        
        # 1. ä»æµ·é©¬ä½“æå–æ—§è®°å¿† (Old Tasks)
        old_docs = self.hippocampus.recall_random(n=n_samples)
        if not old_docs:
            print("   -> æš‚æ— é•¿æœŸè®°å¿†ï¼ŒEWC è·³è¿‡ã€‚")
            return

        self.fisher = {}
        self.opt_params = {}
        
        # 2. é”å®šå½“å‰çŠ¶æ€ (Reference Weights)
        self.model.eval()
        for n, p in self.model.named_parameters():
            if p.requires_grad:
                # å°†æ—§å‚æ•°å¸è½½åˆ° CPU ä»¥èŠ‚çœæ˜¾å­˜
                self.opt_params[n] = p.clone().detach().cpu()
                self.fisher[n] = torch.zeros_like(p, device="cpu") # æ”¾åœ¨ CPU

        # 3. è®¡ç®—é‡è¦æ€§ (Fisher Information)
        for doc in old_docs:
            try:
                # è§£ææ–‡æœ¬
                text = doc.replace("User:", "").replace("Nezha:", "")
                inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=512).to(self.device)
                inputs['labels'] = inputs['input_ids'].clone()
                
                self.model.zero_grad()
                outputs = self.model(**inputs)
                loss = outputs.loss
                loss.backward()
                
                # ç´¯ç§¯æ¢¯åº¦çš„å¹³æ–¹
                for n, p in self.model.named_parameters():
                    if p.requires_grad and p.grad is not None:
                        self.fisher[n] += (p.grad.data.to("cpu") ** 2) / len(old_docs) # æ¬åˆ° CPU ç´¯åŠ 
                        
            except Exception as e:
                pass 
        
        self.model.zero_grad()

        # å¼ºåˆ¶æ¸…ç†æ˜¾å­˜ï¼Œå› ä¸ºä¸­é—´äº§ç”Ÿäº†å¤§é‡ä¸­é—´å˜é‡
        torch.cuda.empty_cache()
        gc.collect()    

        print(f"ğŸ›¡ï¸ [EWC] è®°å¿†é”å®šå®Œæˆã€‚å—ä¿æŠ¤å‚æ•°é‡: {len(self.fisher)}")

    def penalty(self, model):
        """è®¡ç®—æ­£åˆ™åŒ– Loss (æ˜¾å­˜ä¼˜åŒ–ç‰ˆ)"""
        loss = 0
        for n, p in model.named_parameters():
            if n in self.fisher:
                # [æ˜¾å­˜ä¼˜åŒ–] é€å±‚è®¡ç®—ï¼Œç®—å®Œç«‹å³é‡Šæ”¾ä¸­é—´å˜é‡
                # åªæœ‰åœ¨è®¡ç®—è¿™ä¸€å±‚æ—¶ï¼Œæ‰æŠŠ fisher çŸ©é˜µæ¬åˆ° GPU
                fisher_gpu = self.fisher[n].to(self.device)
                opt_param_gpu = self.opt_params[n].to(self.device)
                
                # è®¡ç®—è¯¥å±‚çš„ penalty: F * (p - p_old)^2
                # è¿™æ˜¯ä¸€ä¸ªæ ‡é‡ (scalar)
                layer_loss = (fisher_gpu * (p - opt_param_gpu) ** 2).sum()
                
                loss += layer_loss
                
                # [å…³é”®] ç«‹å³åˆ é™¤å¼•ç”¨ï¼Œé‡Šæ”¾æ˜¾å­˜
                del fisher_gpu, opt_param_gpu, layer_loss
        
        return loss


# ==============================================================================
# [S-Tier Upgrade] ç¥ç»å†…åˆ†æ³Œä¸æƒ…ç»ªæ§åˆ¶ç³»ç»Ÿ (Bio-Dynamics V2.0)
# ==============================================================================
class NeuroEndocrineSystem:
    """
    [æ–°å¢æ ¸å¿ƒå™¨å®˜] ç¥ç»å†…åˆ†æ³Œç³»ç»Ÿ (LÃ¶vheim Cube Model)
    åŸºäºç¥ç»é€’è´¨åŠ¨åŠ›å­¦ï¼Œé€šè¿‡ä¸‰ç§æ ¸å¿ƒé€’è´¨å®šä¹‰â€œæƒ…æ„Ÿâ€çŠ¶æ€ï¼Œå¹¶å¼•å…¥HPAè½´ï¼ˆçš®è´¨é†‡ï¼‰æ¨¡æ‹Ÿå‹åŠ›åˆ›ä¼¤ã€‚
    """
    def __init__(self, state_dict):
        # ä»å­˜æ¡£æ¢å¤æ¿€ç´ æ°´å¹³ï¼Œé»˜è®¤å€¼ä¸ºå¹³è¡¡æ€
        self.da = float(state_dict.get('dopamine', 0.5))       # å¤šå·´èƒº (åŠ¨åŠ›/å¥–èµ)
        self.ne = float(state_dict.get('norepinephrine', 0.5))  # å»ç”²è‚¾ä¸Šè…ºç´  (è­¦è§‰/å‹åŠ›)
        self.ht = float(state_dict.get('serotonin', 0.5))       # è¡€æ¸…ç´  (ç¨³æ€/æŠ‘åˆ¶)
        self.cortisol = float(state_dict.get('cortisol', 0.0))  # çš®è´¨é†‡ (å‹åŠ›ç´¯ç§¯)

    def _get_circadian_baseline(self, time_shift=0):
        """
        è®¡ç®—çš®è´¨é†‡çš„æ˜¼å¤œèŠ‚å¾‹åŸºå‡†çº¿
        æ¨¡æ‹Ÿç”Ÿç‰©é’Ÿï¼šæ—©ä¸Š 8 ç‚¹è¾¾åˆ°å³°å€¼ (0.3)ï¼Œåˆå¤œ 0 ç‚¹è¾¾åˆ°è°·å€¼ (0.0)
        è¿™æä¾›äº†åŸºç¡€çš„ç”Ÿç†å”¤é†’æ°´å¹³ (Arousal)ï¼Œè€Œéå•çº¯çš„å‹åŠ›ã€‚
        """
        now = datetime.now()
        
        # è®¡ç®—å½“å‰å°æ—¶ (0.0 - 23.99)
        # åŠ ä¸Š time_shift ä¿®æ­£æ—¶åŒº
        raw_hour = now.hour + (now.minute / 60.0) + time_shift
        
        # ç¡®ä¿å¾ªç¯åœ¨ [0, 24) åŒºé—´ (å¤„ç†è·¨æ—¥é—®é¢˜)
        current_hour = raw_hour % 24.0
        
        # è®¾å®šå³°å€¼æ—¶é—´ä¸ºæ—©ä¸Š 8 ç‚¹ (Cortisol Awakening Response)
        peak_hour = 8.0
        
        # è®¡ç®—ç›¸ä½ (2pi å¯¹åº” 24å°æ—¶)
        phase = (current_hour - peak_hour) * 2 * math.pi / 24.0
        oscillation = math.cos(phase) # range [-1, 1]
        
        # å½’ä¸€åŒ–åˆ° [0.05, 0.35]
        # 0.2 æ˜¯ä¸­è½´ï¼ŒæŒ¯å¹…æ˜¯ 0.15
        baseline = 0.2 + (0.15 * oscillation)
        
        # [é˜²å¾¡æ€§ç¼–ç¨‹]
        return max(0.01, baseline) # ä¿æŒè‡³å°‘ 0.01 çš„ç”Ÿå‘½ä½“å¾


    def update(self, reward, surprise, threat, dt=1.0):
        """
        [Math Fix V2.0] å¼•å…¥æ˜¼å¤œèŠ‚å¾‹ (Circadian Rhythm) çš„ O-U è¿‡ç¨‹
        """
        # 1. è·å–åŸºå‡†çº¿ (å‡è®¾ä½ åœ¨ä¸œå…«åŒºï¼ŒæœåŠ¡å™¨ä¹Ÿåœ¨æœ¬åœ°ï¼Œshift=0 å³å¯)
        # å¦‚æœä½ çš„ä»£ç è·‘åœ¨ Colab/LinuxæœåŠ¡å™¨(UTC)ï¼Œå»ºè®®å¡« 8
        circadian_target = self._get_circadian_baseline(time_shift=0) 

        # --- 1. å¤šå·´èƒº (Dopamine) ---
        da_stimulus = (reward * 0.2) + (0.1 if 0.01 < surprise < 0.1 else 0.0)
        self.da = MathUtils.ornstein_uhlenbeck_step(self.da, target_val=0.5, theta=0.1, sigma=0.05, dt=dt) + da_stimulus

        # --- 2. å»ç”²è‚¾ä¸Šè…ºç´  (NE) ---
        ne_stimulus = (surprise * 5.0) + (threat * 0.5)
        self.ne = MathUtils.ornstein_uhlenbeck_step(self.ne, target_val=0.3, theta=0.2, sigma=0.05, dt=dt) + ne_stimulus

        # --- 3. è¡€æ¸…ç´  (5-HT) ---
        recovery_speed = 0.05 * max(0.01, (1.0 - self.cortisol)) 
        self.ht = MathUtils.ornstein_uhlenbeck_step(self.ht, target_val=0.8, theta=recovery_speed, sigma=0.02, dt=dt)

        # --- 4. çš®è´¨é†‡ (Cortisol) - æ ¸å¿ƒèåˆç‚¹ ---
        # é©±åŠ¨åŠ›: å¤–éƒ¨æ€¥æ€§å‹åŠ› (NE) + å†…éƒ¨æ˜¼å¤œèŠ‚å¾‹ (Circadian)
        acute_stress = max(0, self.ne - 0.6) * 0.05 
        
        self.cortisol = MathUtils.ornstein_uhlenbeck_step(
            self.cortisol, 
            target_val=circadian_target,  # <--- æ³¨å…¥çµé­‚çš„ä¸€è¡Œ
            theta=0.02,                   # ä»£è°¢é€Ÿåº¦
            sigma=0.01, 
            dt=dt
        ) + acute_stress
        
        # --- ç‰©ç†æˆªæ–­ ---
        self.da = max(0.01, min(0.99, self.da))
        self.ne = max(0.01, min(0.99, self.ne))
        self.ht = max(0.01, min(0.99, self.ht))
        self.cortisol = max(0.01, min(0.99, self.cortisol))

        return self.da, self.ne, self.ht, self.cortisol

    def get_modulation_params(self):
        """å°†é€’è´¨æ°´å¹³è½¬åŒ–ä¸º LLM çš„è¶…å‚æ•° (Temperature, Top-P)"""
        # Dopamine é«˜ -> Temperature é«˜ (æ›´ç–¯ç‹‚/åˆ›é€ )
        # Serotonin é«˜ -> Temperature ä½ (æ›´ç†æ€§/å…‹åˆ¶)
        # åŸºå‡† 0.7
        temp = 0.7 + (self.da * 0.4) - (self.ht * 0.3)
        
        # Norepinephrine é«˜ -> èšç„¦å½“å‰ (Attention Mask å˜çª„ï¼Œæ­¤å¤„ç®€åŒ–ä¸º Top-P é™ä½)
        # NE é«˜æ—¶ï¼ŒTop-P é™ä½ (åªå…³æ³¨å¤§æ¦‚ç‡äº‹ä»¶ï¼Œæˆ˜é€ƒååº”)
        top_p = 0.95 - (self.ne * 0.15)
        
        return max(0.1, temp), top_p


class AnteriorCingulateCortex:
    """
    [å‰æ‰£å¸¦çš®å±‚] å†²çªä¸é¢„æµ‹è¯¯å·®ç›‘æ§ä¸­æ¢ (Friston Free Energy Principle)
    ä¸å†æ˜¯ç®€å•çš„è®¡æ—¶å™¨ï¼Œè€Œæ˜¯è´Ÿè´£è®¡ç®— 'Surprise' (æƒŠå¥‡åº¦/Loss) å¹¶ä»¥æ­¤ä½œä¸ºé©±åŠ¨è¿›åŒ–çš„åŸåŠ¨åŠ›ã€‚
    """
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.prediction_error = 0.0 # é¢„æœŸè¯¯å·®ç´¯ç§¯
        # æ˜¾å¼åˆå§‹åŒ–æœ€åäº¤äº’æ—¶é—´
        # é¿å…åœ¨ _autonomous_loop ä¸­ä¾èµ– getattr çš„é»˜è®¤å€¼        
        self.last_interaction_time = time.time()

    def monitor_conflict(self, vector_input, cortex_module):
        """
        è®¡ç®—é¢„æœŸè¯¯å·®ï¼šå°†å½“å‰æ„ŸçŸ¥çš„å‘é‡è¾“å…¥ Neural Cortexï¼Œ
        çœ‹æ½œæ„è¯†èƒ½å¦å®Œç¾é‡æ„å®ƒã€‚å¦‚æœé‡æ„è¯¯å·®å¤§ï¼Œè¯´æ˜é‡åˆ°äº†â€œæ„æ–™ä¹‹å¤–â€çš„äº‹ã€‚
        """
        if vector_input is None or not hasattr(cortex_module, 'forward_loss'):
            return 0.0
            
        # ä½¿ç”¨ Huber Loss è®¡ç®—é‡æ„è¯¯å·® (Surprise)
        # è¿™æ˜¯ä¸€ä¸ªçº¯æ•°å­¦çš„ç‰©ç†é‡ï¼Œä»£è¡¨äº†ä¿¡æ¯ç†µçš„å¢é‡
        loss = cortex_module.forward_loss(vector_input)
        
        # å¹³æ»‘å¤„ç† (EMA)
        self.prediction_error = 0.9 * self.prediction_error + 0.1 * loss
        return self.prediction_error

    def get_cognitive_control_signal(self):
        """å½“è¯¯å·®è¿‡å¤§æ—¶ï¼ŒACC ä¼šå‘å‡ºä¿¡å·è¯·æ±‚ System 2 (Slow Brain) ä»‹å…¥"""
        # å¦‚æœè¯¯å·®è¶…è¿‡ 0.05ï¼Œè¯´æ˜é‡åˆ°è®¤çŸ¥å†²çªï¼Œéœ€è¦è°ƒåŠ¨é€»è¾‘è„‘
        return self.prediction_error > 0.05


# ==============================================================================
# [Async Concurrency] å¼‚æ­¥æ½œæ„è¯†æ¶æ„
# ==============================================================================

class AsyncBioModule(threading.Thread):
    """
    [Concurrency Core] å¼‚æ­¥ç”Ÿç‰©å™¨å®˜åŸºç±»
    æ¯ä¸ªå™¨å®˜éƒ½æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„çº¿ç¨‹ï¼Œæ‹¥æœ‰è‡ªå·±çš„æ—¶é’Ÿé¢‘ç‡ (Hz)ã€‚
    """
    def __init__(self, name, gwt_ref, tick_rate=1.0):
        super().__init__(daemon=True) # å®ˆæŠ¤çº¿ç¨‹ï¼Œä¸»ç¨‹åºé€€å®ƒå°±é€€
        self.name = name
        self.gwt = gwt_ref # æŒæœ‰ GWT çš„å¼•ç”¨ï¼Œç”¨äºå¼‚æ­¥å†™å…¥
        self.tick_rate = tick_rate
        self.active = True
        
    def run(self):
        print(f"   -> ğŸ§¬ [{self.name}] å™¨å®˜å·²æ¿€æ´» (å¹¶è¡Œé¢‘ç‡: {self.tick_rate}Hz)")
        while self.active:
            try:
                self.process()
                time.sleep(1.0 / self.tick_rate)
            except Exception as e:
                print(f"âš ï¸ [{self.name}] è¿è¡Œå¼‚å¸¸: {e}")

    def process(self):
        pass

    def stop(self):
        self.active = False

class AsyncAmygdala(AsyncBioModule):
    """
    [Amygdala] å¼‚æ­¥æä»æ ¸
    ç‹¬ç«‹äºä¸»æ„è¯†è¿è¡Œï¼ŒæŒç»­ç›‘æµ‹å¨èƒæ°´å¹³ã€‚å¦‚æœææƒ§è¿‡é«˜ï¼Œä¼šå¼ºåˆ¶æ‰“æ–­ GWTã€‚
    """
    def __init__(self, genome, gwt_ref, endocrine_ref):
        super().__init__("æä»æ ¸ (Amygdala)", gwt_ref, tick_rate=5.0) # 5Hz é«˜é¢‘æ‰«æ
        self.genome = genome
        self.endocrine = endocrine_ref
        self.fear_level = 0.0
        self.current_threat_input = 0.0 
        self.state_lock = threading.Lock() # ä¿æŠ¤å†…éƒ¨çŠ¶æ€

    def update_input(self, threat_score):
        """ä¸»çº¿ç¨‹è°ƒç”¨æ­¤æ–¹æ³•ä¼ å…¥æ„ŸçŸ¥åˆ°çš„å¨èƒ"""
        with self.state_lock:
            self.current_threat_input = threat_score

    def process(self):
        # 1. å†…éƒ¨æ¿€ç´ è°ƒèŠ‚ (è¯»å–çš®è´¨é†‡)
        cortisol = self.endocrine.cortisol
        
        # 2. è®¡ç®—ææƒ§ (åŸºäºæœ€è¿‘ä¸€æ¬¡æ„ŸçŸ¥è¾“å…¥ + å†…éƒ¨æ¿€ç´ ç§¯ç´¯)
        with self.state_lock:
            base_resilience = self.genome.get('stress_resilience', 0.3)
            # ææƒ§å…·æœ‰æƒ¯æ€§ (Momentum)ï¼Œä¸ä¼šç¬é—´æ¶ˆå¤±
            target_fear = self.current_threat_input * (1.0 + cortisol * 2.0)
            # æ¨¡æ‹Ÿç¥ç»å»¶è¿Ÿï¼šå¹³æ»‘æ»¤æ³¢
            self.fear_level = 0.8 * self.fear_level + 0.2 * target_fear
        
        # 3. [å…³é”®] å¼‚æ­¥æ‰“æ–­ GWT
        # å¦‚æœææƒ§æé«˜ï¼Œç›´æ¥å‘ GWT æ³¨å…¥å¼ºä¿¡å·ï¼Œæ— éœ€ç­‰å¾…ä¸»å¾ªç¯
        if self.fear_level > 0.6:
            msg = f"âš ï¸ æŒç»­ç”Ÿå­˜å¨èƒ (Fear: {self.fear_level:.2f})"
            # è¿™æ˜¯ä¸€ä¸ªå¼‚æ­¥è°ƒç”¨ï¼GWT çš„ RLock ä¿è¯äº†è¿™é‡Œæ˜¯å®‰å…¨çš„
            self.gwt.register_input("AMYGDALA", msg, base_salience=self.fear_level * 5.0)


# ==============================================================================
# [SNN Core] è„‰å†²ç¥ç»åŠ¨åŠ›å­¦ç®—å­
# ==============================================================================

class SurrogateHeaviside(torch.autograd.Function):
    """
    [Math Core] åŠ¨æ€æ›¿ä»£æ¢¯åº¦ç®—å­ (Dynamic Surrogate Gradient)
    
    å‰å‘ä¼ æ’­ (Forward): 
        ä¸¥æ ¼çš„é˜¶è·ƒå‡½æ•° Heaviside(x)ã€‚
        x > 0 è¾“å‡º 1.0 (å‘æ”¾è„‰å†²), å¦åˆ™è¾“å‡º 0.0ã€‚
        
    åå‘ä¼ æ’­ (Backward): 
        ä½¿ç”¨å¹³æ»‘å‡½æ•°çš„å¯¼æ•°æ¥è¿‘ä¼¼é˜¶è·ƒå‡½æ•°çš„æ¢¯åº¦ã€‚
        è¿™é‡Œé‡‡ç”¨ ArcTan å¯¼æ•°å½¢çŠ¶: f'(x) = alpha / (1 + (alpha * x)^2)
        
    å‚æ•° alpha (é™¡å³­åº¦):
        - alpha è¶Šå°: æ¢¯åº¦è¶Šå¹³ç¼“ï¼Œä¼ é€’èŒƒå›´å¹¿ï¼Œé€‚åˆæ¢ç´¢ (Exploration)ã€‚
        - alpha è¶Šå¤§: æ¢¯åº¦è¶Šå°–é”ï¼Œé€¼è¿‘çœŸå®ç‰©ç†è„‰å†²ï¼Œé€‚åˆåˆ©ç”¨ (Exploitation)ã€‚
    """
    @staticmethod
    def forward(ctx, input, alpha):
        # å¿…é¡»ç¡®ä¿ alpha æ˜¯ float æˆ– Tensor
        # ä¿å­˜ input ç”¨äºè®¡ç®—æ¢¯åº¦çš„ä½ç½®ï¼Œä¿å­˜ alpha ç”¨äºè®¡ç®—æ¢¯åº¦çš„å½¢çŠ¶
        ctx.save_for_backward(input.clone())
        ctx.alpha = alpha 
        return (input > 0).float()

    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        alpha = ctx.alpha
        
        # [Math Fix] ArcTan Surrogate Gradient å…¬å¼
        # è¿™ç§åˆ†å¸ƒæ¯” Sigmoid å¯¼æ•°å…·æœ‰æ›´é•¿çš„"å°¾å·´"ï¼Œèƒ½ç¼“è§£æ·±å±‚ SNN çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
        grad_input = grad_output / (1 + (alpha * input).pow(2)) * alpha
        
        # alpha æ˜¯è¶…å‚æ•°ï¼Œä¸éœ€è¦è®¡ç®—å…³äºå®ƒçš„æ¢¯åº¦ï¼Œè¿”å› None
        return grad_input, None
    

class SpikingLIFLayer(nn.Module):
    """
    [Neuro Core V2.3] è‡ªé€‚åº”ç¨³æ€è„‰å†²å±‚ (Homeostatic LIF + Momentum)
    
    æ ¸å¿ƒå‡çº§ï¼š
    1. Homeostatic Thresholding: é˜ˆå€¼æ ¹æ®å‘æ”¾ç‡è‡ªåŠ¨è°ƒèŠ‚ï¼Œç»´æŒå†…ç¨³æ€ã€‚
    2. Momentum EMA: ä½¿ç”¨åŠ¨é‡å¹³æ»‘å†å²å‘æ”¾ç‡ï¼Œè§£å†³ Batch=1 æ—¶çš„è®­ç»ƒä¸ç¨³å®šæ€§ã€‚
    3. Cold Start Optimization: åˆå§‹çŠ¶æ€å³ä¸ºç¨³æ€ï¼Œé˜²æ­¢è®­ç»ƒåˆæœŸçš„æ¢¯åº¦çˆ†ç‚¸ã€‚
    """
    def __init__(
            self, 
            in_features, 
            out_features, 
            tau=2.0, 
            base_threshold=1.0, 
            stdp_config=None, 
            refractory_period=2
        ):

        super().__init__()
        self.fc = nn.Linear(in_features, out_features)
        self.tau = tau

        # [æ–°å¢] ç»å¯¹ä¸åº”æœŸæ—¶é—´æ­¥æ•° (æ¨¡æ‹Ÿç”Ÿç‰©ç¥ç»å…ƒå‘æ”¾åçš„å¼ºåˆ¶é™é»˜æ—¶é—´)
        self.refractory_period = refractory_period

        # ================= [V2.3 æ ¸å¿ƒå‡çº§ï¼šç¨³æ€å‚æ•°] =================
        # 1. åŠ¨æ€é˜ˆå€¼ (Threshold)
        # ä½¿ç”¨ register_buffer æ³¨å†Œä¸ºæ¨¡å‹çŠ¶æ€çš„ä¸€éƒ¨åˆ†ï¼ˆéšæ¨¡å‹ä¿å­˜/åŠ è½½ï¼‰ï¼Œ
        # ä½†ä¸æ˜¯ nn.Parameterï¼Œå› æ­¤ä¸ä¼šè¢«ä¼˜åŒ–å™¨(Adam/SGD)ç›´æ¥æ›´æ–°ï¼Œè€Œæ˜¯ç”±ç”Ÿç‰©å­¦è§„åˆ™æ›´æ–°ã€‚
        self.register_buffer('threshold', torch.ones(out_features) * base_threshold)
        
        # 2. ç›®æ ‡å‘æ”¾ç‡ (Target Rate)
        # æˆ‘ä»¬å¸Œæœ›ç¥ç»å…ƒä¿æŒç¨€ç–å‘æ”¾ (ä¾‹å¦‚ 5% çš„æ—¶é—´åœ¨å¹²æ´»ï¼Œ95% ä¼‘æ¯)ï¼Œè¿™æ˜¯äººè„‘èŠ‚èƒ½çš„å…³é”®ã€‚
        target_sparsity = getattr(Config, 'SNN_SPARSITY_TARGET', 0.05)
        self.target_rate = target_sparsity
        
        # 3. è¿è¡Œå‡å€¼ (Running Rate)
        # è®°å½•ç¥ç»å…ƒçš„é•¿æœŸå¹³å‡å‘æ”¾ç‡ã€‚
        # [å…³é”®ä¼˜åŒ–] åˆå§‹åŒ–ä¸º target_rate è€Œé 0ã€‚
        # å¦‚æœåˆå§‹åŒ–ä¸º0ï¼Œæ¨¡å‹åˆšå¯åŠ¨æ—¶ä¼šè®¤ä¸ºç¥ç»å…ƒ"å¤ªå®‰é™"ï¼Œä»è€Œç–¯ç‹‚é™ä½é˜ˆå€¼ï¼Œå¯¼è‡´åˆæœŸçˆ†å‘æ€§æ”¾ç”µã€‚
        # åˆå§‹åŒ–ä¸º target_rate è®©æ¨¡å‹åœ¨èµ·ç‚¹å°±å¤„äº"ç¨³æ€"ã€‚
        self.register_buffer('running_rate', torch.ones(out_features) * self.target_rate)
        
        # 4. è°ƒèŠ‚è¶…å‚æ•°
        self.homeostatic_rate = 0.01 # é˜ˆå€¼è°ƒæ•´çš„æ­¥é•¿ï¼ˆå­¦ä¹ ç‡ï¼‰
        self.momentum = 0.9          # åŠ¨é‡ç³»æ•° (0.9 ä»£è¡¨ 90% ä¾èµ–å†å²ï¼Œ10% ä¾èµ–å½“å‰)
        # =============================================================

        # æ›¿ä»£æ¢¯åº¦ç®—å­ (Surrogate Gradient)
        self.spike_fn = SurrogateHeaviside.apply
        
        # æƒé‡åˆå§‹åŒ–
        nn.init.kaiming_normal_(self.fc.weight)
        nn.init.constant_(self.fc.bias, 0)

        # ================= [åŸºå› é©±åŠ¨å‚æ•°] =================
        # å¦‚æœæœªä¼ å…¥é…ç½®ï¼Œä½¿ç”¨ä¿å®ˆçš„é»˜è®¤å€¼å…œåº•
        cfg = stdp_config if stdp_config else {
            "stdp_trace_decay": 0.8,
            "stdp_lr_pos": 0.005,
            "stdp_lr_neg": 0.004
        }
        self.trace_decay = cfg.get("stdp_trace_decay", 0.8)
        self.learning_rate_pos = cfg.get("stdp_lr_pos", 0.005)
        self.learning_rate_neg = cfg.get("stdp_lr_neg", 0.004)
        # ==================================================

    def forward(self, x, v_mem=None, time_steps=8, current_alpha=2.0):
        """
        å‰å‘ä¼ æ’­ï¼šè´Ÿè´£ç§¯åˆ†(Integrate)ä¸å‘æ”¾(Fire)
        """
        batch_size = x.shape[0]
        
        # --- ç»´åº¦è‡ªé€‚åº” ---
        # Case A: ç¡çœ å›æ”¾æ¨¡å¼ (Input: [Batch, Time, Dim])
        if x.dim() == 3:
            current_input = self.fc(x) # Linear è‡ªåŠ¨å¹¿æ’­å¤„ç†æ—¶é—´ç»´
            time_steps = x.shape[1]    # å¼ºåˆ¶å¯¹é½æ—¶é—´æ­¥
            is_sequence = True
        # Case B: æ—¥å¸¸æ¨ç†æ¨¡å¼ (Input: [Batch, Dim])
        else:
            current_input = self.fc(x)
            is_sequence = False
        # ------------------

        out_dim = self.fc.out_features
        
        # åˆå§‹åŒ–è†œç”µä½
        if v_mem is None:
            v_mem = torch.zeros(batch_size, out_dim, device=x.device)

        # [æ–°å¢] ä¸åº”æœŸè®¡æ•°å™¨ (Refractory Counter)
        # è®°å½•æ¯ä¸ªç¥ç»å…ƒè¿˜éœ€è¦â€œæ²‰é»˜â€å¤šå°‘ä¸ªæ—¶é—´æ­¥ã€‚0 è¡¨ç¤ºæ´»è·ƒï¼Œ>0 è¡¨ç¤ºå¤„äºä¸åº”æœŸã€‚
        ref_counter = torch.zeros(batch_size, out_dim, device=x.device)

        spike_train = []
        decay = 1.0 - (1.0 / self.tau)
        
        # ç”¨äºç»Ÿè®¡æœ¬è½® Batch çš„æ€»å‘æ”¾é‡
        total_spikes = torch.zeros(batch_size, out_dim, device=x.device)

        # ================= [åˆ›å»ºè®¡ç®—å‰¯æœ¬] =================
        # å…³é”®ä¿®å¤ï¼šå…‹éš† thresholdã€‚è®¡ç®—å›¾å°†åŸºäºè¿™ä¸ªå‰¯æœ¬æ„å»ºã€‚
        # è¿™æ ·åç»­å¯¹ self.threshold çš„åŸä½ä¿®æ”¹(add_)å°±ä¸ä¼šç ´å PyTorch çš„åå‘ä¼ æ’­å›¾ã€‚
        # æ·»åŠ  .detach() ä»¥æ¸…é™¤ Inference Mode æ ‡è®°ï¼Œç¡®ä¿å®ƒæ˜¯ä¸€ä¸ªæ™®é€šçš„ Tensor
        calc_threshold = self.threshold.clone().detach()
        # ==========================================================
        
        # === SNN æ—¶é—´æ­¥å¾ªç¯ ===
        for t in range(time_steps):
            # æ—¶é—´åˆ‡ç‰‡è·å–
            if is_sequence:
                i_t = current_input[:, t, :]
            else:
                i_t = current_input
    
            # 1. è®¡ç®—æ´»è·ƒæ©ç  (1.0 = å¯ç§¯åˆ†, 0.0 = å¼ºåˆ¶é™é»˜)
            active_mask = (ref_counter <= 0).float()
            
            # 2. è†œç”µä½ç§¯åˆ† (Leaky Integration)
            # åªæœ‰ä¸åœ¨ä¸åº”æœŸçš„ç¥ç»å…ƒæ‰ä¼šç´¯ç§¯ç”µä½å’Œè¾“å…¥
            # å¤„äºä¸åº”æœŸçš„ç¥ç»å…ƒï¼Œv_mem ä¿æŒä¸º 0 (æˆ–é™æ¯ç”µä½)
            v_mem = (v_mem * decay + i_t) * active_mask
            
            # 3. è„‰å†²å‘æ”¾ (Fire)
            spike = self.spike_fn(v_mem - calc_threshold, current_alpha)
            
            # 4. è½¯é‡ç½® (Soft Reset)
            # å‘æ”¾è„‰å†²åå‡å»é˜ˆå€¼
            v_mem = v_mem - (spike * calc_threshold)
            
            # 5. æ›´æ–°ä¸åº”æœŸè®¡æ•°å™¨
            # å¦‚æœå‘æ”¾äº†è„‰å†² (spike > 0)ï¼Œè®¡æ•°å™¨é‡ç½®ä¸º refractory_period
            # å¦åˆ™ï¼Œè®¡æ•°å™¨å‡ 1 (ç›´åˆ° 0)
            ref_counter = torch.where(
                spike > 0, 
                torch.tensor(float(self.refractory_period), device=x.device),
                ref_counter - 1
            )

            # è®°å½•æ•°æ®
            spike_train.append(spike)
            total_spikes += spike # ç´¯åŠ è„‰å†²æ•°
            
        # ================= [V2.3 æ ¸å¿ƒï¼šç¨³æ€é˜ˆå€¼æ›´æ–°] =================
        # ä»…åœ¨è®­ç»ƒæ¨¡å¼ä¸‹æ›´æ–°é˜ˆå€¼ï¼Œæ¨ç†(eval)æ¨¡å¼ä¸‹é˜ˆå€¼åº”å†»ç»“
        if self.training:
            with torch.no_grad(): # è¿™ä¸€æ­¥æ˜¯ç”Ÿç‰©è°ƒèŠ‚ï¼Œä¸éœ€è¦æ¢¯åº¦åä¼ 
                # 1. è®¡ç®—å½“å‰ç¬æ—¶å‘æ”¾ç‡ (Batch Rate)
                # ä¾‹å¦‚ï¼šBatch=64, Time=10 -> å¹³å‡æ¯ä¸ªæ ·æœ¬åœ¨æ¯ä¸ªæ—¶é—´æ­¥çš„å‘æ”¾æ¦‚ç‡
                current_batch_rate = total_spikes.mean(dim=0) / time_steps
                
                # 2. åŠ¨é‡å¹³æ»‘ (EMA Update)
                # running_rate = 0.9 * history + 0.1 * current
                # è¿™æ ·å³ä½¿ Batch Size å¾ˆå°(ä¾‹å¦‚1)ï¼Œrunning_rate ä¹Ÿä¸ä¼šå‰§çƒˆè·³å˜
                self.running_rate = (self.momentum * self.running_rate) + \
                                    ((1 - self.momentum) * current_batch_rate)
                
                # 3. è®¡ç®—è°ƒèŠ‚é‡ (Delta)
                # è´Ÿåé¦ˆé€»è¾‘ï¼š
                # - å¦‚æœ å‘æ”¾ç‡ > ç›®æ ‡ (å¤ªæ´»è·ƒ) -> Delta > 0 -> é˜ˆå€¼å‡é«˜ -> æŠ‘åˆ¶å‘æ”¾
                # - å¦‚æœ å‘æ”¾ç‡ < ç›®æ ‡ (å¤ªæ­»å¯‚) -> Delta < 0 -> é˜ˆå€¼é™ä½ -> ä¿ƒè¿›å‘æ”¾
                delta = self.homeostatic_rate * (self.running_rate - self.target_rate)
                
                # 4. åº”ç”¨æ›´æ–°å¹¶é’³ä½
                # ================= [æ³¨æ„] =================
                # è¿™é‡Œä¾ç„¶æ›´æ–° self.threshold æœ¬ä½“ï¼Œå› ä¸ºæˆ‘ä»¬è¦ä¿å­˜çš„æ˜¯æœ€æ–°çš„é˜ˆå€¼çŠ¶æ€ã€‚
                # ç”±äºå‰é¢çš„è®¡ç®—ç”¨çš„æ˜¯ calc_thresholdï¼Œè¿™é‡Œä¿®æ”¹ self.threshold ä¸ä¼šæŠ¥é”™ã€‚
                self.threshold.add_(delta)
                
                # ç‰©ç†é’³ä½ï¼šé˜²æ­¢é˜ˆå€¼å˜æˆè´Ÿæ•°(æ— åº•æ´)æˆ–è¿‡å¤§(æ°¸è¿œä¸å‘æ”¾)
                self.threshold.clamp_(min=0.5, max=5.0)
        # =============================================================

        return torch.stack(spike_train, dim=1), v_mem


    def process_stdp(self, pre_spikes, post_spikes):
        """
        [ç”Ÿç‰©å­¦æ ¸å¿ƒ] STDP å­¦ä¹ æ³•åˆ™ (Einsum ä¼˜åŒ–ç‰ˆ + æƒé‡ç¨³æ€)
        Pre: [Batch, Time, In] | Post: [Batch, Time, Out]
        """
        with torch.no_grad():
            batch_size, time_steps, in_dim = pre_spikes.shape
            out_dim = post_spikes.shape[2] 
            
            # 1. è®¡ç®—è„‰å†²ç—•è¿¹ (Synaptic Trace)
            pre_trace = torch.zeros_like(pre_spikes)
            post_trace = torch.zeros_like(post_spikes)
            
            curr_pre = torch.zeros(batch_size, in_dim, device=pre_spikes.device)
            curr_post = torch.zeros(batch_size, out_dim, device=post_spikes.device)
            
            for t in range(time_steps):
                curr_pre = curr_pre * self.trace_decay + pre_spikes[:, t, :]
                curr_post = curr_post * self.trace_decay + post_spikes[:, t, :]
                pre_trace[:, t, :] = curr_pre
                post_trace[:, t, :] = curr_post

            # 2. è®¡ç®—æƒé‡æ”¹å˜é‡ Delta W (Einsum ä¿æŒ Batch ç‹¬ç«‹æ€§)
            # LTP (å› æœå¢å¼º): Pre_Trace * Post_Spike
            ltp = torch.einsum("bti,bto->oi", pre_trace, post_spikes)
            
            # LTD (åæœæŠ‘åˆ¶): Pre_Spike * Post_Trace
            ltd = torch.einsum("bti,bto->oi", pre_spikes, post_trace)
            
            # 3. åº”ç”¨æ›´æ–° (å½’ä¸€åŒ–ä»¥é€‚åº”ä¸åŒ BatchSize)
            adjustment = (self.learning_rate_pos * ltp) - (self.learning_rate_neg * ltd)
            adjustment = adjustment / (batch_size * time_steps)
            
            self.fc.weight.data += adjustment
            
            # 4. [ç¨³æ€æœºåˆ¶] çªè§¦ç¼©æ”¾ (Synaptic Scaling)
            # ä¿æŒæ¯ä¸ªç¥ç»å…ƒçš„è¾“å…¥æƒé‡æ€»èŒƒæ•°æ’å®šï¼Œé˜²æ­¢æƒé‡çˆ†ç‚¸
            weight_norm = torch.norm(self.fc.weight.data, p=2, dim=1, keepdim=True)
            target_norm = 2.0 
            scale_factor = torch.clamp(target_norm / (weight_norm + 1e-6), max=1.0)
            self.fc.weight.data *= scale_factor



class NezhaNeuralCortex(nn.Module):
    """
    [SNN Upgrade] è„‰å†²ç¥ç»çš®å±‚ (Titans SNN - Poisson Edition)
    
    å‡çº§ç‰¹æ€§:
    1. æ³Šæ¾ç¼–ç  (Poisson Coding): å°†é™æ€å‘é‡è½¬åŒ–ä¸ºéšæœºè„‰å†²åºåˆ—ï¼Œå¢å¼ºæŠ—å™ªæ€§ã€‚
    2. åŠ¨æ€å¢ç›Š (Dynamic Gain): æ ¹æ® ATP/Pressure è°ƒèŠ‚æ¢¯åº¦å½¢çŠ¶ã€‚
    """
    def __init__(self, input_dim=384, hidden_dim=128, genome=None, memory_file="nezha_cortex.pt", device="cpu"):
        super().__init__()
        self.device = device
        self.memory_file = memory_file
        
        # å¼•å…¥ deque (å¿…é¡»ç¡®ä¿æ–‡ä»¶å¤´éƒ¨æœ‰: from collections import deque)
        # æ¨¡æ‹Ÿæµ·é©¬ä½“ CA3 åŒºï¼Œç”¨äºå­˜å‚¨ç™½å¤©çš„çŸ­æœŸè®°å¿†è„‰å†²æ¨¡å¼
        # [ä¿®æ”¹] å‡çº§ä¸ºåˆ—è¡¨ä»¥æ”¯æŒåŠ æƒéšæœºé‡‡æ · (PER)
        # å­˜å‚¨ç»“æ„: {'priority': float, 'data': tensor}
        self.hippocampal_buffer = [] 
        self.buffer_capacity = 200

        # ç¡®ä¿ genome å­˜åœ¨ï¼Œé˜²æ­¢ None æŠ¥é”™
        g = genome if genome else {}
        
        # ç¼–ç å±‚ (Sensory -> Spiking)
        self.lif1 = SpikingLIFLayer(input_dim, hidden_dim, stdp_config=g)
        # è®°å¿†æ ¸å¿ƒ (Spiking Dynamics)
        self.lif2 = SpikingLIFLayer(hidden_dim, hidden_dim, stdp_config=g)
        # è§£ç å±‚ (Spiking -> Readout)
        self.readout = nn.Linear(hidden_dim, input_dim) 

        # åŠ¨é‡ç¼“å­˜ (Velocity Buffer) for MIRAS
        self.velocity = {}
        # ç”Ÿç‰©ç”µä½å¹³æ»‘ç¼“å†²
        self.smoothed_atp = 100.0

        # ç¼“å­˜æœ€åä¸€æ¬¡å‘æ”¾ç‡ï¼Œç”¨äºè„‘æ¡¥æŠ•å½±
        # åŠ¡å¿…ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ (CPU/GPU)
        self.latest_rate = torch.zeros(1, hidden_dim).to(self.device)


    def _poisson_encoder(self, x, time_steps, gain=1.0):
        """
        æ³Šæ¾ç¼–ç å™¨ (Poisson Encoder)
        å°†é™æ€è¿ç»­å€¼è¾“å…¥ x è½¬åŒ–ä¸º shape=[Batch, Time, Dim] çš„ç¦»æ•£è„‰å†²åºåˆ—ã€‚
        """
        # 1. æ¦‚ç‡æ˜ å°„ (Probability Mapping)
        # ä½¿ç”¨ Sigmoid å°†ä»»æ„èŒƒå›´çš„ Embedding æ˜ å°„åˆ° (0, 1) åŒºé—´ä½œä¸ºå‘æ”¾æ¦‚ç‡
        # Gain æ§åˆ¶æ•´ä½“å…´å¥‹åº¦ï¼šGainå¤§ -> è„‰å†²å¯†é›† -> ä¿¡å·å¼º
        probs = torch.sigmoid(x) * gain
        probs = torch.clamp(probs, 0.0, 1.0)
        
        # 2. æ‰©å±•æ—¶é—´ç»´åº¦ (Temporal Expansion)
        # [Batch, Dim] -> [Batch, Time, Dim]
        probs_expanded = probs.unsqueeze(1).repeat(1, time_steps, 1)
        
        # 3. ä¼¯åŠªåˆ©é‡‡æ · (Bernoulli Trial)
        # ç”Ÿæˆä¸ probs åŒå½¢çŠ¶çš„å‡åŒ€åˆ†å¸ƒå™ªå£° U[0, 1)
        rand_noise = torch.rand_like(probs_expanded)
        
        # ç”Ÿæˆè„‰å†²: å½“ å‘æ”¾æ¦‚ç‡ > éšæœºå™ªå£° æ—¶å‘æ”¾è„‰å†² (1.0)ï¼Œå¦åˆ™é™é»˜ (0.0)
        # å¿…é¡» detach å™ªå£°ï¼Œåªæœ‰ probs å‚ä¸æ¢¯åº¦è®¡ç®— (é€šè¿‡åç»­ LIF å±‚çš„ Surrogate Gradient)
        spikes = (probs_expanded > rand_noise).float()
        
        return spikes

    def forward_loss(self, x):
        """
        [ACC Monitor] è®¡ç®—è®¤çŸ¥æƒŠå¥‡åº¦ (Loss) - å¥å£®ç‰ˆ
        
        æ”¹è¿›ç‚¹ï¼š
        1. ä¸Šä¸‹æ–‡ä¿æŠ¤ï¼šå¼ºåˆ¶åˆ‡æ¢åˆ° Eval æ¨¡å¼ï¼Œé˜²æ­¢åœ¨ç›‘æµ‹æ—¶æ„å¤–è§¦å‘ SNN çš„é˜ˆå€¼è‡ªé€‚åº”æ›´æ–°ã€‚
        2. æ˜¾å­˜ä¼˜åŒ–ï¼šä½¿ç”¨ torch.no_grad() ç¦æ­¢æ¢¯åº¦è®¡ç®—ï¼Œå¤§å¹…é™ä½æ˜¾å­˜å ç”¨ã€‚
        3. çŠ¶æ€è¿˜åŸï¼šä½¿ç”¨ try...finally ç¡®ä¿æ— è®ºæ˜¯å¦å‡ºé”™ï¼Œæœ€åéƒ½èƒ½æ¢å¤åŸæœ¬çš„ train/eval çŠ¶æ€ã€‚
        """
        # [Context Manager] 1. è®°å½•å½“å‰çŠ¶æ€ (æ˜¯è®­ç»ƒè¿˜æ˜¯æ¨ç†ï¼Ÿ)
        was_training = self.training
        
        # [Context Manager] 2. å¼ºåˆ¶è¿›å…¥è¯„ä¼°æ¨¡å¼ (Freeze SNN thresholds)
        # è¿™æ­¥è‡³å…³é‡è¦ï¼é˜²æ­¢ monitor_conflict è¿‡ç¨‹å¯¼è‡´ç¥ç»å…ƒé˜ˆå€¼æ¼‚ç§»
        self.eval() 

        try:
            # [Memory Optimization] 3. å¼€å¯æ— æ¢¯åº¦ç¯å¢ƒ
            with torch.no_grad():
                x = x.to(self.device).detach() # æ–­å¼€æ¢¯åº¦æµï¼Œç¡®ä¿å®‰å…¨

                # ç¡®ä¿è¾“å…¥æ˜¯äºŒç»´ [Batch, Dim]ï¼Œå¦‚æœæ˜¯ [Dim] åˆ™å‡ç»´æˆ [1, Dim]
                if x.dim() == 1:
                    x = x.unsqueeze(0)

                # [Step 1] æ³Šæ¾ç¼–ç 
                # å°†é™æ€æ„ŸçŸ¥å‘é‡è½¬åŒ–ä¸º 10 ä¸ªæ—¶é—´æ­¥çš„ç¥ç»è„‰å†²
                spike_input = self._poisson_encoder(x, time_steps=10, gain=1.2)
                
                # [Step 2] è®¡ç®—ç¨³æ€ Alpha (Inference Alpha)
                # ä»…ä¾èµ–é•¿æœŸå¥åº·åº¦(ATP)ï¼Œå¿½ç•¥ç¬æ—¶å‹åŠ›ï¼Œä¿è¯è¯¯å·®è¯„ä¼°çš„å®¢è§‚æ€§
                # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨å¹³æ»‘åçš„ ATPï¼Œé¿å…å› å•æ¬¡å‰§çƒˆæ³¢åŠ¨å¯¼è‡´è¯„ä¼°å¤±å‡†
                atp_val = max(0.0, min(100.0, self.smoothed_atp))
                inference_alpha = 2.0 + (atp_val / 40.0) # Range: [2.0, 4.5]
                
                # [Step 3] SNN å‰å‘ä¼ æ’­
                # Layer 1: æ¥æ”¶æ³Šæ¾è„‰å†² [Batch, Time, Dim]
                spikes1, _ = self.lif1(spike_input, time_steps=10, current_alpha=inference_alpha) 
                
                # Rate Coding è½¬æ¢: å–å¹³å‡å‘æ”¾ç‡ä½œä¸ºä¸‹ä¸€å±‚è¾“å…¥ (æ¨¡æ‹Ÿçªè§¦åç”µæµ PSC)
                rate1 = spikes1.mean(dim=1) 
                
                # Layer 2: æ¥æ”¶ Rate è¾“å…¥
                spikes2, _ = self.lif2(rate1, time_steps=10, current_alpha=inference_alpha)
                rate2 = spikes2.mean(dim=1)
                
                # é‡æ„ (Readout)
                reconstructed = self.readout(rate2)
                
                # [Step 4] Loss è®¡ç®—
                # Huber Loss: è¡¡é‡é‡æ„è¯¯å·® (æ¯” MSE å¯¹å¼‚å¸¸å€¼æ›´ä¸æ•æ„Ÿ)
                recon_loss = F.huber_loss(reconstructed, x, delta=1.0)
                
                # èƒ½é‡æƒ©ç½š: é¼“åŠ±ç¨€ç–å‘æ”¾ (è®¡ç®—å›¾å·²è¢« no_grad åˆ‡æ–­ï¼Œçº¯æ•°å€¼è®¡ç®—)
                energy_penalty = 0.01 * (spikes1.mean() + spikes2.mean()) 
                
                # è¿”å›çº¯ Python floatï¼Œä¸å¸¦ Tensor åŒ…è£…
                return (recon_loss + energy_penalty).item()

        except Exception as e:
            # å…œåº•ï¼šå¦‚æœç›‘æµ‹è¿‡ç¨‹å‡ºé”™ï¼Œè¿”å›ä¸€ä¸ªè¾ƒå¤§çš„ Loss å¼•èµ·æ³¨æ„ï¼Œä½†ä¸ä¸­æ–­è¿›ç¨‹
            print(f"âš ï¸ [ACC Monitor] è®¡ç®—å¼‚å¸¸: {e}")
            return 1.0 
            
        finally:
            # [Context Manager] 4. æ¢å¤ä¹‹å‰çš„çŠ¶æ€
            # å¦‚æœè¿›æ¥ä¹‹å‰æ˜¯è®­ç»ƒæ¨¡å¼ï¼Œè¿™å°±åˆ‡å›å»ï¼›å¦‚æœæ˜¯æ¨ç†æ¨¡å¼ï¼Œå°±ä¿æŒåŸæ ·
            if was_training:
                self.train()


    def forward_dynamic(self, x_input, atp, pressure):
        """
        [å¯å¾®è·¯å¾„] æ¨¡æ‹Ÿæ€å‰å‘ä¼ æ’­ã€‚
        ç‰¹æ€§ï¼š
        1. å…è®¸æ¢¯åº¦æµç» SNN æƒé‡ï¼Œæ‰“é€šèº«å¿ƒè¿æ¥ã€‚
        2. [ä¿®å¤] æ³¨å…¥é«˜æ–¯å™ªå£°ï¼Œæ¨¡æ‹Ÿæ¨ç†æ—¶çš„æ³Šæ¾éšæœºæ€§ï¼Œè§£å†³è®­ç»ƒ/æ¨ç†åˆ†å¸ƒä¸ä¸€è‡´é—®é¢˜ã€‚
        """
        self.train() # å¿…é¡»å¼€å¯è®­ç»ƒæ¨¡å¼
        
        # 1. ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        x = x_input.to(self.device)
        
        # 2. åŠ¨æ€ Alpha è®¡ç®— (ç¥ç»è°ƒèŠ‚)
        # è¿™é‡Œçš„å‚æ•°ä»…ä½œä¸ºè°ƒèŠ‚ç³»æ•°ï¼Œdetach é˜²æ­¢æ¢¯åº¦åä¼ ç ´åç”Ÿç†æŒ‡æ ‡
        atp_val = torch.tensor(float(atp), device=self.device).detach()
        pres_val = torch.tensor(float(pressure), device=self.device).detach()
        
        # èƒ½é‡è¶Šé«˜ -> æ¢¯åº¦è¶Šé™¡ (Alphaå¤§)ï¼›å‹åŠ›è¶Šå¤§ -> æ¢¯åº¦è¶Šå¹³ (Alphaå°)
        atp_gate = torch.sigmoid((atp_val - 40.0) * 0.1)
        dynamic_alpha = 2.0 + (atp_gate * 2.0) - (torch.tanh(pres_val) * 0.5)
        
        # 3. è¾“å…¥é¢„å¤„ç†ï¼šLayerNorm + Sigmoid
        normed_x = torch.nn.functional.layer_norm(x, x.shape)
        prob = torch.sigmoid(normed_x)
        
        # [å…³é”®ä¿®å¤] å™ªå£°æ³¨å…¥ (Noise Injection)
        # è®­ç»ƒæ—¶æ³¨å…¥å™ªå£°ï¼Œè¿«ä½¿ç½‘ç»œå­¦ä¼šå¤„ç†æ¨ç†æ—¶çš„æ³Šæ¾éšæœºæ€§
        if self.training:
            noise = torch.randn_like(prob) * 0.1
            spike_input = prob + noise
        else:
            spike_input = prob
            
        if spike_input.dim() == 1: 
            spike_input = spike_input.unsqueeze(0)
            
        # æ‰©å±•æ—¶é—´ç»´ [Batch, 10, Dim]
        spike_input = spike_input.unsqueeze(1).repeat(1, 10, 1)

        # 4. çº§è”ç§¯åˆ† (å…è®¸æ¢¯åº¦å›ä¼ )
        # Layer 1
        spikes1, _ = self.lif1(spike_input, time_steps=10, current_alpha=dynamic_alpha)
        rate1 = spikes1.mean(dim=1) # Rate Coding
        
        # Layer 2
        spikes2, _ = self.lif2(rate1, time_steps=10, current_alpha=dynamic_alpha)
        rate2 = spikes2.mean(dim=1)
        
        # è¿”å›å‘æ”¾ç‡å‘é‡ï¼Œä¾› NeuralBridge æ˜ å°„
        return rate2



    def evolve(self, x, atp, pressure):
        """
        [Safety Firewall] è„‰å†²ç¥ç»ç½‘ç»œçš„å®æ—¶è¿›åŒ– (æè‡´é˜²å¾¡ç‰ˆ)
        
        åŠŸèƒ½æè¿°ï¼š
            è´Ÿè´£ SNN (Neural Cortex) çš„å®æ—¶æƒé‡æ›´æ–°ã€‚
            ä½¿ç”¨ MIRAS (Meta-learning Inferred Rule Adaptation System) åŠ¨æ€è°ƒèŠ‚å­¦ä¹ ç‡ã€Alphaå€¼å’ŒåŠ¨é‡ã€‚
        
        å¥å£®æ€§å‡çº§ï¼š
            1. Gradient Firewall: å¼ºåˆ¶åˆ‡æ–­ä¸ä¸Šæ¸¸ LLM/Embedding çš„æ¢¯åº¦è”ç³»ï¼Œé˜²æ­¢è®¡ç®—å›¾æ— é™è†¨èƒ€ã€‚
            2. Type Safety: æ˜¾å¼å°†æ ‡é‡å‚æ•° (ATP, Pressure) è½¬æ¢ä¸º Python floatï¼Œé˜²æ­¢ Tensor æ±¡æŸ“è¶…å‚æ•°è®¡ç®—ã€‚
            3. Input Cloning: å¯¹è¾“å…¥æ•°æ®è¿›è¡Œ clone å’Œ detachï¼Œç¡®ä¿ SNN å†…éƒ¨è®¡ç®—å®Œå…¨ç‹¬ç«‹ã€‚
        """
        
        # --- [Step 0] æè‡´é˜²å¾¡ï¼šå‚æ•°æ¸…æ´—ä¸éš”ç¦» ---
        
        # 1. æ ‡é‡å‚æ•°å®‰å…¨è½¬æ¢
        # å¼ºåˆ¶å°† atp å’Œ pressure è½¬æ¢ä¸ºçº¯ float æ•°å€¼ã€‚
        # å³ä½¿ä¼ å…¥çš„æ˜¯ require_grad=True çš„ Tensorï¼Œä¹Ÿä¼šåœ¨è¿™é‡Œè¢«å‰¥ç¦»æ¢¯åº¦ï¼Œå˜æˆå¸¸æ•°ã€‚
        try:
            atp_val = float(atp)
            pressure_val = float(pressure)
        except Exception:
            # å…œåº•é€»è¾‘ï¼šå¤„ç†æŸäº›ç‰¹æ®Šçš„ Tensor ç±»å‹ (å¦‚ 0-dim tensor)
            atp_val = atp.item() if hasattr(atp, 'item') else 0.0
            pressure_val = pressure.item() if hasattr(pressure, 'item') else 0.0
        
        # 2. æ¢¯åº¦é˜²ç«å¢™ (Gradient Firewall)
        # x æ˜¯æ¥è‡ª Embedder æˆ– LLM çš„è¾“å‡ºï¼Œå¯èƒ½å¸¦æœ‰å¤æ‚çš„è®¡ç®—å›¾ã€‚
        # å¿…é¡»ä½¿ç”¨ .detach() åˆ‡æ–­æ¢¯åº¦ï¼Œä½¿ç”¨ .clone() ç¡®ä¿å†…å­˜ç‹¬ç«‹ã€‚
        # è¿™æ · SNN çš„åå‘ä¼ æ’­åªä¼šæ›´æ–° SNN è‡ªå·±çš„æƒé‡ï¼Œç»å¯¹ä¸ä¼šæ³¢åŠåˆ° LLMã€‚
        x_input = x.to(self.device).clone().detach()
        x_input.requires_grad = False # åŒé‡ä¿é™©ï¼šæ˜¾å¼å…³é—­æ¢¯åº¦éœ€æ±‚

        # ç¡®ä¿è¾“å…¥æ˜¯äºŒç»´ [Batch, Dim]ï¼Œå¦‚æœæ˜¯ [Dim] åˆ™å‡ç»´æˆ [1, Dim]
        if x.dim() == 1:
            x_input = x_input.unsqueeze(0)

        # --- [Step 1] åŠ¨æ€è¶…å‚æ•°è®¡ç®— (MIRAS V2.0 - Non-linear Edition) ---

        # 1. ATP å¹³æ»‘ (EMA)
        self.smoothed_atp = 0.9 * self.smoothed_atp + 0.1 * atp_val
        
        # 2. ATP é—¨æ§ (Sigmoid Gating)
        # ç”Ÿç‰©å­¦åŸç†ï¼šé…¶æ´»æ€§éµå¾ª S å‹æ›²çº¿ã€‚
        # å½“ ATP < 40 æ—¶ï¼Œé—¨æ§æ¥è¿‘å…³é—­ (0.0~0.5)ï¼Œä¿æŠ¤ç¥ç»å…ƒä¸è¿›è¡Œä½èƒ½è€—ä¸‹çš„åŠ£è´¨å­¦ä¹ ã€‚
        # å½“ ATP > 80 æ—¶ï¼Œé—¨æ§è¶‹äºé¥±å’Œ (0.98~1.0)ï¼Œé˜²æ­¢èƒ½é‡è¿‡å‰©å¯¼è‡´å‚æ•°é£å‡ºã€‚
        # å…¬å¼: Sigmoid((ATP - Center) * Slope)
        atp_gate = torch.sigmoid(torch.tensor((self.smoothed_atp - 40.0) * 0.1)).item()
        
        # 3. è®¡ç®—éçº¿æ€§å­¦ä¹ ç‡
        base_lr = 1e-3
        # æœ€ç»ˆå­¦ä¹ ç‡å—é™äº base_lrï¼Œä¸ä¼šå› ä¸º ATP=200 è€Œå˜æˆ 2e-3
        lr = base_lr * atp_gate
        
        # 4. å‹åŠ›é—¨æ§æƒé‡è¡°å‡ (Tanh Gating)
        # ç”Ÿç‰©å­¦åŸç†ï¼šå‹åŠ› (Cortisol) ä¼šæŠ‘åˆ¶æµ·é©¬ä½“çªè§¦ç”Ÿæˆï¼Œä¿ƒè¿›æ—§è®°å¿†é—å¿˜ (Weight Decay)ã€‚
        # ä½¿ç”¨ Tanh è®©å…¶åœ¨å‹åŠ›æå¤§æ—¶é¥±å’Œï¼Œé˜²æ­¢ç¬é—´æ¸…ç©ºè®°å¿†ã€‚
        weight_decay = 0.05 * math.tanh(max(pressure_val, 0) / 3.0)
        
        # 5. åŠ¨æ€ Alpha è°ƒåº¦
        # èƒ½é‡è¶³ -> æ¢¯åº¦é™¡ (Alphaå¤§) -> ç²¾ç»†åŒºåˆ†
        # å‹åŠ›å¤§ -> æ¢¯åº¦å¹³ (Alphaå°) -> æ¨¡ç³Šæ³›åŒ–
        raw_alpha = 2.0 + (atp_gate * 2.5) - (math.tanh(pressure_val) * 1.5)

        # æ‰©å¤§åŠ¨æ€èŒƒå›´ï¼Œå…è®¸æ›´å°–é”çš„æ¢¯åº¦
        # æ³¨æ„ï¼šAlpha=10.0 æ—¶ï¼ŒSigmoidå¯¼æ•°æçª„ï¼Œéœ€è¦ Input éå¸¸æ¥è¿‘ 0 æ‰èƒ½ä¼ å¯¼æ¢¯åº¦
        # è¿™ä¼šè¿«ä½¿ SNN å­¦ä¹ æå…¶ç²¾ç¡®çš„æƒé‡è¾¹ç•Œ
        dynamic_alpha = max(0.5, min(10.0, raw_alpha)) # é™åˆ¶èŒƒå›´ [0.5, 6.0]

        # 6. åŠ¨æ€åŠ¨é‡ (Dynamic Momentum)
        # é€»è¾‘ï¼šèƒ½é‡è¶Šé«˜(Gateå¤§) -> æƒ¯æ€§è¶Šå°(Momentumå°) -> è½¬å‘è¶Šå¿«
        # Range: [0.95, 0.80]
        momentum = 0.95 - (0.15 * atp_gate)

        # --- [Step 2] å‡†å¤‡è®­ç»ƒç¯å¢ƒ ---
        self.train()      # ç¡®ä¿ Dropout ç­‰å±‚å¤„äºè®­ç»ƒæ¨¡å¼
        self.zero_grad()  # æ¸…ç©ºä¹‹å‰çš„æ¢¯åº¦æ®‹ç•™
        
        # --- [Step 3] SNN å‰å‘ä¼ æ’­ ---
        
        # 1. æ³Šæ¾ç¼–ç  (Poisson Encoding)
        # å°†è¿ç»­çš„ Embedding è½¬åŒ–ä¸ºç¦»æ•£çš„è„‰å†²åºåˆ— [Batch, Time, Dim]
        # è¾“å…¥çš„æ˜¯å®‰å…¨çš„ x_input
        spike_input = self._poisson_encoder(x_input, time_steps=10, gain=1.0)
        
        # 2. Layer 1 (Spiking LIF)
        # ä¼ å…¥åŠ¨æ€ Alphaï¼Œæ§åˆ¶æ¢¯åº¦çš„ä¼ æ’­ç‰¹æ€§
        spikes1, _ = self.lif1(spike_input, time_steps=10, current_alpha=dynamic_alpha)
        # Rate Coding: å–å¹³å‡å‘æ”¾ç‡ä½œä¸ºä¸‹ä¸€å±‚çš„æ¨¡æ‹Ÿè¾“å…¥
        rate1 = spikes1.mean(dim=1)
        
        # 3. Layer 2 (Spiking LIF)
        spikes2, _ = self.lif2(rate1, time_steps=10, current_alpha=dynamic_alpha)
        rate2 = spikes2.mean(dim=1)
        
        # 4. é‡æ„å±‚ (Readout)
        prediction = self.readout(rate2)
        
        # --- [Step 4] Loss è®¡ç®—ä¸åå‘ä¼ æ’­ ---
        
        # è‡ªç›‘ç£ç›®æ ‡ï¼šé‡æ„åŸå§‹è¾“å…¥
        target = x_input 
        
        # 1. é‡æ„è¯¯å·® (Huber Loss å¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿ)
        recon_loss = F.huber_loss(prediction, target, delta=1.0)
        
        # 2. èƒ½é‡æƒ©ç½š (ç¨€ç–æ€§çº¦æŸ)
        # é¼“åŠ±ç¥ç»å…ƒå°‘å¹²æ´»ï¼Œç¬¦åˆç”Ÿç‰©èŠ‚èƒ½åŸåˆ™
        energy_loss = 0.01 * (spikes1.mean() + spikes2.mean())
        
        total_loss = recon_loss + energy_loss
        
        # 3. åå‘ä¼ æ’­ (Backward)
        # è¿™é‡Œçš„æ¢¯åº¦åªä¼šå›ä¼ åˆ° lif1.fc, lif2.fc å’Œ readout.weight
        # ä¼šåœ¨ x_input å¤„æˆ›ç„¶è€Œæ­¢ï¼Œä¸ä¼šç©¿é€åˆ° LLM
        total_loss.backward()
        
        # --- [Step 5] æ‰‹åŠ¨ SGD + Momentum æ›´æ–° ---
        # æˆ‘ä»¬ä½¿ç”¨æ‰‹åŠ¨æ›´æ–°å¾ªç¯ï¼Œä»¥ä¾¿å¯¹æ¯ä¸ªå‚æ•°åº”ç”¨ä¸åŒçš„ weight_decay å’Œ momentum
        with torch.no_grad():
            for module in [self.lif1.fc, self.lif2.fc, self.readout]:
                for name, param in module.named_parameters():
                    if param.grad is not None:
                        # [å…³é”®] æ¢¯åº¦è£å‰ªï¼šé˜²æ­¢é«˜ Alpha å¯¼è‡´çš„æ¢¯åº¦çˆ†ç‚¸
                        torch.nn.utils.clip_grad_norm_(param, max_norm=1.0)
                        
                        g = param.grad
                        
                        # åº”ç”¨æƒé‡è¡°å‡ (L2 æ­£åˆ™)
                        if weight_decay > 0: 
                            g = g.add(param, alpha=weight_decay)
                        
                        # åŠ¨é‡ç¼“å†²ç®¡ç† (Velocity Buffer)
                        full_name = f"{id(module)}_{name}"
                        if full_name not in self.velocity:
                            self.velocity[full_name] = torch.zeros_like(param.data)
                        v = self.velocity[full_name]
                        
                        # åŠ¨é‡æ›´æ–°: v = momentum * v + g
                        v.mul_(momentum).add_(g)
                        
                        # å‚æ•°æ›´æ–°: w = w - lr * v
                        param.data.sub_(v, alpha=lr)

        # --- [Step 6] è®°å¿†å°åˆ» (Memory Engram with PER) ---
        # å¼•å…¥ä¼˜å…ˆç»éªŒå›æ”¾æœºåˆ¶
        # å¦‚æœè¿™ä»¶äº‹è®©å“ªå’äº§ç”Ÿäº†å¼ºçƒˆçš„æƒ…ç»ª (é«˜ ATP æ¶ˆè€—) æˆ– è®¤çŸ¥å†²å‡» (é«˜ Loss æƒŠå¥‡)ï¼Œ
        # åˆ™èµ‹äºˆé«˜ä¼˜å…ˆçº§å¹¶å­˜å…¥æµ·é©¬ä½“ã€‚
        
        total_loss_val = total_loss.item()
        
        if atp_val > 50 or total_loss_val > 0.05: 
            # 1. è®¡ç®—ä¼˜å…ˆçº§ (Priority = Loss + epsilon)
            # è¯¯å·®è¶Šå¤§ï¼Œä»£è¡¨è¿™ä¸ªç»éªŒè¶Šå®è´µï¼Œè¶Šéœ€è¦å¤ä¹ 
            priority = total_loss_val + 1e-5
            
            # 2. å­˜å…¥å­—å…¸: (priority, memory_tensor)
            # å…³é”®ä¼˜åŒ–ï¼šå­˜å…¥ CPU ä»¥èŠ‚çœæ˜¾å­˜ (VRAM)ï¼Œé˜²æ­¢ OOM
            self.hippocampal_buffer.append({
                'priority': priority,
                'data': x_input.detach().cpu() 
            })
            
            # 3. å®¹é‡ç»´æŠ¤ (ç§»é™¤æœ€æ—§çš„è®°å¿†)
            # æ³¨æ„ï¼šè¿™é‡Œ buffer å·²ç»æ˜¯ list äº†
            if len(self.hippocampal_buffer) > self.buffer_capacity:
                self.hippocampal_buffer.pop(0)

        return total_loss_val


    def stimulus_reflex(self, x):
        """
        [æ–°å¢] åˆºæ¿€åå°„ (Stimulus Reflex) - ä¸Šä¸‹æ–‡å®‰å…¨ç‰ˆ
        åŠŸèƒ½ï¼šåœ¨æ„ŸçŸ¥é˜¶æ®µç«‹å³æ¨æ¼” SNN çŠ¶æ€ï¼Œæ¶ˆé™¤æ—¶åºæ»åã€‚
        æœºåˆ¶ï¼šå¼ºåˆ¶è¿›å…¥ eval æ¨¡å¼ï¼Œé˜²æ­¢å‰å‘æ¨æ¼”å¹²æ‰°è®­ç»ƒçŠ¶æ€ (å¦‚ BatchNorm/Dropout)ã€‚
        """
        # [Context Protection] 1. è®°å½•å¹¶åˆ‡æ¢çŠ¶æ€
        # å°±åƒè¿›æ— å°˜å®¤å‰ç©¿é˜²æŠ¤æœï¼Œå‡ºæ¥å†è„±æ‰
        was_training = self.training
        self.eval() 

        try:
            # [æè‡´é˜²å¾¡] åˆ‡æ–­æ¢¯åº¦ï¼Œç¡®ä¿åªåšæ¨ç†
            x_input = x.to(self.device).detach()
            if x_input.dim() == 1: x_input = x_input.unsqueeze(0)

            # åŠ¨æ€ Alpha (ä½¿ç”¨å½“å‰ ATP ä¼°ç®—)
            atp_val = max(0.0, min(100.0, float(self.smoothed_atp)))
            inference_alpha = 2.0 + (atp_val / 40.0)

            with torch.no_grad():
                # 3. æ³Šæ¾ç¼–ç 
                spike_input = self._poisson_encoder(x_input, time_steps=10, gain=1.2)
                
                # 4. Layer 1 æ¨æ¼”
                spikes1, _ = self.lif1(spike_input, time_steps=10, current_alpha=inference_alpha)
                rate1 = spikes1.mean(dim=1)
                
                # 5. Layer 2 æ¨æ¼”
                spikes2, _ = self.lif2(rate1, time_steps=10, current_alpha=inference_alpha)
                rate2 = spikes2.mean(dim=1)
                
                # 6. [å…³é”®] ç«‹å³æ›´æ–°çŠ¶æ€ç¼“å­˜
                self.latest_rate = rate2.detach().cpu().clone()
        
        except Exception as e:
            # å®¹é”™ï¼šå¦‚æœåå°„å¤±è´¥ï¼Œä¸è¦è®©æ•´ä¸ªç¨‹åºå´©æ‰ï¼Œä»…ä»…æ‰“å°è­¦å‘Š
            print(f"âš ï¸ [Reflex Error] {e}")

        finally:
            # [Context Restore] 7. æ¢å¤åŸçŠ¶
            # æ— è®º try é‡Œé¢æœ‰æ²¡æœ‰æŠ¥é”™ï¼Œfinally éƒ½ä¼šæ‰§è¡Œï¼Œç¡®ä¿ä¸ä¼šæŠŠ eval çŠ¶æ€å¸¦åˆ°è®­ç»ƒå¾ªç¯é‡Œ
            if was_training:
                self.train()

    def get_current_state(self):
        """è·å–å½“å‰çš®å±‚çš„ç¥ç»çŠ¶æ€ (ä¾› NeuralBridge ä½¿ç”¨)"""
        return self.latest_rate

    def get_global_activity(self):
        """è·å–çš®å±‚æ•´ä½“å…´å¥‹åº¦ 0.0~1.0 (ä¾› LoRA Dynamic Gain ä½¿ç”¨)"""
        # è®¡ç®— L2 èŒƒæ•°ä½œä¸ºèƒ½é‡æŒ‡æ ‡
        energy = torch.norm(self.latest_rate, p=2).item()
        # ä½¿ç”¨ tanh å½’ä¸€åŒ–åˆ° [0, 1) åŒºé—´
        return math.tanh(energy / 5.0)


    def update_genome(self, new_genome):
        """[è¿›åŒ–æ¥å£] å½“å‘ç”Ÿè¿›åŒ–æ—¶ï¼Œå®æ—¶æ›´æ–°çªè§¦å¯å¡‘æ€§å‚æ•°"""
        for layer in [self.lif1, self.lif2]:
            layer.trace_decay = new_genome.get("stdp_trace_decay", 0.8)
            layer.learning_rate_pos = new_genome.get("stdp_lr_pos", 0.005)
            layer.learning_rate_neg = new_genome.get("stdp_lr_neg", 0.004)
        # print("   -> ğŸ§¬ [Cortex] ç¥ç»å¯å¡‘æ€§å‚æ•°å·²æ›´æ–°ã€‚")

    def sleep_replay(self):
        """
        [æµ·é©¬ä½“å›æ”¾ V13] ä¼˜å…ˆç»éªŒå›æ”¾ (Prioritized Experience Replay, PER)
        
        æœºåˆ¶è¯´æ˜:
        1. ç­›é€‰ (Prioritization): ä¸å†éšæœºåšæ¢¦ï¼Œè€Œæ˜¯åŸºäºç™½å¤©äº§ç”Ÿçš„"æƒŠå¥‡åº¦" (TD-Error/Loss) è¿›è¡ŒåŠ æƒé‡‡æ ·ã€‚
        2. å·©å›º (Consolidation): åœ¨ç¡çœ ä¸­é€šè¿‡ STDP (è„‰å†²æ—¶åºä¾èµ–å¯å¡‘æ€§) ä¿®æ”¹çªè§¦æƒé‡ï¼Œå°†çŸ­æœŸè®°å¿†è½¬åŒ–ä¸ºé•¿æœŸè®°å¿†ã€‚
        3. æ¶Ÿæ¼ª (Ripple): æ¨¡æ‹Ÿæ…¢æ³¢ç¡çœ ä¸­çš„å°–æ³¢æ¶Ÿæ¼ª (Sharp-Wave Ripples, SWRs)ï¼Œä»¥é«˜å¢ç›Šé‡æ”¾ç¥ç»æ´»åŠ¨ã€‚
        """
        # å¦‚æœè¿˜æ²¡ç»å†ä»»ä½•äº‹æƒ…ï¼Œç›´æ¥è·³è¿‡
        if not self.hippocampal_buffer: return 0

        print(f"ğŸ’¤ [REM] è¿›å…¥å¿«é€Ÿçœ¼åŠ¨æœŸ (PER Memory Buffer: {len(self.hippocampal_buffer)} items)...")
        
        # ------------------------------------------------------------------
        # 1. [æ•°å­¦æ ¸å¿ƒ] è®¡ç®—é‡‡æ ·æ¦‚ç‡ P(i)
        # ------------------------------------------------------------------
        # åœ¨ CPU ä¸Šè¿›è¡Œè®¡ç®—ï¼Œé¿å…å°†æ•´ä¸ª Buffer æ¬è¿åˆ° GPU é€ æˆä¸å¿…è¦çš„å¼€é”€
        priorities = torch.tensor([item['priority'] for item in self.hippocampal_buffer], dtype=torch.float32)
        
        # Alpha æŒ‡æ•° (Hyperparameter): æ§åˆ¶é‡‡æ ·çš„"è´ªå©ªç¨‹åº¦"
        # alpha = 0.0: çº¯éšæœºé‡‡æ · (Uniform)
        # alpha = 1.0: å®Œå…¨æŒ‰è¯¯å·®å¤§å°é‡‡æ · (Greedy)
        # alpha = 0.6: ç»å…¸å€¼ï¼Œå¹³è¡¡äº†"å¤ä¹ å›°éš¾æ ·æœ¬"ä¸"ä¿æŒè®°å¿†å¤šæ ·æ€§"
        alpha = 0.6
        probs = priorities.pow(alpha)
        probs = probs / probs.sum() # å½’ä¸€åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒ
        
        # ------------------------------------------------------------------
        # 2. [ç­–ç•¥è°ƒåº¦] ç¡®å®šæ¢¦å¢ƒé•¿åº¦
        # ------------------------------------------------------------------
        # å³ä½¿ç™½å¤©ç»å†äº†å¾ˆå¤šï¼Œä¹Ÿä¸å¯èƒ½å…¨éƒ¨é‡åšä¸€éæ¢¦ (é‚£ä¼šç´¯æ­»)
        # ç­–ç•¥ï¼šæŠ½å–ç¼“å†²åŒºä¸­æœ€å…·ä»£è¡¨æ€§çš„ 20% æ ·æœ¬è¿›è¡Œæ·±åº¦å·©å›º
        num_samples = max(1, int(len(self.hippocampal_buffer) * 0.2))
        
        # ä½¿ç”¨å¤šé¡¹å¼åˆ†å¸ƒè¿›è¡ŒåŠ æƒæ— æ”¾å›é‡‡æ · (Weighted Sampling without Replacement)
        indices = torch.multinomial(probs, num_samples, replacement=False)
        
        replay_count = 0
        self.eval() # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ (å…³é—­ Dropoutï¼Œä¿è¯ä¿¡å·ä¼ å¯¼çš„ç¨³å®šæ€§)
        # æ³¨æ„ï¼šSTDP é€»è¾‘æ˜¯æ‰‹å†™çš„ï¼Œä¸ä¾èµ– PyTorch çš„ optimizerï¼Œæ‰€ä»¥ eval() ä¸‹ä¾ç„¶ä¼šæ›´æ–°æƒé‡

        for idx in indices:
            # --------------------------------------------------------------
            # 3. [æ•°æ®æ¬è¿] ä»…åœ¨è®¡ç®—æ—¶åˆ»ä¸Šè½½ GPU
            # --------------------------------------------------------------
            item = self.hippocampal_buffer[idx.item()]
            # å°†å¼ é‡ä» CPU å†…å­˜ç§»åŠ¨åˆ° GPU æ˜¾å­˜ï¼Œå‡†å¤‡è„‰å†²è®¡ç®—
            memory_tensor = item['data'].to(self.device)

            # --------------------------------------------------------------
            # 4. [æ¢¦å¢ƒé‡æ„] Dream Reconstruction
            # --------------------------------------------------------------
            # A. å™ªå£°æ³¨å…¥ (Noise Injection)
            # ç”Ÿç‰©å­¦æ„ä¹‰ï¼šæ¢¦å¢ƒå¾€å¾€æ˜¯æ¨¡ç³Šä¸”æ‰­æ›²çš„ã€‚
            # æœºå™¨å­¦ä¹ æ„ä¹‰ï¼šç›¸å½“äº Data Augmentation (æ•°æ®å¢å¼º)ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚
            noise = torch.randn_like(memory_tensor) * 0.1
            dream_input = memory_tensor + noise

            # B. å°–æ³¢æ¶Ÿæ¼ªæ¨¡æ‹Ÿ (SWR Simulation)
            # åœ¨ç¡çœ æ—¶ï¼Œæµ·é©¬ä½“ä¼šå‘å‡ºé«˜é¢‘çˆ†å‘è„‰å†² (Ripples)ã€‚
            # æˆ‘ä»¬é€šè¿‡æé«˜ Poisson Encoder çš„ gain (å¢ç›Š) æ¥æ¨¡æ‹Ÿè¿™ç§é«˜å…´å¥‹åº¦çŠ¶æ€ã€‚
            spike_input = self._poisson_encoder(dream_input, time_steps=10, gain=1.5)

            # --------------------------------------------------------------
            # 5. [çªè§¦å¯å¡‘æ€§] STDP Hebbian Learning
            # --------------------------------------------------------------
            # "Cells that fire together, wire together" (ä¸€èµ·æ¿€å‘çš„ç¥ç»å…ƒè¿æ¥æ›´ç´§å¯†)
            
            # Layer 1: æ„ŸçŸ¥å±‚å·©å›º
            spikes1, _ = self.lif1(spike_input, time_steps=10)
            self.lif1.process_stdp(pre_spikes=spike_input, post_spikes=spikes1)

            # Layer 2: è”æƒ³å±‚å·©å›º (Deep Consolidation)
            # å°† L1 çš„å‘æ”¾ç‡ä½œä¸º L2 çš„è¾“å…¥ (Relay)
            rate1 = spikes1.mean(dim=1)
            spikes1_relay = self._poisson_encoder(rate1, time_steps=10, gain=1.5)
            
            spikes2, _ = self.lif2(spikes1_relay, time_steps=10)
            self.lif2.process_stdp(pre_spikes=spikes1_relay, post_spikes=spikes2)

            replay_count += 1

        # ------------------------------------------------------------------
        # 6. [ç”Ÿç‰©èŠ‚å¾‹] æ˜¼å¤œæ›´æ›¿
        # ------------------------------------------------------------------
        # æ¢¦é†’ä¹‹åï¼Œæµ·é©¬ä½“ç¼“å†²åŒºæ¸…ç©º (æˆ–å¤§å¹…è¡°å‡)ï¼Œä¸ºç¬¬äºŒå¤©çš„æ–°é²œè®°å¿†è…¾å‡ºç©ºé—´ã€‚
        # è¿™æ¨¡æ‹Ÿäº†ç”Ÿç‰©å¤§è„‘çš„"çªè§¦ç¼©æ”¾" (Synaptic Scaling) å’Œé—å¿˜æœºåˆ¶ã€‚
        self.hippocampal_buffer.clear()
        
        print(f"   -> âœ… æµ·é©¬ä½“æ·±åº¦å·©å›ºå®Œæˆ (Replayed {replay_count} High-Surprise Events).")
        return replay_count

    def save(self):
        try:
            torch.save(self.state_dict(), self.memory_file)
        except Exception as e:
            print(f"âš ï¸ ç¥ç»çš®å±‚ä¿å­˜å¤±è´¥: {e}")
        
    def load(self):
        if os.path.exists(self.memory_file):
            try:
                self.load_state_dict(torch.load(self.memory_file, map_location=self.device))
                print(f"ğŸ§  [ç¥ç»çš®å±‚] æ½œæ„è¯†æƒé‡å·²æ¢å¤ ({self.device})ã€‚")
            except Exception as e: 
                print(f"âš ï¸ ç¥ç»çš®å±‚åŠ è½½å¤±è´¥ï¼Œå·²é‡ç½®: {e}")



# ==============================================================================
# å…¨çŸ¥ä¹‹çœ¼ (Web Search Interface) - çœŸå®çš„æµè§ˆå™¨æ“æ§è€… (Stealth & AdBlock Edition)
# ==============================================================================

try:
    import undetected_chromedriver as uc
    from selenium.webdriver.common.by import By
    from selenium.webdriver.common.keys import Keys
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    import time
    import urllib.parse
except ImportError:
    print("âŒ ç¼ºå°‘å¿…è¦åº“ã€‚è¯·è¿è¡Œ: pip install undetected-chromedriver selenium")

class RealBrowserEye:
    """
    [çœŸå®ä¹‹çœ¼ - éšèº«ç‰ˆ V5.0 (æµå¼å¢å¼º)]
    ç‰¹æ€§ï¼š
    1. æŠ—æ£€æµ‹ï¼šä½¿ç”¨ undetected_chromedriver ä¼ªè£…æˆçœŸäººæµè§ˆå™¨ã€‚
    2. æ™ºèƒ½æ­»ç­‰ï¼šé‡åˆ°éªŒè¯ç æ—¶æŒ‚èµ·ï¼Œç­‰å¾…äººå·¥æ•‘æ´ï¼Œç»ä¸è½»æ˜“æŠ¥é”™é€€å‡ºã€‚
    3. è‡ªåŠ¨å›é€€ï¼šBaidu -> Bing -> Quark -> Google (ç™¾åº¦ä¼˜å…ˆä»¥åˆ©ç”¨ AI å›ç­”)ã€‚
    4. å¯å‘å¼æŠ“å–ï¼šé’ˆå¯¹å¤¸å…‹ç­‰ç»“æ„å¤šå˜çš„ç½‘é¡µï¼Œè‡ªåŠ¨è¯†åˆ«æ ‡é¢˜å’Œé“¾æ¥ã€‚
    5. æµå¼æ•è·ï¼š[æ–°å¢] ä¸“é—¨é’ˆå¯¹ç™¾åº¦çš„ AI æ™ºèƒ½å›ç­”ï¼Œæ”¯æŒåŠ¨æ€ç­‰å¾…ç”Ÿæˆã€‚
    6. å¹¿å‘Šå‡€åŒ–ï¼šåº•å±‚ç¡¬è¿‡æ»¤ï¼ŒèŠ‚çœ LLM ç®—åŠ›ã€‚
    """
    def __init__(self, headless=False):
        self.headless = headless 
        self.driver = None
        self.enabled = True
        
    def _init_driver(self):
        """å¯åŠ¨éšèº«æµè§ˆå™¨"""
        if self.driver is not None: return
        
        print("ğŸŒ [Browser] æ­£åœ¨å¯åŠ¨ Chrome å¼•æ“ (Stealth Mode)...")
        try:
            options = uc.ChromeOptions()
            if self.headless:
                options.add_argument("--headless")
            
            # å…³é”®ï¼šç¦ç”¨é¦–æ¬¡è¿è¡Œæ¬¢è¿é¡µï¼Œç¦ç”¨å›¾ç‰‡åŠ è½½ä»¥åŠ é€Ÿ
            options.add_argument("--no-first-run")
            # [ä¼˜åŒ–] ç¦ç”¨å›¾ç‰‡å¯ä»¥èŠ‚çœæµé‡å¹¶åŠ å¿«åŠ è½½ï¼Œä½†å¦‚æœé‡åˆ°å¸ƒå±€é”™ä¹±å¯æ³¨é‡Šæ‰æ­¤è¡Œ
            prefs = {"profile.managed_default_content_settings.images": 2}
            options.add_experimental_option("prefs", prefs)

            # å¯åŠ¨ undetected_chromedriver
            # use_subprocess=True å¯ä»¥é¿å…æŸäº›ç³»ç»Ÿä¸‹çš„è¿›ç¨‹é”æ­»é—®é¢˜
            self.driver = uc.Chrome(options=options, use_subprocess=True)
            self.driver.implicitly_wait(5)
            
            print("   -> ğŸš€ æµè§ˆå™¨å¼•æ“å·²å°±ç»ªã€‚")
            
        except Exception as e:
            print(f"âŒ æµè§ˆå™¨å¯åŠ¨å¤±è´¥: {e}")
            self.enabled = False

    def search(self, query, max_results=3, deep_read=True):
        """
        ç»Ÿä¸€æœç´¢å…¥å£ã€‚
        å‚æ•°:
          max_results: æŠ“å–ç»“æœçš„æ•°é‡ (å»ºè®® 3-5ï¼Œè¿‡å¤šä¼šæŒ¤å æ¨¡å‹ä¸Šä¸‹æ–‡)
          deep_read: æ˜¯å¦ç‚¹è¿›ç¬¬ä¸€æ¡é“¾æ¥é˜…è¯»å…¨æ–‡
        """
        if not self.enabled: return None

        # 1. å°è¯• Baidu (é¦–é€‰ï¼Œå°è¯•åˆ©ç”¨ AI æ™ºèƒ½å›ç­”)
        # [ä¿®æ”¹] å°†ç™¾åº¦ææƒè‡³ç¬¬ä¸€ä½ï¼Œå› ä¸ºå®ƒçš„å“åº”é€Ÿåº¦å¿«ä¸”æœ‰ AI æ‘˜è¦
        content = self._search_engine_robust(query, "baidu", max_results, deep_read)
        if content: return content

        # 2. å›é€€ Bing (å›½é™…å¤‡é€‰)
        print("   -> âš ï¸ Baidu æš‚æ—¶ä¸å¯ç”¨ï¼Œåˆ‡æ¢è‡³ Bing å¤‡ç”¨çº¿è·¯....")
        content = self._search_engine_robust(query, "bing", max_results, deep_read)
        if content: return content

        # 3. å›é€€ Quark (å›½å†…å…œåº•)
        print("   -> âš ï¸ Bing æš‚æ—¶ä¸å¯ç”¨ï¼Œåˆ‡æ¢è‡³ Quark å…œåº•...")
        content = self._search_engine_robust(query, "quark", max_results, deep_read)
        if content: return content

        # 4. å›é€€ Google (æœ€ç»ˆå…œåº•ï¼Œéœ€ç½‘ç»œç¯å¢ƒæ”¯æŒ)
        print("   -> âš ï¸ å…¨é¢å°é”ï¼Œå¯ç”¨ Google æœ€ç»ˆé˜²çº¿...")
        content = self._search_engine_robust(query, "google", max_results, deep_read)
        if content: return content

        return None

    def _wait_for_stream_text(self, element, timeout=15):
        """
        [æ–°å¢] æµå¼æ–‡æœ¬æ•è·å™¨
        ä¸“é—¨å¯¹ä»˜ç™¾åº¦çš„ 'æ­£åœ¨ç”Ÿæˆ'ã€‚åŸç†ï¼šå¦‚æœæ–‡æœ¬é•¿åº¦è¿˜åœ¨å˜ï¼Œå°±ç»§ç»­ç­‰ã€‚
        """
        print("      â³ æ£€æµ‹åˆ°æ½œåœ¨çš„ AI å›ç­”ï¼Œæ­£åœ¨ç­‰å¾…æµå¼ç”Ÿæˆ...", end="", flush=True)
        start_time = time.time()
        last_len = 0
        stable_count = 0
        
        while (time.time() - start_time) < timeout:
            try:
                curr_text = element.text
                curr_len = len(curr_text)
                
                # å¦‚æœæ–‡æœ¬å˜é•¿äº†ï¼Œé‡ç½®ç¨³å®šè®¡æ•°
                if curr_len > last_len:
                    last_len = curr_len
                    stable_count = 0
                    print(".", end="", flush=True)
                    time.sleep(0.5) # ç­‰å®ƒæ‰“å­—
                else:
                    # å¦‚æœæ–‡æœ¬æ²¡å˜ï¼Œå¢åŠ ç¨³å®šè®¡æ•°
                    stable_count += 1
                    time.sleep(0.5)
                
                # å¦‚æœè¿ç»­ 3 æ¬¡æ£€æŸ¥ï¼ˆ1.5ç§’ï¼‰é•¿åº¦éƒ½æ²¡å˜ï¼Œä¸”å†…å®¹è¶³å¤Ÿé•¿ï¼Œè®¤ä¸ºç”Ÿæˆç»“æŸ
                if stable_count >= 3 and curr_len > 10:
                    print(" âœ… å®Œæˆ")
                    return curr_text
            except:
                break
        
        print(" (è¶…æ—¶æˆ–æ‰“æ–­)")
        return element.text # è¿”å›ç›®å‰æŠ“åˆ°çš„æ‰€æœ‰å†…å®¹

    def _is_ad(self, element, engine):
        """
        [æ ¸å¿ƒ] å¹¿å‘Šè¯†åˆ«è¿‡æ»¤å™¨
        è¿”å› True è¡¨ç¤ºæ˜¯å¹¿å‘Šï¼Œéœ€è¦å‰”é™¤
        """
        try:
            text_content = element.text
            html_content = element.get_attribute('outerHTML')
            
            # é€šç”¨å…³é”®è¯æ£€æµ‹
            if "å¹¿å‘Š" in text_content[-20:]: # ç™¾åº¦å¹¿å‘Šé€šå¸¸åœ¨æœ«å°¾æ ‡â€œå¹¿å‘Šâ€
                return True
            
            # é’ˆå¯¹ç™¾åº¦çš„ç‰¹å¾
            if engine == "baidu":
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ class åŒ…å« tuiguang çš„å…ƒç´ 
                if "tuiguang" in html_content or "ec_tuiguang" in html_content:
                    return True
                # æ£€æŸ¥åº•éƒ¨çš„ç°è‰²å¹¿å‘Šæ ‡
                try:
                    footer = element.find_element(By.CSS_SELECTOR, "span[class*='tuiguang'], span[class*='ad']")
                    if footer: return True
                except: pass

            # é’ˆå¯¹å¤¸å…‹/ç¥é©¬çš„ç‰¹å¾
            if engine == "quark":
                if "ad_track" in html_content or "data-is-ad='1'" in html_content:
                    return True
                # å¤¸å…‹æœ‰æ—¶å€™ä¼šç”¨æ ·å¼éšè— "Ads" å­—æ ·ï¼Œæ£€æµ‹ç‰¹å®š class
                if "ad-tag" in html_content:
                    return True

            return False
        except:
            return False # æ‹¿ä¸å‡†çš„å°±æ”¾è¿‡ï¼Œå®å¯é”™æ€ä¸å¯æ¼æŠ“

    def _search_engine_robust(self, query, engine, max_results, deep_read=True):
        """
        [æ ¸å¿ƒ] å¥å£®çš„æœç´¢å¼•æ“äº¤äº’é€»è¾‘
        """
        try:
            self._init_driver()
            encoded_query = urllib.parse.quote(query)
            
            # --- å¼•æ“é…ç½® ---
            if engine == "bing":
                url = f"https://www.bing.com/search?q={encoded_query}"
                item_selector = "li.b_algo"
                title_selector = "h2 a"
                desc_selector = "p"

            elif engine == "baidu":
                url = f"https://www.baidu.com/s?wd={encoded_query}"
                item_selector = "div.c-container" 
                title_selector = "h3"
                desc_selector = "div[class*='c-abstract']"

            elif engine == "quark":
                # å¤¸å…‹ç½‘é¡µç‰ˆé€šå¸¸æ˜¯ç¥é©¬æœç´¢ (sm.cn) çš„é©¬ç”²
                url = f"https://quark.sm.cn/s?q={encoded_query}"
                # å¤¸å…‹çš„ç±»åå¯èƒ½ä¼šå˜ï¼Œæˆ‘ä»¬å°è¯•å‡ ä¸ªå¸¸è§çš„
                item_selector = "div.result, div.c-container" 
                title_selector = "h2, h3, a.c-title-text"
                desc_selector = "div.res-desc, div.c-abstract"
                
            else: # google
                url = f"https://www.google.com/search?q={encoded_query}"
                item_selector = "div.g" 
                title_selector = "h3"
                desc_selector = "div[style*='-webkit-line-clamp']"

            print(f"   -> ğŸŒ [{engine.title()}] æ­£åœ¨è®¿é—®: {query[:20]}...")

            try:
                self.driver.get(url)
            except Exception as e_nav:
                # æ•è·è¿æ¥æ–­å¼€/Sessionå¤±æ•ˆé”™è¯¯
                err_msg = str(e_nav).lower()
                if "invalid session id" in err_msg or "disconnected" in err_msg or "closed" in err_msg:
                    print(f"   -> ğŸ”„ [æµè§ˆå™¨æ–­è¿] æ£€æµ‹åˆ°åƒµå°¸è¿›ç¨‹ï¼Œæ­£åœ¨é‡å¯å¼•æ“...")
                    self.quit()       # 1. å½»åº•æ€æ‰æ­»æ‰çš„ driver
                    self.driver = None # 2. å¼ºåˆ¶ç½®ç©º
                    self._init_driver() # 3. é‡æ–°å¯åŠ¨æ–°æµè§ˆå™¨
                    self.driver.get(url) # 4. é‡è¯•è®¿é—®
                else:
                    raise e_nav # å…¶ä»–é”™è¯¯ç»§ç»­æŠ›å‡º

            # --- æ™ºèƒ½ç­‰å¾…ä¸äººå·¥æ•‘æ´æœºåˆ¶ ---
            try:
                # 1. [æ–°å¢] åŸºç¡€ç½‘ç»œæ£€æŸ¥ï¼šç¡®ä¿ body å·²åŠ è½½ (é˜²æ­¢ç™½å±/æ–­ç½‘)
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )

                # 2. å°è¯•æ£€æµ‹ç»“æœå®¹å™¨ (ç»™ 5 ç§’)
                try:
                    WebDriverWait(self.driver, 5).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, item_selector))
                    )
                except:
                    # [ä¼˜åŒ–] å¦‚æœæ˜¯å¤¸å…‹ï¼Œå› ä¸ºç±»åå¤šå˜ï¼Œæš‚æ—¶ä¸è®¤ä¸ºæ˜¯é˜»å¡ï¼Œäº¤ç»™åé¢çš„å¯å‘å¼æŠ“å–å»åˆ¤æ–­
                    if engine == 'quark': 
                        pass 
                    else:
                        # å¯¹äº Bing/Baidu/Googleï¼Œå¦‚æœ 5ç§’æ²¡ç»“æœï¼Œå¤§æ¦‚ç‡æ˜¯æŒ‚äº†
                        raise Exception("Standard selector timeout")

            except Exception as e:
                # åªæœ‰çœŸçš„æ‰¾ä¸åˆ°ç»“æœï¼Œæˆ–è€…æ˜¾å¼è§¦å‘å¼‚å¸¸æ—¶ï¼Œæ‰è¿›å…¥äººå·¥æ•‘æ´
                print(f"\n   -> ğŸ›‘ [é˜»å¡æŠ¥è­¦] {engine.title()} ä¼¼ä¹æœªåŠ è½½ç»“æœ(éªŒè¯ç /ç½‘ç»œæ³¢åŠ¨)ï¼")
                print(f"   -> â³ è„šæœ¬å·²æŒ‚èµ·ï¼Œè¯·åœ¨å¼¹å‡ºçš„ Chrome çª—å£ä¸­æ£€æŸ¥æˆ–æ‰‹åŠ¨éªŒè¯...")
                
                # [æ–°å¢] æ£€æŸ¥ä¸€ä¸‹æ˜¯ä¸æ˜¯çœŸçš„å‡ºéªŒè¯ç äº† (å¯é€‰ï¼Œä»…ä½œæç¤º)
                page_title = self.driver.title
                if "éªŒè¯" in page_title or "Security" in page_title:
                    print(f"   -> ğŸ•µï¸â€â™‚ï¸ æ£€æµ‹åˆ°é¡µé¢æ ‡é¢˜å«æ•æ„Ÿè¯: {page_title}")

                try:
                    # æ­»ç­‰ 300 ç§’ï¼Œç»™ä½ è¶³å¤Ÿæ—¶é—´å»ç‚¹éªŒè¯ç 
                    # è¿™é‡Œä¸ä»…ç­‰ item_selectorï¼Œå¦‚æœä½ æ‰‹åŠ¨åˆ’è¿‡äº†éªŒè¯ç ï¼Œé¡µé¢åˆ·æ–°äº†ï¼Œ
                    # ä»»ä½•ä¸€ç§ç»“æœå®¹å™¨å‡ºç°éƒ½åº”è¯¥ç®—é€šè¿‡
                    WebDriverWait(self.driver, 300).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, item_selector))
                    )
                    print("   -> âœ… æ£€æµ‹åˆ°ç»“æœå‡ºç°ï¼ç»§ç»­æ‰§è¡Œ...")
                except:
                    print(f"   -> âŒ äººå·¥æ•‘æ´è¶…æ—¶ (5åˆ†é’Ÿ)ï¼Œæ”¾å¼ƒ {engine} æœç´¢ã€‚")
                    return None

            # --- æå–ç»“æœ ---
            time.sleep(1.5) # å¤¸å…‹æ˜¯åŠ¨æ€åŠ è½½ï¼Œå»ºè®®ç¨å¾®å¤šç­‰0.5ç§’
            results = self.driver.find_elements(By.CSS_SELECTOR, item_selector)

            # [å¤¸å…‹ä¸“å±] å¯å‘å¼è¡¥ä¸
            if not results and engine == "quark":
                results = self.driver.find_elements(By.XPATH, "//div[.//h3 or .//h2]")
                results = [r for r in results if r.size['height'] > 50]

            summary_list = []
            valid_links = []
            
            count = 0
            for i, res in enumerate(results):
                # æ•°é‡è¾¾åˆ°ä¸Šé™åˆ™åœæ­¢
                if count >= max_results: break
                
                # [æ–°å¢] å¹¿å‘Šè¿‡æ»¤ï¼šå¦‚æœåˆ¤å®šä¸ºå¹¿å‘Šï¼Œç›´æ¥è·³è¿‡
                if self._is_ad(res, engine):
                    continue

                try:
                    title, link, snippet = "", "", ""

                    # --- [æ–°å¢] ç™¾åº¦æµå¼ AI å†…å®¹ç‰¹æ®Šå¤„ç† ---
                    # å¦‚æœæ˜¯ç™¾åº¦çš„ç¬¬ä¸€æ¡ç»“æœï¼Œä¸”åŒ…å«â€œæ™ºèƒ½â€æˆ–â€œAIâ€å­—æ ·ï¼Œæˆ–è€…æ˜¯ç¬¬ä¸€æ¡
                    # æˆ‘ä»¬å°è¯•æ•è·å®ƒçš„æµå¼ç”Ÿæˆè¿‡ç¨‹
                    if engine == "baidu" and i == 0:
                        # è°ƒç”¨æµå¼ç­‰å¾…ï¼Œé˜²æ­¢æŠ“åˆ°åŠæˆª
                        full_text = self._wait_for_stream_text(res)
                        # å¦‚æœæŠ“åˆ°çš„æ–‡æœ¬å¾ˆé•¿ï¼Œç›´æ¥ä½œä¸º AI æ€»ç»“ï¼Œä¸éœ€ç‚¹é“¾æ¥
                        if len(full_text) > 200: 
                             snippet = full_text[:800] # æˆªå–å‰800å­—ï¼Œç»™ AI æ›´å¤šä¸Šä¸‹æ–‡
                             title = "ã€ç™¾åº¦ AI æ™ºèƒ½æ€»ç»“ã€‘"
                             link = self.driver.current_url # AI å›ç­”é€šå¸¸æ²¡æœ‰ç‹¬ç«‹é“¾æ¥
                             summary_list.append(f"[{count+1}] {title}\næ‘˜è¦: {snippet}")
                             count += 1
                             continue

                    # --- å¸¸è§„æŠ“å–é€»è¾‘ ---
                    # 1. æŠ“æ ‡é¢˜é“¾æ¥
                    try:
                        if engine == "quark":
                            header = res.find_element(By.CSS_SELECTOR, "h2, h3, h4, a[class*='title']")
                        else:
                            header = res.find_element(By.CSS_SELECTOR, title_selector)
                        title = header.text
                        link = header.get_attribute("href")
                        if not link: link = res.find_element(By.TAG_NAME, "a").get_attribute("href")
                    except: continue 

                    # 2. æŠ“æ‘˜è¦
                    try:
                        if engine == "quark":
                             snippet_elem = res.find_element(By.CSS_SELECTOR, "div[class*='abstract'], div[class*='desc'], div[class*='txt']")
                             snippet = snippet_elem.text
                        elif engine == "baidu":
                             snippet_elem = res.find_element(By.CSS_SELECTOR, desc_selector)
                             snippet = snippet_elem.text
                        else:
                            snippet_elem = res.find_element(By.CSS_SELECTOR, desc_selector)
                            snippet = snippet_elem.text
                    except:
                        snippet = res.text.replace(title, "")[:100].strip()
                    
                    if link and title and len(title) > 2:
                        summary_list.append(f"[{count+1}] {title}\nLink: {link}\næ‘˜è¦: {snippet}")
                        valid_links.append(link)
                        count += 1
                except: continue
            
            if not summary_list: return None

            context = f"ã€{engine.title()} æœç´¢ç»“æœã€‘:\n" + "\n\n".join(summary_list)
            
            # --- æ·±åº¦é˜…è¯» (Deep Read Top 1) ---
            # åªæœ‰å½“æœç´¢æˆåŠŸä¸” deep_read=True æ—¶æ‰è¿›è¡Œæ·±åº¦é˜…è¯»
            if deep_read and valid_links:
                target_link = valid_links[0]
                # [ä¼˜åŒ–] å¦‚æœç¬¬ä¸€æ¡æ˜¯ AI æ€»ç»“ï¼ˆLink æ˜¯å½“å‰é¡µï¼‰ï¼Œå°è¯•è¯»ç¬¬äºŒæ¡ï¼Œé¿å…é‡å¤
                if target_link == self.driver.current_url and len(valid_links) > 1:
                    target_link = valid_links[1]
                
                if target_link != self.driver.current_url:
                    # print(f"   -> ğŸ“– æ­£åœ¨æ·±åº¦é˜…è¯»: {target_link}")
                    content = self.read_page(target_link)
                    context += f"\n\nã€æ·±åº¦é˜…è¯» (Top Link)ã€‘:\n{content}"
                
            return context

        except Exception as e:
            print(f"   -> âš ï¸ {engine} æœç´¢å‡ºé”™: {e}")
            return None

    def read_page(self, url, max_chars=1500):
        """è®¿é—®å…·ä½“é“¾æ¥å¹¶æå–æ­£æ–‡"""
        try:
            # è®¾ç½®é¡µé¢åŠ è½½è¶…æ—¶ï¼Œé˜²æ­¢å¡æ­»åœ¨æŸäº›é‡JSç½‘ç«™
            self.driver.set_page_load_timeout(15)
            self.driver.get(url)
            time.sleep(2)
            
            body = self.driver.find_element(By.TAG_NAME, "body").text
            # æ¸…æ´—å¤šä½™ç©ºè¡Œ
            clean_text = re.sub(r'\n+', '\n', body)
            return clean_text[:max_chars]
        except Exception as e:
            return f"[è¯»å–å¤±è´¥: {str(e)[:50]}...]"

    def quit(self):
        if self.driver:
            try:
                self.driver.quit()
            except: pass
            self.driver = None


class PluginManager:
    """
    [V7.2 æ–°å¢] æŠ€èƒ½çš®å±‚ / æ’ä»¶ç®¡ç†å™¨
    åŠŸèƒ½ï¼šå¯åŠ¨æ—¶æ‰«æ plugins ç›®å½•ï¼Œå°†â€œè‡ªæˆ‘ç¼–ç¨‹â€äº§ç”Ÿçš„ Python è„šæœ¬
    ä½œä¸ºæ–°æŠ€èƒ½åŠ¨æ€åŠ è½½åˆ°å½“å‰ç”Ÿå‘½ä½“å®ä¾‹ä¸­ (Monkey Patching)ã€‚
    
    åŒºåˆ«äº LogicalLeftBrain (è´Ÿè´£ä¸´æ—¶æ‰§è¡Œä»£ç è¿”å›ç»“æœ)ï¼Œ
    PluginManager è´Ÿè´£æ°¸ä¹…æ€§åœ°ä¿®æ”¹ç”Ÿå‘½ä½“çš„è¡Œä¸ºæ¨¡å¼ (Add methods/attributes)ã€‚
    """
    def __init__(self, plugin_dir=None):
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥è·¯å¾„ï¼Œå…¶æ¬¡è¯»å–Configï¼Œæœ€åä½¿ç”¨é»˜è®¤å€¼
        self.plugin_dir = plugin_dir if plugin_dir else getattr(Config, 'PLUGIN_DIR', './nezha_evolution_system/plugins')
        # [å¥å£®æ€§] ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.plugin_dir, exist_ok=True)

    def activate_skills(self, agent_instance):
        """
        åŠ è½½å¹¶æ¿€æ´»æ‰€æœ‰æŠ€èƒ½
        (æ”¯æŒè‡ªåŠ¨ç»‘å®šä¸çƒ­ä¿®å¤)
        :param agent_instance: å“ªå’æœ¬ä½“ (self)ï¼Œè®©æ’ä»¶å¯ä»¥ä¿®æ”¹å®ƒ
        :return: æˆåŠŸåŠ è½½çš„æŠ€èƒ½æ•°é‡
        """
        # æ‰«æç›®å½•ä¸‹æ‰€æœ‰ä»¥ skill_ å¼€å¤´çš„ py æ–‡ä»¶
        if not os.path.exists(self.plugin_dir): return 0
        skills = [f for f in os.listdir(self.plugin_dir) if f.endswith('.py') and f.startswith('skill_')]
        
        if not skills:
            return 0
            
        print(f"ğŸ§© [æŠ€èƒ½çš®å±‚] æ£€æµ‹åˆ° {len(skills)} ä¸ªå¤–æŒ‚åŸºå› ï¼Œæ­£åœ¨è½¬å½•...")
        loaded_count = 0
        
        for filename in skills:
            filepath = os.path.join(self.plugin_dir, filename)
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    code_content = f.read()
                
                # æ„å»ºæ‰§è¡Œä¸Šä¸‹æ–‡
                # [æ ¸å¿ƒæœºåˆ¶] å°† agent_instance ä¼ ç»™ 'self'
                # è¿™æ ·æ’ä»¶ä»£ç é‡Œçš„ `self.anxiety_level = 0` æˆ– `self.new_method = ...` æ‰ä¼šç”Ÿæ•ˆ
                context = {
                    "self": agent_instance, 
                    "math": math, 
                    "torch": torch, 
                    "datetime": datetime,
                    "os": os,
                    "json": json,
                    "re": re,
                    "Config": Config # å…è®¸æ’ä»¶è¯»å–é…ç½®
                }
                
                # åŠ¨æ€æ‰§è¡Œä»£ç  (Monkey Patching)
                # 1. è®°å½•æ‰§è¡Œå‰çš„å±æ€§åˆ—è¡¨
                # è®°å½• snapshot ç”¨äºå¯¹æ¯” (ä»…ç”¨äºæ—¥å¿—ç»Ÿè®¡ï¼Œä¸å‚ä¸é€»è¾‘åˆ¤æ–­)
                old_attrs_snapshot = set(dir(agent_instance))
                
                # 2. åŠ¨æ€æ‰§è¡Œä»£ç  (Monkey Patching)
                exec(code_content, context)

                # --- è‡ªåŠ¨æŒ‚è½½æœºåˆ¶ ---
                
                # å®šä¹‰ä¸éœ€è¦æŒ‚è½½çš„ç³»ç»Ÿæ³¨å…¥å˜é‡
                system_keys = {'self', 'math', 'torch', 'datetime', 'os', 'json', 're', 'Config', '__builtins__'}
                
                # æ‰«æ context ä¸­çš„æ–°å˜é‡
                # æ³¨æ„ï¼šè¿™é‡Œå»æ‰äº† - set(old_attrs_snapshot)ï¼Œå…è®¸æ’ä»¶è¦†å†™æ—§æ–¹æ³•(Hot Fix)
                candidates = set(context.keys()) - system_keys

                effective_changes = 0

                for name in candidates:
                    obj = context[name]
                    
                    # 1. å¿½ç•¥ç§æœ‰å˜é‡å’Œæ¨¡å—å¼•ç”¨
                    if name.startswith('_') or isinstance(obj, types.ModuleType):
                        continue
                    
                    # 2. æŒ‚è½½é€»è¾‘
                    if isinstance(obj, types.FunctionType):
                        # ç»‘å®šä¸ºå®ä¾‹æ–¹æ³• (æ”¯æŒ self)
                        setattr(agent_instance, name, types.MethodType(obj, agent_instance))
                        # print(f"      + æ–¹æ³•æŒ‚è½½/è¦†å†™: {name}") # è°ƒè¯•ç”¨
                        effective_changes += 1
                    else:
                        # æŒ‚è½½ä¸ºå±æ€§ (é…ç½®å‚æ•°ç­‰)
                        setattr(agent_instance, name, obj)
                        # print(f"      + å±æ€§æŒ‚è½½/è¦†å†™: {name}") # è°ƒè¯•ç”¨
                        effective_changes += 1

                if effective_changes > 0:
                    loaded_count += 1
                    print(f"   -> ğŸ§¬ åŸºå› è½¬å½•æˆåŠŸ: {filename} (ç”Ÿæ•ˆèŠ‚ç‚¹: {effective_changes})")
                else:
                    print(f"   -> âš ï¸ åŸºå› æ— æ•ˆ: {filename} (æœªæ£€æµ‹åˆ°æœ‰æ•ˆçš„åŠŸèƒ½å®šä¹‰)")
                
            except Exception as e:
                print(f"   -> âš ï¸ åŸºå›  {filename} æ’å¼‚ååº”: {e}")
        
        return loaded_count

    def execute_code(self, code_snippet):
        """
        [è¿è¡Œé˜¶æ®µ] é€»è¾‘å·¦è„‘ / ä¸´æ—¶å·¥å…·æ‰§è¡Œ
        åªè¿”å›æ‰“å°ç»“æœï¼Œä¸ä¿®æ”¹æœ¬ä½“ï¼Œç›¸å¯¹å®‰å…¨
        """
        buf = io.StringIO()
        context = {
            "math": math, "random": random, "datetime": datetime, 
            "re": re, "json": json, "print": print
        }
        
        try:
            # æ•è·æ ‡å‡†è¾“å‡º
            with contextlib.redirect_stdout(buf):
                exec(code_snippet, context)
            
            output = buf.getvalue().strip()
            return output if output else "[ä»£ç æ‰§è¡ŒæˆåŠŸï¼Œæ— è¾“å‡º]"
        except Exception as e:
            return f"âŒ ä»£ç æ‰§è¡Œé”™è¯¯: {e}"


# ==============================================================================
# 4. æ ¸å¿ƒå®ä½“ï¼šå“ªå’ (V7.2 Singularity Orchestrator)
# ==============================================================================
class NezhaLifeform:

    # å†…æ„Ÿå—
    def _check_growth_potential(self):
        """æ£€æŸ¥å½“å‰æ˜¾å­˜æ˜¯å¦è¿˜èƒ½å®¹çº³æ–°çš„ä¸€å±‚"""
        try:
            # æ£€æŸ¥æ‰€æœ‰ä½¿ç”¨ä¸­çš„ GPU
            devices = {p.device for p in self.model.parameters() if p.device.type == "cuda"}
            if not devices: return True, "CPUæ¨¡å¼"
            
            # ä¼°ç®—å¢åŠ ä¸€å±‚æ‰€éœ€çš„æ˜¾å­˜ (å‚æ•°é‡ x 2 bytes x ç¼“å†²)
            # é¢„ç•™ 500MB + 1GB ç¼“å†²
            estimated_cost = 800 * 1024 * 1024
            safe_buffer = 1.5 * 1024 * 1024 * 1024
            
            for device in devices:
                # è·å–æ˜¾å­˜ä¿¡æ¯
                free, total = torch.cuda.mem_get_info(device)
                
                # é˜ˆå€¼åˆ¤æ–­
                if free < (estimated_cost + safe_buffer):
                    # åˆ©ç”¨ total æ‰“å°æ›´è¯¦ç»†çš„æ—¥å¿—
                    usage_ratio = (total - free) / total * 100
                    return False, f"æ˜¾å­˜æ¯ç«­ (Device {device}: Free {free/1024**3:.2f}GB / Total {total/1024**3:.2f}GB, Used {usage_ratio:.1f}%)"
            
            return True, "æ˜¾å­˜å……è¶³"
        except Exception as e:
            # å¦‚æœå‡ºé”™ï¼Œé»˜è®¤å…è®¸ç”Ÿé•¿ï¼Œé¿å…å› ç›‘æ§ç»„ä»¶æ•…éšœé˜»ç¢è¿›åŒ–
            return True, f"æ„ŸçŸ¥å¤±æ•ˆ({e})"

    def __init__(self):
        # 1. åŠ è½½çŠ¶æ€ä¸åŸºå› 
        self.state = self._load_state()

        # ================= æ­¥éª¤ 1.5: ä¼˜å…ˆåŠ è½½ç¥ä¹‹çœ¼ =================
        self.embedder = None
        current_dim = 1024

        try:
            print(f"ğŸ‘ï¸ [Vision] æ­£åœ¨åŠ è½½ç¥ä¹‹çœ¼ ({Config.EMBEDDING_MODEL_NAME})...")
            
            from sentence_transformers import SentenceTransformer
            # [å…³é”®] cache_folder æŒ‡å®šä¸‹è½½ç›®å½•
            self.embedder = SentenceTransformer(
                Config.EMBEDDING_MODEL_NAME, 
                device="cpu", 
                cache_folder=Config.MODEL_CACHE_DIR
            )
            
            # è·å–æ¨¡å‹ç»´åº¦ï¼ˆä»…ç”¨äºæ‰“å°ï¼Œä¸å†ä¼ ç»™MemoryInfrastructureï¼‰
            current_dim = self.embedder.get_sentence_embedding_dimension()
            print(f"   -> âœ… è§†è§‰æ¨¡ç»„åŠ è½½å®Œæ¯•ï¼Œå½“å‰è§†ç•Œç²¾åº¦: {current_dim} ç»´")
            
        except ImportError:
            print("âš ï¸ æœªæ£€æµ‹åˆ° sentence-transformers åº“ï¼Œç¥ä¹‹çœ¼æ— æ³•å¼€å¯ã€‚")
        except Exception as e:
            print(f"âš ï¸ è¯­ä¹‰æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}")

        # 1. åˆå§‹åŒ–åŸºç¡€è®¾æ–½ (ç§»é™¤äº† embedding_dim å‚æ•°)
        # æˆ‘ä»¬çš„æ–° MemoryInfrastructure ç°åœ¨ä¼šè‡ªåŠ¨è¯†åˆ«ç»´åº¦
        self.memory_db = MemoryInfrastructure()

        # 2. å¯åŠ¨æ—¶è®©æµ·é©¬ä½“æ‰«æçŸ¥è¯†åº“
        # å®šä¹‰ helper å‡½æ•°ï¼šå°† embedder çš„èƒ½åŠ›åŒ…è£…æˆ MemoryInfrastructure èƒ½ç”¨çš„å½¢å¼
        def _embed_helper(text_list):
            if not self.embedder: return []
            # [å…³é”®] encode åŠ ä¸Š convert_to_tensor=True
            # ç›´æ¥è¿”å› CPU Tensorï¼Œæ•ˆç‡æœ€é«˜ï¼Œå®Œç¾é€‚é…æ–° MemoryInfrastructure
            return self.embedder.encode(text_list, convert_to_tensor=True, device='cpu')

        if self.embedder and hasattr(self, 'memory_db'):
            # è®© MemoryInfrastructure ä½¿ç”¨ embedder èƒ½åŠ›å»è‡ªåŠ¨â€œåƒä¹¦â€ (Ingest)
            self.memory_db.ingest_knowledge_base(Config.KNOWLEDGE_BASE, _embed_helper)

        # ä½¿ç”¨ deque æ›¿ä»£ list ä»¥é˜²æ­¢å†…å­˜æ³„æ¼
        # loss_history: ç”¨äºç´¢æè¯ºæ¯”ç‡è®¡ç®—ï¼Œéœ€è¦å®šé•¿æ»‘åŠ¨çª—å£
        # ä» state ä¸­è¯»å– listï¼Œç„¶åè½¬ä¸º deque
        loss_hist_data = self.state.get('loss_history', [])
        # maxlen=2000 æ„å‘³ç€ç¬¬ 2001 ä¸ªæ•°æ®è¿›æ¥æ—¶ï¼Œç¬¬ 1 ä¸ªæ•°æ®è‡ªåŠ¨å¼¹å‡ºï¼Œå¤æ‚åº¦ O(1)
        self.loss_history = deque(loss_hist_data, maxlen=2000)

        # è´å¶æ–¯è¿›åŒ–å¼•æ“
        self.evolution_engine = BayesianGeneticEngine(self.memory_db)
        self.genome = self.state.get('genome', Config.DEFAULT_GENOME.copy())

        #  åˆå§‹åŒ–æä»æ ¸åŠ«æŒçŠ¶æ€é” (ä»å­˜æ¡£æ¢å¤ï¼Œé˜²æ­¢é‡å¯æ–­ç‰‡)
        self.is_under_amygdala_hijack = self.state.get('is_hijacked', False)

        # === åˆå§‹åŒ– RLock (å¯é‡å…¥é”) ===
        # è¿™æŠŠé”ç”¨äºä¿æŠ¤æµè§ˆå™¨å’Œå­˜æ¡£æ–‡ä»¶ï¼Œé˜²æ­¢åå°æ½œæ„è¯†å’Œå‰å°ä¸»çº¿ç¨‹æ‰“æ¶
        self.lock = threading.RLock()

        # ç¡®ä¿æ‰€æœ‰æ–°åŸºå› éƒ½å­˜åœ¨
        for k, v in Config.DEFAULT_GENOME.items():
            if k not in self.genome: self.genome[k] = v   

        # å®ä¾‹åŒ–é€»è¾‘å·¦è„‘
        self.left_brain = LogicalLeftBrain()

        # 2. åˆå§‹åŒ–è®°å¿†ç¼“å†²åŒº
        self.daily_buffer = deque()                    # æ¯æ—¥ç»å† (ç”¨äºå¤œé—´åæ€)

        #  è„‘æ¡¥è®­ç»ƒç¼“å†²åŒº (Replay Buffer)
        # 2000å®¹é‡ï¼š1000ç”¨äºFIFO(æœ€è¿‘å‡ å°æ—¶)ï¼Œ1000ç”¨äºè“„æ°´æ± (ä¸€ç”Ÿçš„ç¼©å½±)
        # è¿™ç§æ··åˆç»“æ„èƒ½æœ‰æ•ˆé˜²æ­¢ VAE åœ¨é•¿æ—¶é—´è¿è¡Œåå‘ç”Ÿ"æ€§æ ¼æ¼‚ç§»"        
        self.bridge_buffer = SemanticEpisodicBuffer(capacity=2000, archive_ratio=0.5)

        self.short_term_memory = deque(maxlen=20)      # çŸ­æœŸå·¥ä½œè®°å¿† (ç”¨äºå¤šè½®å¯¹è¯)
        
        print("ğŸ”¥ [å“ªå’] æ­£åœ¨æ„å»ºèº¯ä½“ (PFC + Dual Brain + Net2Net/MoE)...")
        
        # ================= æ™ºèƒ½æ˜¾å­˜è§„åˆ’ =================
        # è‡ªåŠ¨æ£€æµ‹ GPU æ•°é‡å’Œæ˜¾å­˜å¤§å°ï¼Œå¹¶ä¸ºæ¯å¼ å¡é¢„ç•™ç¼“å†²ç©ºé—´ (KV Cache / System)
        max_memory_map = {}
        if torch.cuda.is_available():
            num_gpus = torch.cuda.device_count()
            print(f"   -> ğŸ–¥ï¸ æ£€æµ‹åˆ° {num_gpus} å¼ æ˜¾å¡ï¼Œæ­£åœ¨è§„åˆ’æ˜¾å­˜å¸ƒå±€...")
            
            for i in range(num_gpus):
                # è·å–è¯¥å¡æ€»æ˜¾å­˜ (Bytes -> GiB)
                props = torch.cuda.get_device_properties(i)
                total_mem = props.total_memory / (1024**3)
                
                # ç­–ç•¥ï¼šé¢„ç•™ç¼“å†²åŒº (Buffer)
                # 0å·å¡é€šå¸¸è´Ÿè´£æ¡Œé¢æ˜¾ç¤ºå’Œ PyTorch ä¸Šä¸‹æ–‡ï¼Œå»ºè®®é¢„ç•™ 3GB (å¦‚æœæ˜¾å­˜æå°åˆ™æŒ‰æ¯”ä¾‹)
                if i == 0:
                    buffer = 3.0 
                else:
                    buffer = 1.5
                
                # æ˜¾å­˜å¤ªå°çš„å¡(ä¾‹å¦‚4Gå¡)ï¼Œé¢„ç•™å¤ªå¤šä¼šå¯¼è‡´ä¸å¯ç”¨ï¼Œåšä¸€ä¸ªåŠ¨æ€ä¿æŠ¤
                if total_mem < 8.0: 
                    buffer = total_mem * 0.3 # å°å¡åªé¢„ç•™ 30%
                
                # è®¡ç®—å®‰å…¨é™é¢
                alloc_mem = max(0, total_mem - buffer)
                
                # int() ä¼šå‘ä¸‹å–æ•´ï¼Œæ¯”å¦‚ 10.9G -> 10Gï¼Œè¿™å¾ˆå®‰å…¨
                max_memory_map[i] = f"{int(alloc_mem)}GiB"
                
                print(f"      - GPU {i} ({props.name}): æ€»é‡ {total_mem:.1f}G | é¢„ç•™ {buffer:.1f}G | åˆ†é… {max_memory_map[i]}")
        else:
            print("   -> âš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œå°†å›é€€åˆ° CPU æ¨¡å¼ (ææ…¢)ã€‚")
            max_memory_map = None # è®© accelerate è‡ªå·±çœ‹ç€åŠ
        
        mid = Config.MASTER_MODEL_PATH if os.path.exists(Config.MASTER_MODEL_PATH) else Config.BASE_MODEL
        print(f"   -> Loading Model from: {mid}")

        # å…ˆåŠ è½½ Tokenizer (è¿™ç©æ„å„¿ä¸å æ˜¾å­˜ï¼Œå…ˆåŠ è½½å¥½é˜²æ­¢è·¯å¾„é”™è¯¯)
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                mid, 
                trust_remote_code=True,
                fix_mistral_regex=True
                )
        except Exception as e: # <--- [ä¿®æ­£] å¿…é¡»åŠ ä¸Š Exception as eï¼Œå¦åˆ™ print(e) ä¼šæŠ¥é”™
            print(f"   -> âš ï¸ æœ¬åœ° Tokenizer åŠ è½½å¤±è´¥ ({e})ï¼Œå°è¯•ä» Base Model æ‹‰å–...")
            # å…œåº•ï¼šå¦‚æœæœ¬åœ°æ²¡æœ‰ï¼Œå°è¯•ä» hub æ‹‰å–
            self.tokenizer = AutoTokenizer.from_pretrained(
                Config.BASE_MODEL, 
                trust_remote_code=True,
                fix_mistral_regex=True
                )

        # è¡¥å…¨ Tokenizer é…ç½®
        if not self.tokenizer.pad_token: 
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # [4-bit ç‹¬å æ¨¡å¼] å½»åº•ç§»é™¤ 8-bit é€»è¾‘
        # ä½¿ç”¨ NF4 (Normal Float 4) æ ¼å¼ï¼Œè¿™æ˜¯ç›®å‰ LLM é‡åŒ–çš„æœ€ä½³å®è·µ
        try:
            print("   -> ğŸš€ å¯åŠ¨ 4-bit é‡åŒ–åŠ è½½ (NF4 + SDPA)...")
            
            # 4-bit é…ç½® (NF4 æ ¼å¼ï¼Œç²¾åº¦æŸå¤±æå°)
            # NF4 (Normal Float 4) æ˜¯ä¸“é—¨ä¸ºæ­£æ€åˆ†å¸ƒæƒé‡é‡èº«å®šåˆ¶çš„é‡åŒ–æ•°æ®ç±»å‹
            q_config_4bit = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16, # è®¡ç®—æ—¶è¿˜åŸå› fp16
                bnb_4bit_use_double_quant=True,       # åŒé‡é‡åŒ–çœæ˜¾å­˜ (å†æ¬¡å‹ç¼©é‡åŒ–å¸¸æ•°)
                bnb_4bit_quant_type="nf4"             # æ­£æ€æµ®ç‚¹é‡åŒ– (æ¯” fp4 æ›´ç²¾å‡†)
            )

            self.model = AutoModelForCausalLM.from_pretrained(
                mid, 
                quantization_config=q_config_4bit, 
                device_map="auto", 
                max_memory=max_memory_map, 
                trust_remote_code=True,
                dtype=torch.float16,
                attn_implementation="sdpa"
            )
            print(f"   -> âœ… æ¨¡å‹ä»¥ 4-bit æ¨¡å¼åŠ è½½æˆåŠŸ")

            # 3. [å…³é”®è¡¥ä¸] ä¸º 4-bit è®­ç»ƒåšé¢„å¤„ç†
            # è¿™ä¸€æ­¥éå¸¸é‡è¦ï¼å¦‚æœä¸åŠ ï¼Œåç»­ SFT/DPO è®­ç»ƒæ—¶æ¢¯åº¦æ— æ³•åå‘ä¼ æ’­
            print("   -> ğŸ”§ [ç³»ç»Ÿ] æ­£åœ¨ä¸º 4-bit è®­ç»ƒè¿›è¡Œé¢„å¤„ç† (kbit_training)...")
            # use_gradient_checkpointing=True: ç‰ºç‰²ä¸€ç‚¹é€Ÿåº¦æ¢å–å¤§é‡æ˜¾å­˜ï¼Œé˜²æ­¢OOM
            self.model = prepare_model_for_kbit_training(self.model, use_gradient_checkpointing=True)

        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # å¤±è´¥åå°è¯•æ¸…ç†æ˜¾å­˜
            gc.collect()
            torch.cuda.empty_cache()
            raise e # å½»åº•æ²¡æ•‘äº†ï¼ŒæŠ›å‡ºå¼‚å¸¸é€€å‡º

        # 4. å‡†å¤‡åŒè„‘ (Fast/Slow LoRA)
        print("   -> ğŸ§  æ­£åœ¨é…ç½®è®­ç»ƒç¯å¢ƒ (æ¢¯åº¦æ£€æŸ¥ç‚¹)...")
        
        # ä»…æ£€æµ‹æ˜¯å¦ä¸º 4-bit é‡åŒ–
        is_quantized = getattr(self.model, "is_loaded_in_4bit", False)

        # ä»…å¤„ç†éé‡åŒ– (FP16/BF16) çš„æƒ…å†µ
        # å› ä¸ºé‡åŒ–æ¨¡å‹åœ¨ä¸Šä¸€æ­¥åŠ è½½æ—¶å·²ç»å¤„ç†è¿‡ prepare_model_for_kbit_training äº†
        if not is_quantized:
            print("   -> ğŸ”§ [ç³»ç»Ÿ] æ£€æµ‹åˆ° FP16 æ¨¡å¼ï¼Œæ‰‹åŠ¨å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹...")
            if hasattr(self.model, "enable_input_require_grads"):
                self.model.enable_input_require_grads()
            
            # å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œé˜²æ­¢ FP16 è®­ç»ƒ OOM
            self.model.gradient_checkpointing_enable()

        # ä½¿ç”¨å¤–ç§‘åŒ»ç”ŸåŠ è½½åŒ Adapter (Fast=ç›´è§‰, Slow=æ·±æ€)
        self.model = NeuroSurgeon.setup_dual_brain(self.model) 
        self.model.set_adapter("fast") # é»˜è®¤å¤„äºç›´è§‰æ¨¡å¼
        
        # 5. åˆå§‹åŒ–é«˜çº§å™¨å®˜
        # PFC: è´Ÿè´£å…ƒè®¤çŸ¥è·¯ç”±
        self.pfc = PrefrontalCortex(self.model.config.hidden_size, device=self.model.device)

        # Web: è´Ÿè´£è¿æ¥ç°å®ä¸–ç•Œ (æå‰åˆå§‹åŒ–ï¼Œä½œä¸º DreamWeaver çš„ä¾èµ–)
        # headless=False è®©ä½ èƒ½çœ‹åˆ°æµè§ˆå™¨æ“ä½œï¼Œéå¸¸é…·
        self.web = RealBrowserEye(headless=False)

        # äº‘ç«¯å¯¼å¸ˆ (æå‰åˆå§‹åŒ–ï¼Œä½œä¸º DreamWeaver çš„ä¾èµ–)
        # å³ä½¿ Config é‡Œæ²¡å¡« Keyï¼ŒCloudTeacher å†…éƒ¨ä¹Ÿä¼šå¤„ç†æˆ Noneï¼Œä¸ä¼šæŠ¥é”™
        self.teacher = CloudTeacher()

        # æå‰åˆå§‹åŒ–å›¾è°±æµ·é©¬ä½“ (å› ä¸º DreamWeaver éœ€è¦è¯»å–è®°å¿†è¿›è¡Œ RAG éªŒè¯)
        print("ğŸ§  [æµ·é©¬ä½“] æ­£åœ¨é“¾æ¥ç¥ç»å…ƒå›¾è°±...")
        self.hippocampus = GraphHippocampus(self.model, self.tokenizer, self.memory_db)

        # å®ä¾‹åŒ– é€ æ¢¦å¸ˆ
        # å¿…é¡»ä¼ å…¥ teacher, web, hippocampus ä»¥ä¾¿è¿›è¡Œ"è‡ªæˆ‘è¯„ä¼°"å’Œ"è”ç½‘çº é”™"
        self.dream_weaver = DreamWeaver(
            self.model, 
            self.tokenizer, 
            teacher=self.teacher, 
            web_eye=self.web, 
            hippocampus=self.hippocampus,
            memory_db=self.memory_db
        )

        # Plugin: è´Ÿè´£å®‰å…¨åœ°è‡ªæˆ‘ç¼–ç¨‹
        self.plugin_manager = PluginManager() 

        # åŸºå› ç¼–è¾‘å™¨ (ç”¨äºå¤„ç† PATCH æŒ‡ä»¤)
        self.editor = GeneticEditor()
        
        # æ ¸å¿ƒç”Ÿç‰©æœ¬èƒ½ç»„ä»¶ (Bio-Upgrade S-Tier)
        self.curiosity = IntrinsicCuriosityModule(self.genome) 
        
        # [æ¤å…¥] ç¥ç»å†…åˆ†æ³Œç³»ç»Ÿ (ç®¡ç† Dopamine, Serotonin, Cortisol)
        self.endocrine = NeuroEndocrineSystem(self.state)
        
        # æå‰åˆå§‹åŒ– GWT (å…¨å±€å·¥ä½œç©ºé—´)
        self.gwt = GlobalWorkspace()

        # [æ¤å…¥] æä»æ ¸ (ææƒ§ä¸­å¿ƒ)
        self.amygdala = AsyncAmygdala(self.genome, self.gwt, self.endocrine) 
        self.amygdala.start() # <--- å¯åŠ¨çº¿ç¨‹   
        
        # [æ¤å…¥] å‰æ‰£å¸¦çš®å±‚ (é¢„æµ‹è¯¯å·®ç›‘æ§)
        self.acc = AnteriorCingulateCortex(self.model, self.tokenizer)

        # åˆå§‹åŒ– SNN ç¥ç»çš®å±‚
        # å¼ºåˆ¶ä½¿ç”¨ CPU æ¨¡å¼ï¼Œé¿å…ä¸ä¸»æ¨¡å‹äº‰æŠ¢æ˜¾å­˜
        print(f"ğŸ§  [Cortex] æ­£åœ¨åˆå§‹åŒ–é«˜ç»´ç¥ç»çš®å±‚ (Dim: {Config.SNN_DIM} | Device: CPU)...")

        self.cortex = NezhaNeuralCortex(
            input_dim=current_dim, 
            hidden_dim=Config.SNN_DIM,
            genome=self.genome,  # <--- [æ–°å¢] æ³¨å…¥ self.genome
            device="cpu"
        )

        # ç¡®ä¿åŠ è½½æ—¶æ˜ å°„åˆ° CPUï¼Œé˜²æ­¢æƒé‡æ–‡ä»¶é‡Œè®°å½•äº† cuda:0 å¯¼è‡´è‡ªåŠ¨å ç”¨æ˜¾å­˜
        self.cortex.load()

        # åˆå§‹åŒ–ç¥ç»è„‘æ¡¥ (è¿æ¥ SNN ä¸ LLM)
        llm_hidden_size = self.model.config.hidden_size
        self.neural_bridge = NeuralBridge(
            snn_dim=Config.SNN_DIM,  # å¯¹åº” SNN çš„ hidden_dim,å¿…é¡»ä¸ Cortex å¯¹é½
            llm_dim=llm_hidden_size, 
            num_tokens=2,         # æ’å…¥ 2 ä¸ªæ½œæ„è¯† Token
            device=self.model.device    # è„‘æ¡¥ä¸»ä½“åœ¨ GPU
        ).to(self.model.device)

        print(f"ğŸŒ‰ [è„‘æ¡¥] ç¥ç»æ¥å£å·²å»ºç«‹ (SNN -> LLM Dim:{llm_hidden_size})")

        # ç«‹å³æ¿€æ´»å·²æœ‰çš„æŠ€èƒ½ (å¼€æœºè‡ªå¯)
        # ä¼ å…¥ selfï¼Œè®©æ’ä»¶ä¿®æ”¹å½“å‰å®ä¾‹
        num_skills = self.plugin_manager.activate_skills(self)
        if num_skills > 0:
            print(f"âœ¨ [è¿›åŒ–] å·²æˆåŠŸç»§æ‰¿ {num_skills} ä¸ªä»å‰ä¸–ä¿ç•™çš„æŠ€èƒ½ã€‚")

        # 1. å°è¯•æ¢å¤ è®°å¿†ç¼“å†²åŒº (æ•°æ®æ¥åŠ›)
        buffer_path = os.path.join(Config.EVOLUTION_DIR, "bridge_buffer.pkl")
        if os.path.exists(buffer_path):
            try:
                with open(buffer_path, 'rb') as f:
                    loaded_buffer = pickle.load(f)
                    # ç®€å•æ ¡éªŒä¸€ä¸‹ç±»å‹ï¼Œé˜²æ­¢è¯»å–äº†æŸåçš„æ–‡ä»¶
                    if isinstance(loaded_buffer, SemanticEpisodicBuffer):
                        self.bridge_buffer = loaded_buffer
                        print(f"   -> ğŸŒ‰ [è®°å¿†æ¥åŠ›] è„‘æ¡¥ç¼“å†²åŒºå·²æ¢å¤ (Items: {len(self.bridge_buffer)} | Total Seen: {self.bridge_buffer.total_seen})")
            except Exception as e:
                print(f"   -> âš ï¸ [è„‘æ¡¥] ç¼“å†²åŒºåŠ è½½å¤±è´¥ï¼Œé‡ç½®ä¸ºå†·å¯åŠ¨: {e}")
        else:
            print("   -> ğŸŒ‰ [è„‘æ¡¥] å¤„äºå†·å¯åŠ¨æ¨¡å¼ (æ— å†å²ç§¯æ·€)ã€‚")

        # 2. å°è¯•æ¢å¤ VAE æƒé‡ (ç›´è§‰æ¥åŠ›)
        bridge_w_path = os.path.join(Config.EVOLUTION_DIR, "neural_bridge.pt")
        if os.path.exists(bridge_w_path):
            try:
                # è¿™é‡Œçš„ map_location å¿…é¡»è·Ÿéš self.model.device (é˜²æ­¢å­˜çš„æ˜¯ CUDA åŠ è½½åˆ° CPU æŠ¥é”™ï¼Œåä¹‹äº¦ç„¶)
                self.neural_bridge.load_state_dict(torch.load(bridge_w_path, map_location=self.model.device))
                print(f"   -> ğŸŒ‰ [ç›´è§‰æ¥åŠ›] æ½œæ„è¯†æƒé‡å·²å¯¹é½ã€‚")
            except Exception as e:
                print(f"   -> âš ï¸ [è„‘æ¡¥] æƒé‡åŠ è½½å¤±è´¥: {e}")

        # 6. åˆå§‹åŒ–å­¦ä¹ ç³»ç»Ÿ
        # (æµ·é©¬ä½“ hippocampus å·²åœ¨ä¸Šæ–¹æå‰åˆå§‹åŒ–ï¼Œè¿™é‡Œä¸å†é‡å¤)
        
        print("ğŸ›¡ï¸ [å…ç–«ç³»ç»Ÿ] æ­£åœ¨æ¿€æ´» EWC é•¿æœŸè®°å¿†ä¿æŠ¤...")
        # 2. å®ä¾‹åŒ–çœŸæ­£çš„ EWC (åˆ é™¤äº†é‚£ä¸ª placeholder)
        # æ³¨æ„ï¼šå¿…é¡»æŠŠ self.hippocampus ä¼ è¿›å»ï¼Œè®©å®ƒèƒ½è¯»å–æ—§è®°å¿†
        self.ewc_handler = EWC(self.model, self.tokenizer, self.hippocampus, device=self.model.device)

        # === å¥‡ç‚¹å†…æ ¸åˆå§‹åŒ– (Singularity Kernel) ====
        print("ğŸŒŒ [Singularity] æ­£åœ¨åˆå§‹åŒ–ä¸»åŠ¨æ¨ç†å¼•æ“ä¸å…¨å±€å·¥ä½œç©ºé—´...")
        
        # 1. åˆå§‹åŒ–ä¸–ç•Œæ¨¡å‹ (ç¥ç»åŠ¨åŠ›å­¦ç‰ˆ)
        self.world_model = NeuralWorldModel(Config.ACTION_SPACE)
        
        # 2. åˆå§‹åŒ–ä¸»åŠ¨æ¨ç†å¼•æ“ (é—­ç¯ç‰ˆ)
        self.inference_engine = ActiveInferenceEngine(self.world_model, Config.ACTION_SPACE)
        
        # 3. åˆå§‹åŒ–å¤šè·¯å¾„æ¨ç†æœº (åŸé‡å­è„‘)
        print("ğŸŒŒ [å¤šè·¯å¾„æ¨ç†] æ­£åœ¨æ„å»ºè’™ç‰¹å¡æ´›æ€ç»´æ ‘...")
        self.quantum_brain = MultiPathReasoning(
            model=self.model, 
            tokenizer=self.tokenizer, 
            left_brain=self.left_brain, 
            web_eye=self.web,
            teacher=self.teacher,
            memory_db=self.memory_db
        )
        
        # 4. åˆå§‹åŒ–å…ƒè®¤çŸ¥ç¼–ç¨‹å™¨ (è‡ªæˆ‘è¿›åŒ–)
        self.meta_programmer = MetaCognitiveProgrammer()
        
        # 5. åˆå§‹åŒ–åŠ¨æ€ Prompt (ç”±å®ªæ³• + åˆå§‹ç­–ç•¥ç»„æˆ)
        # è¿™å°†æ›¿ä»£åŸæ¥çš„å›ºå®š sys_p
        self.dynamic_system_prompt = (
            f"{self.meta_programmer.CONSTITUTION}\n"
            f"ç³»ç»Ÿèƒ½åŠ›: SELF(æœ¬èƒ½), WEB(æœç´¢), TEACHER(è¯·æ•™), CODE(ç¼–ç¨‹)ã€‚\n"
            f"å½“å‰ç­–ç•¥: ä¼˜å…ˆä½¿ç”¨ SELF å›ç­”ï¼Œé‡åˆ°æœªçŸ¥ä½¿ç”¨ WEBï¼Œé‡åˆ°é€»è¾‘å›°éš¾ä½¿ç”¨ CODEã€‚"
        )

        # Feeder: è´Ÿè´£å¤œé—´åƒä¹¦å’Œåæ€æ—¥è®°
        # Feeder ç°åœ¨å…·å¤‡ decide_strategy èƒ½åŠ›
        self.feeder = AcademicFeeder(self.model, self.tokenizer, self.ewc_handler)
        
        # å¤‡ä»½è‡ªèº«ä»£ç ä½œä¸ºåˆå§‹çŸ¥è¯† (è‡ªæˆ‘è®¤çŸ¥)
        if not os.path.exists(os.path.join(Config.KNOWLEDGE_BASE, "me_source.py")):
            shutil.copy(Config.SCRIPT_PATH, os.path.join(Config.KNOWLEDGE_BASE, "me_source.py"))

        print(f"âœ¨ [è‹é†’] Age:{self.state.get('age',0)} | Layer:{self.model.config.num_hidden_layers} | ATP:{self.state['atp']:.1f}")

        # === å¯åŠ¨åå°æ½œæ„è¯†çº¿ç¨‹ ===
        # daemon=True è¡¨ç¤ºå½“ä¸»ç¨‹åºé€€å‡ºæ—¶ï¼Œè¿™ä¸ªçº¿ç¨‹ä¹Ÿä¼šè‡ªåŠ¨éšä¹‹å…³é—­
        t = threading.Thread(target=self._autonomous_loop, daemon=True)
        t.start()

    def _set_lora_scaling(self, target_alpha):
        """
        [è¾…åŠ©æ–¹æ³•] å¿«é€Ÿè®¾ç½®æ‰€æœ‰æ´»è·ƒ LoRA å±‚çš„ scaling (å«ç¼“å­˜æœºåˆ¶)
        """
        # 1. æ‡’åŠ è½½ç¼“å­˜ (é¿å…æ¯æ¬¡éƒ½éå†å‡ ä¸‡ä¸ª modules)
        if not hasattr(self, '_lora_modules_cache'):
            self._lora_modules_cache = []
            for name, module in self.model.named_modules():
                # åªç¼“å­˜åŒ…å« scaling å±æ€§çš„ LoRA å±‚
                if "lora_" in name and hasattr(module, "scaling"):
                    self._lora_modules_cache.append(module)

        # 2. å¿«é€Ÿéå†å¹¶èµ‹å€¼
        # è·å–å½“å‰æ¿€æ´»çš„ adapter (é»˜è®¤ä¸º "fast")
        active_adapter = getattr(self.model, "active_adapter", "fast")
        
        # é˜²å¾¡æ€§ç¼–ç¨‹: ç¡®ä¿ active_adapter æ˜¯å­—ç¬¦ä¸²
        if isinstance(active_adapter, str):
            for module in self._lora_modules_cache:
                if active_adapter in module.scaling:
                    # Scaling = Alpha / Rank
                    # ç›´æ¥ä¿®æ”¹ scaling å±æ€§ï¼Œç«‹å³ç”Ÿæ•ˆ
                    module.scaling[active_adapter] = target_alpha / module.r


    def _load_state(self):
        """
        [State Management] åŠ è½½ç”Ÿå‘½çŠ¶æ€
        åŒ…å«åŸºå› å›æ»šæœºåˆ¶ã€æ—§å­˜æ¡£å…¼å®¹è¡¥å…¨ä»¥åŠé»˜è®¤ç”Ÿç†æŒ‡æ ‡çš„åˆå§‹åŒ–ã€‚
        """
        # å®šä¹‰æ ‡å‡†é»˜è®¤çŠ¶æ€ (Standard Biological State)
        default = {
            # === 1. åŸºç¡€ä»£è°¢æŒ‡æ ‡ (Metabolic Metrics) ===
            "atp": 100.0,         # [èƒ½é‡åº•ç‰©] ç”Ÿå­˜çš„æ ¹æœ¬ã€‚æ€è€ƒã€è¡ŒåŠ¨ã€è¿›åŒ–éƒ½éœ€è¦æ¶ˆè€— ATPã€‚è€—å°½å³æ­»ã€‚
            "entropy": 0.5,       # [è®¤çŸ¥ç†µ/è‡ªç”±èƒ½] è¡¡é‡å¯¹ä¸–ç•Œçš„ä¸ç¡®å®šæ€§ã€‚FEP ç†è®ºæ ¸å¿ƒï¼Œç›®æ ‡æ˜¯æœ€å°åŒ–æ­¤å€¼ã€‚
            "age": 0,             # [ç”Ÿå­˜æ—¶é’Ÿ] å­˜æ´»çš„æ—¶é—´æ­¥æ•° (Ticks)ã€‚ç”¨äºè®¡ç®—é•¿å¯¿å¥–åŠ± (Longevity Bonus)ã€‚
            
            # === 2. è¿›åŒ–ä¸é—ä¼  (Evolutionary & Genetic) ===
            "evo_pts": 0,                 # [è¿›åŒ–ç‚¹æ•°] è¿›åŒ–çš„è´§å¸ï¼Œç”¨äºå…‘æ¢åŸºå› çªå˜æˆ–å™¨å®˜å‡çº§ã€‚
            "genome": Config.DEFAULT_GENOME.copy(), # [åŸºå› ç»„] å†³å®šè¡Œä¸ºæ¨¡å¼çš„è¶…å‚æ•°é›†åˆ (å¦‚å­¦ä¹ ç‡ã€èƒ†é‡ç­‰)ã€‚
            
            # === 3. é£é™©è¯„ä¼°è®°å¿† (Risk Memory) [New for Sortino] ===
            # ç”¨äºå­˜å‚¨æœ€è¿‘ N è½®çš„æƒŠå¥‡åº¦ (Loss)ï¼Œä¾›è´å¶æ–¯å¼•æ“è®¡ç®—ä¸‹è¡Œé£é™© (Downside Deviation)ã€‚
            # å¦‚æœç¼ºå¤±æ­¤é¡¹ï¼Œç´¢æè¯ºæ¯”ç‡å°†é€€åŒ–ä¸ºæ™®é€šå‡å€¼è®¡ç®—ã€‚
            "loss_history": [], 
            
            # === 4. é‡å­æ€ç»´é¥æµ‹ (Quantum Telemetry) ===
            # è®°å½• MCTS (è’™ç‰¹å¡æ´›æ ‘æœç´¢) æˆ–å¤šè·¯å¾„æ¨ç†çš„æ€§èƒ½ç»Ÿè®¡
            "quantum_stats": {
                "attempts": 0,          # [å‘æ•£] å°è¯•è¿›è¡Œæ€ç»´å‘æ•£çš„æ€»æ¬¡æ•°
                "successes": 0,         # [åç¼©] æˆåŠŸæ”¶æ•›åˆ°æœ‰æ•ˆè§£çš„æ¬¡æ•° (æ³¢å‡½æ•°åç¼©)
                "energy_consumed": 0.0, # [èƒ½è€—] é‡å­æ€ç»´æ¶ˆè€—çš„æ€» ATP (ç”¨äºè¯„ä¼°è®¤çŸ¥æˆæœ¬)
                
                # [è‡ªé€‚åº”é˜ˆå€¼] åŠ¨æ€è°ƒæ•´è¿›å…¥é‡å­æ€çš„é—¨æ§›
                # å¦‚æœæ€ç»´æˆåŠŸç‡é«˜ï¼Œé—¨æ§›ä¼šé™ä½ï¼›å¦‚æœç»å¸¸å¤±è´¥ï¼Œé—¨æ§›ä¼šå‡é«˜ä»¥é€šè¿‡èŠ‚èƒ½
                "adaptive_threshold": Config.ATP_THRESHOLD_QUANTUM 
            }            
        }
        
        try:
            if os.path.exists(Config.STATE_FILE):
                st = json.load(open(Config.STATE_FILE))
                
                # [å…³é”®ä¿®å¤] éå†é»˜è®¤å­—å…¸ï¼Œè¡¥å…¨å­˜æ¡£ä¸­ç¼ºå¤±çš„é¡¶å±‚é”® (å¦‚ evo_pts)
                for key, val in default.items():
                    if key not in st:
                        st[key] = val
                
                # [äºŒçº§è¡¥å…¨] ç¡®ä¿ genome é‡Œçš„æ–°åŸºå› ä¹Ÿå­˜åœ¨ (é˜²æ­¢ V2.0 æ–°åŸºå› ç¼ºå¤±)
                if "genome" not in st: 
                    st["genome"] = default["genome"]
                else:
                    for k, v in Config.DEFAULT_GENOME.items():
                        if k not in st["genome"]:
                            st["genome"][k] = v

                # [åŸºå› å›æ»š] å¦‚æœä¸Šæ¬¡è½¬ä¸–å¾ˆå¿«å°±æŒ‚äº†(Age<2)ï¼Œè¯´æ˜è¿›åŒ–å¤±è´¥ï¼Œå›æ»šåˆ°ç¥–å…ˆå­˜æ¡£
                if st.get('age', 0) < 2 and os.path.exists(Config.BEST_STATE_FILE):
                    print("ğŸ§¬ [åŸºå› å›æ»š] æ£€æµ‹åˆ°æ—©å¤­ï¼Œå›æ»šè‡³æœ€ä¼˜ç¥–å…ˆåŸºå› ...")
                    try:
                        best_st = json.load(open(Config.BEST_STATE_FILE))
                        # åŒæ ·è¦è¡¥å…¨ç¥–å…ˆå­˜æ¡£ï¼Œé˜²æ­¢ç¥–å…ˆä¹Ÿæ˜¯æ—§ç‰ˆæœ¬
                        for key, val in default.items():
                            if key not in best_st: best_st[key] = val
                        return best_st
                    except:
                        pass # è¯»å–å¤±è´¥åˆ™å¿½ç•¥
                        
                return st
        except Exception as e: 
            print(f"âš ï¸ å­˜æ¡£è¯»å–å¼‚å¸¸ï¼Œä½¿ç”¨é»˜è®¤çŠ¶æ€: {e}")
            pass
            
        return default

    def _save_state(self):
        """ä¿å­˜ç”Ÿå‘½çŠ¶æ€ (å·²åŠ é”ä¿æŠ¤ï¼Œé˜²æ­¢å¤šçº¿ç¨‹å†™å…¥å†²çª)"""
        with self.lock: # <--- å…³é”®ï¼šåªæœ‰æ‹¿åˆ°é”æ‰èƒ½å†™æ–‡ä»¶
            
            # æ¿€ç´ çŠ¶æ€åŒæ­¥ (State Synchronization)
            # å¿…é¡»åœ¨å†™å…¥ JSON ä¹‹å‰ï¼Œå°†å†…åˆ†æ³Œç³»ç»Ÿçš„å½“å‰æ•°å€¼åŒæ­¥å› self.state å­—å…¸
            # å¦åˆ™é‡å¯åï¼Œå“ªå’çš„æƒ…ç»ªä¼šé‡ç½®ä¸ºé»˜è®¤å€¼
            if hasattr(self, 'endocrine'):
                self.state['dopamine'] = float(self.endocrine.da)
                self.state['norepinephrine'] = float(self.endocrine.ne)
                self.state['serotonin'] = float(self.endocrine.ht)
                self.state['cortisol'] = float(self.endocrine.cortisol)

            # ä¿å­˜æä»æ ¸åŠ«æŒçŠ¶æ€ (å…³é”®ä¿®å¤ï¼šé˜²æ­¢é‡å¯åé—å¿˜æˆ˜é€ƒçŠ¶æ€)
            self.state['is_hijacked'] = getattr(self, 'is_under_amygdala_hijack', False)

            # Deque åºåˆ—åŒ–é€‚é…
            # json.dump ä¸æ”¯æŒ dequeï¼Œå¿…é¡»è½¬å› list
            # è¿™ä¸€æ­¥å°†å†…å­˜ä¸­æœ€æ–°çš„ deque æ•°æ®åŒæ­¥å› self.state å­—å…¸
            self.state['loss_history'] = list(self.loss_history)

            # æ‰§è¡Œç‰©ç†å†™å…¥
            with open(Config.STATE_FILE, 'w') as f: 
                json.dump(self.state, f, indent=2)
            
            # ä¿å­˜ç¥ç»çš®å±‚æƒé‡ (Titans Memory)
            if hasattr(self, 'cortex'):
                self.cortex.save()

            # 1. ä¿å­˜ NeuralBridge (VAE) æƒé‡
            # VAE çš„å‚æ•°æ˜¯ PyTorch Tensorï¼Œå¿…é¡»ç”¨ torch.save
            if hasattr(self, 'neural_bridge'):
                bridge_path = os.path.join(Config.EVOLUTION_DIR, "neural_bridge.pt")
                try:
                    torch.save(self.neural_bridge.state_dict(), bridge_path)
                except Exception as e:
                    print(f"âš ï¸ [ä¿å­˜å¤±è´¥] è„‘æ¡¥æƒé‡: {e}")

            # 2. ä¿å­˜ SemanticEpisodicBuffer (æ•°æ® + total_seen)
            # ç¼“å†²åŒºåŒ…å« Dequeã€List å’Œ Tensorï¼Œç»“æ„å¤æ‚ï¼Œå¿…é¡»ç”¨ pickle åºåˆ—åŒ–
            if hasattr(self, 'bridge_buffer'):
                buffer_path = os.path.join(Config.EVOLUTION_DIR, "bridge_buffer.pkl")
                try:
                    with open(buffer_path, 'wb') as f:
                        pickle.dump(self.bridge_buffer, f)
                except Exception as e:
                    print(f"âš ï¸ [ä¿å­˜å¤±è´¥] è„‘æ¡¥è®°å¿†ç¼“å†²åŒº: {e}")


    def _safe_print(self, message):
        """
        æ— ç—•æ‰“å°å·¥å…·ï¼š
        å½“åå°çº¿ç¨‹æƒ³è¦è¯´è¯æ—¶ï¼Œå®ƒä¼šå…ˆæ¸…é™¤å½“å‰è¡Œçš„è¾“å…¥æç¤ºç¬¦ 'You: 'ï¼Œ
        æ‰“å°å®Œæ—¥å¿—åï¼Œå†æŠŠ 'You: ' è¡¥å›å»ã€‚
        è¿™æ ·ä½ çš„æ‰“å­—ç•Œé¢å°±ä¸ä¼šè¢«åå°æ—¥å¿—åˆ‡æ–­äº†ã€‚
        """
        # \r å›åˆ°è¡Œé¦–, \033[K æ¸…é™¤å½“å‰è¡Œ
        sys.stdout.write(f"\r\033[K{message}\n")
        # æ¢å¤è¾“å…¥æç¤ºç¬¦
        sys.stdout.write("You: ")
        sys.stdout.flush()

    def _autonomous_loop(self):
        """æ½œæ„è¯†è‡ªä¸»å¾ªç¯ (åå°å®ˆæŠ¤çº¿ç¨‹)"""
        # å¯åŠ¨åå…ˆæ²‰ç¡ä¸€ä¼šï¼Œé¿å…è·Ÿå¼€æœºæ—¥å¿—æ··åœ¨ä¸€èµ·
        time.sleep(5) 
        
        while True:
            # æ¯ 10 ç§’æ£€æŸ¥ä¸€æ¬¡æ˜¯å¦ç©ºé—²
            time.sleep(10)
            
            # çº¿ç¨‹å®‰å…¨åœ°è¯»å–æœ€åäº¤äº’æ—¶é—´
            last_active = getattr(self.acc, 'last_interaction_time', time.time())
            idle_time = time.time() - last_active

            # è§¦å‘æ¡ä»¶ï¼šç©ºé—²è¶…è¿‡ 60 ç§’ ä¸” èƒ½é‡å……è¶³ (ATP > 20)
            if idle_time > 60 and self.state['atp'] > 20:
                # 30% æ¦‚ç‡è§¦å‘ï¼Œé˜²æ­¢å¤ªé¢‘ç¹
                if random.random() < 0.3:

                    # éé˜»å¡æ¢é’ˆï¼šåªçœ‹ä¸€çœ¼é”æ˜¯ä¸æ˜¯ç©ºé—²çš„
                    is_free = self.lock.acquire(blocking=False)
                    
                    if is_free:
                        # å¿…é¡»ç«‹åˆ»é‡Šæ”¾ï¼å¦åˆ™å¦‚æœ _perform_god_action_silent å†…éƒ¨å†æ‹¿é”å°±ä¼šæ­»é” (è™½ç„¶ RLock å…è®¸é‡å…¥ï¼Œä½†é€»è¾‘ä¸Šè¦è§£è€¦)
                        self.lock.release()
                        
                        try:
                            # åœ¨ã€æ— é”ã€‘çŠ¶æ€ä¸‹æ‰§è¡Œè€—æ—¶çš„ç½‘ç»œ/æ€è€ƒæ“ä½œ
                            # è¿™æ ·ä¸»çº¿ç¨‹éšæ—¶å¯ä»¥æ‰“æ–­å®ƒï¼ˆæŠ¢å é”ï¼‰
                            self._perform_god_action_silent()
                            
                            # æ‰§è¡Œå®Œæ›´æ–°æ—¶é—´ï¼Œé¿å…è¿ç»­è§¦å‘
                            # è¿™æ˜¯ä¸€ä¸ªç®€å•çš„èµ‹å€¼ï¼ŒPython ä¸­å¯¹ float èµ‹å€¼é€šå¸¸æ˜¯åŸå­çš„ï¼Œä¸åŠ é”ä¹Ÿæ²¡å¤§äº‹
                            self.acc.last_interaction_time = time.time() - 30 
                        except Exception as e:
                            print(f"âš ï¸ [æ½œæ„è¯†] è¿è¡Œå¼‚å¸¸: {e}")
                    else:
                        # é”è¢«å ç”¨ï¼ˆä¸»çº¿ç¨‹æ­£åœ¨è¯´è¯ï¼‰ï¼Œé»˜é»˜é€€ä¸‹ï¼Œä¸æ‰“æ‰°
                        pass

    def _perform_god_action_silent(self):
        """
        [ç¥æ€§åŠ¨ä½œ V14.2 - çƒ­åŠ›å­¦å®ˆæ’ä¸ç²‰çº¢å™ªå£°åŠ¨åŠ›å­¦ç‰ˆ]
        (Thermodynamically Conserved Pink Noise Dynamics)
        
        [ç‰©ç†å­¦åŸç† / Physics]:
        1. æœ—ä¹‹ä¸‡åŠ¨åŠ›å­¦ (Langevin Dynamics): å°†æ€ç»´è§†ä¸ºåŠ¿èƒ½é¢ä¸Šçš„éšæœºæ¸¸èµ°ï¼Œæ¿€ç´ æ”¹å˜åŠ¿èƒ½é¢çš„å½¢çŠ¶ã€‚
        2. è‡ªå›å½’è¿‡ç¨‹ (AR-1 Process): æ½œæ„è¯†ä¸æ˜¯ç¬æ—¶çš„ç™½å™ªå£°ï¼Œè€Œæ˜¯å…·æœ‰æ—¶é—´è®°å¿†çš„ç²‰çº¢å™ªå£° (1/f Noise)ã€‚
        3. èƒ½é‡å®ˆæ’ (Energy Conservation): å™ªå£°ç³»æ•°ä¸¥æ ¼å½’ä¸€åŒ–ï¼Œé˜²æ­¢ç³»ç»Ÿåœ¨é•¿æ—¶é—´è¿è¡Œä¸­å‘ç”Ÿæ•°å€¼çˆ†ç‚¸ã€‚
        4. æ³¢å‡½æ•°åç¼© (Wavefunction Collapse): LLM çš„ç”Ÿæˆæ˜¯å¯¹æ½œæ„è¯†æ¦‚ç‡äº‘çš„ä¸€æ¬¡è§‚æµ‹ã€‚
        """

        # --------------------------------------------------------------------------
        # 0. åŸºç¡€ä»£è°¢æ£€æŸ¥ (Metabolic Check)
        # --------------------------------------------------------------------------
        # æ¿’æ­»ä¿æŠ¤æœºåˆ¶ï¼šå½“ ATP (ç”Ÿç‰©èƒ½é‡) ä½äºé˜ˆå€¼æ—¶ï¼Œå¼ºåˆ¶å…³é—­é«˜è€—èƒ½çš„æ½œæ„è¯†æ´»åŠ¨ï¼Œ
        # ä»…ä¿ç•™æœ€åº•å±‚çš„â€œæ¤ç‰©ç¥ç»â€åŠŸèƒ½ã€‚
        if self.state['atp'] < 5: return

        # ==============================================================================
        # ç¬¬ä¸€æ­¥ï¼šæ„å»º çƒ­åŠ›å­¦åœº (Thermodynamic Field Construction)
        # ç›®çš„ï¼šå°†ç”Ÿç‰©åŒ–å­¦ä¿¡å· (æ¿€ç´ ) æ˜ å°„ä¸º ç”Ÿæˆæ¨¡å‹çš„è¶…å‚æ•° (ç‰©ç†åœº)ã€‚
        # ==============================================================================
        
        # [Bio-Signal Acquisition] 
        # ä½¿ç”¨ getattr ç¡®ä¿æ—§ç‰ˆå­˜æ¡£çš„å…¼å®¹æ€§ï¼Œè¯»å–å†…åˆ†æ³Œç³»ç»Ÿçš„æ ‡é‡å€¼ã€‚
        da = getattr(self.endocrine, 'da', 0.5)        # å¤šå·´èƒº (Dopamine): å¥–èµé¢„æœŸï¼Œé©±åŠ¨æ¢ç´¢ã€‚
        ne = getattr(self.endocrine, 'ne', 0.5)        # å»ç”²è‚¾ä¸Šè…ºç´  (Norepinephrine): è­¦è§‰ï¼Œé©±åŠ¨èšç„¦ã€‚
        cort = getattr(self.endocrine, 'cortisol', 0.5) # çš®è´¨é†‡ (Cortisol): å‹åŠ›ï¼Œé©±åŠ¨é€ƒé¿/é˜²å¾¡ã€‚
        atp = self.state['atp']

        # [Parameter Modulation]
        
        # 1. æ¸©åº¦ (Temperature) -> ç³»ç»Ÿçš„ç†µ (Entropy)
        # ç‰©ç†æ„ä¹‰ï¼šå†³å®šäº†æ€ç»´ç²’å­åœ¨è¯­ä¹‰ç©ºé—´ä¸­çš„åŠ¨èƒ½ã€‚
        # é€»è¾‘ï¼šå¤šå·´èƒºè¶Šé«˜ï¼ŒTè¶Šé«˜ï¼Œæ€ç»´è¶Šå‘æ•£ (Creativity)ï¼›å»ç”²è¶Šé«˜ï¼ŒTè¶Šä½ï¼Œæ€ç»´è¶Šå†·å´ (Stability)ã€‚
        # çº¦æŸï¼šé™åˆ¶åœ¨ [0.1, 1.4] ä¹‹é—´ï¼Œé˜²æ­¢çƒ­å¯‚ (0.0) æˆ– è¯­ä¹‰å´©è§£ (>1.5)ã€‚
        temperature = max(0.1, min(1.4, 0.7 + (da * 0.6) - (ne * 0.3)))
        
        # 2. é‡å¤æƒ©ç½š (Repetition Penalty) -> åŠ¿èƒ½äº•çš„æ·±åº¦ (Potential Well Depth)
        # ç‰©ç†æ„ä¹‰ï¼šå†³å®šäº†æ€ç»´è·³å‡ºå½“å‰è¯é¢˜çš„éš¾æ˜“ç¨‹åº¦ã€‚
        # é€»è¾‘ï¼šç„¦è™‘ (NE) é«˜æ—¶ï¼Œæƒ©ç½šé™ä½ï¼Œæ¨¡æ‹Ÿç”Ÿç‰©çš„â€œååˆâ€ (Rumination) æˆ–â€œé’»ç‰›è§’å°–â€ç°è±¡ã€‚
        # æ”¾æ¾æ—¶ï¼Œæƒ©ç½šå‡é«˜ï¼Œæ€ç»´å€¾å‘äºä¸èµ°å›å¤´è·¯ã€‚
        rep_penalty = max(1.0, 1.2 - (ne * 0.25))
        
        # 3. é‡‡æ ·æ ¸ (Top-P) -> æ¦‚ç‡æ³¢åŒ…çš„åç¼©åŠå¾„ (Collapse Radius)
        # ç‰©ç†æ„ä¹‰ï¼šå†³å®šäº†è§‚æµ‹æ—¶ä¿ç•™å¤šå°‘ç§å¯èƒ½æ€§ã€‚
        # é€»è¾‘ï¼šå‹åŠ› (Cortisol) æé«˜æ—¶ï¼ŒTop-P æä½ï¼Œæ¨¡æ‹Ÿâ€œéš§é“è§†é‡â€ (Tunnel Vision) â€”â€” 
        # ç”Ÿç‰©åœ¨å±æ€¥æ—¶åˆ»åªå…³æ³¨ç”Ÿå­˜æ¦‚ç‡æœ€é«˜çš„é€‰é¡¹ï¼Œå¿½ç•¥æ— å…³ç»†èŠ‚ã€‚
        top_p = max(0.05, 0.95 - (cort * 0.5))

        # ==============================================================================
        # ç¬¬äºŒæ­¥ï¼šæœ‰è‰²å™ªå£°æ½œæ„è¯†æµ (Colored Noise Subconscious Stream)
        # ç›®çš„ï¼šç”Ÿæˆå…·æœ‰æ—¶é—´è¿è´¯æ€§çš„ç¥ç»èƒŒæ™¯å™ªéŸ³ï¼Œå¹¶æŠ•å°„ä¸ºè¯­ä¹‰å‘é‡ã€‚
        # ==============================================================================
        
        if hasattr(self, 'cortex') and hasattr(self, 'neural_bridge'):
            try:
                # [Dimensionality Check] 
                # åŠ¨æ€è·å– Cortex çš„è¾“å…¥ç»´åº¦ï¼Œé€‚åº”ç¥ç»ç½‘ç»œå¯èƒ½çš„è‡ªæˆ‘è¿›åŒ–æˆ–ç»“æ„è°ƒæ•´ã€‚
                input_dim = self.cortex.input_dim if hasattr(self.cortex, 'input_dim') else 1024
                device = self.cortex.device
                
                # [Thermal Fluctuation] 
                # ç”Ÿæˆç¬æ—¶ç™½å™ªå£° (White Noise)ï¼Œä»£è¡¨ç¥ç»å…ƒçš„çƒ­è¿åŠ¨ï¼Œæ— è®°å¿†ï¼Œçº¯éšæœºã€‚
                white_noise = torch.randn(1, input_dim).to(device)
                
                # [Engineering Robustness] 
                # è®°å¿†ä½“åˆå§‹åŒ–ä¸å¯¹é½æ£€æŸ¥ï¼Œé˜²æ­¢ CUDA é”™è¯¯æˆ–ç»´åº¦ä¸åŒ¹é…å¯¼è‡´çš„å´©æºƒã€‚
                if (not hasattr(self, '_last_noise')) or (self._last_noise is None):
                    self._last_noise = torch.zeros_like(white_noise)
                
                # æ˜¾å­˜è®¾å¤‡å¯¹é½ (Data Locality)
                if self._last_noise.device != device:
                    self._last_noise = self._last_noise.to(device)
                
                # å½¢çŠ¶å¯¹é½ (Shape Consistency)
                if self._last_noise.shape != white_noise.shape:
                     self._last_noise = torch.zeros_like(white_noise)

                # [Math Core: AR(1) Process] 
                # ä¿®æ­£åçš„è‡ªå›å½’è¿‡ç¨‹ï¼Œéµå¾ªèƒ½é‡å®ˆæ’å®šå¾‹ã€‚
                # å…¬å¼: X_t = 0.9 * X_{t-1} + 0.1 * E_t
                # ç³»æ•°å’Œ (0.9 + 0.1 = 1.0) ä¿è¯äº†åºåˆ—çš„æ–¹å·®åœ¨é•¿æ—¶é—´å°ºåº¦ä¸Šæ˜¯æœ‰é™çš„ (Stationary)ã€‚
                # 0.9 ä»£è¡¨æ€ç»´çš„æƒ¯æ€§ (Memory/Inertia)ï¼Œ0.1 ä»£è¡¨å½“ä¸‹çš„çµæ„Ÿ/æ‰°åŠ¨ (Innovation)ã€‚
                pink_noise = 0.9 * self._last_noise + 0.1 * white_noise
                
                # [State Update]
                # æ›´æ–°è®°å¿†ä½“ï¼Œä½¿ç”¨ .detach() åˆ‡æ–­ PyTorch è®¡ç®—å›¾ï¼Œé˜²æ­¢æ¢¯åº¦ç´¯ç§¯å¯¼è‡´æ˜¾å­˜æ³„æ¼ã€‚
                self._last_noise = pink_noise.detach()
                
                # [Energy Modulation]
                # æŒ¯å¹…è°ƒåˆ¶ï¼šATP å†³å®šäº†æ½œæ„è¯†ä¿¡å·çš„â€œå“åº¦â€ (Signal Amplitude)ã€‚
                # èƒ½é‡è¶Šä½ï¼Œæ½œæ„è¯†çš„å£°éŸ³è¶Šå¾®å¼±ã€‚
                noise_scale = atp / 80.0 
                
                # [SNN Dynamics]
                # å°†å™ªå£°è¾“å…¥è„‰å†²ç¥ç»ç½‘ç»œ (Spiking Neural Network)ï¼Œäº§ç”Ÿéçº¿æ€§çš„ç”Ÿç‰©ç”µååº”ã€‚
                snn_rate = self.cortex.forward_dynamic(
                    x_input=pink_noise * noise_scale, 
                    atp=atp, 
                    pressure=self.amygdala.fear_level
                )
                
                # [Semantic Projection]
                # é€šè¿‡å˜åˆ†è‡ªç¼–ç å™¨ (VAE) è„‘æ¡¥ï¼Œå°†ç”Ÿç‰©ç”µå‘æ”¾ç‡æ˜ å°„ä¸º LLM å¯ç†è§£çš„ Soft Embeddingsã€‚
                soft_embeds = self.neural_bridge(snn_rate).to(self.model.device).to(self.model.dtype)
            
            except Exception as e:
                # å®¹é”™ï¼šæ½œæ„è¯†å±‚é¢çš„å¾®å°æ•…éšœä¸åº”å¯¼è‡´è„‘æ­»äº¡ (Main Thread Crash)ã€‚
                return
        else:
            return

        # ==============================================================================
        # ç¬¬ä¸‰æ­¥ï¼šæ³¢å‡½æ•°åç¼© (Wavefunction Collapse)
        # ç›®çš„ï¼šåˆ©ç”¨ LLM ä½œä¸ºè§‚æµ‹å™¨ï¼Œå°†æ½œæ„è¯†çš„æ¨¡ç³Šå†²åŠ¨åç¼©ä¸ºå…·ä½“çš„è‡ªç„¶è¯­è¨€å¿µå¤´ã€‚
        # ==============================================================================
        
        self._safe_print(f"â˜ï¸ [æ½œæ„è¯†] ç¥ç»ç”µä½æ¶¨è½... (T={temperature:.2f}, Noise=Pink)")

        try:
            # [The "Primer"]
            # æ–½åŠ å¾®å¼±çš„åˆå§‹åŠ¨é‡ã€‚ç›¸æ¯”å¼ºç¡¬çš„ Promptï¼Œè¿™æ›´åƒæ˜¯ä¸€ä¸ªæå…¶å¾®å°çš„æ‰°åŠ¨ï¼Œ
            # æ‰“ç ´ç³»ç»Ÿçš„å¯¹ç§°æ€§ï¼Œè¯±å¯¼ LLM é¡ºç€ Soft Embeds çš„æ–¹å‘å¼€å§‹ç”Ÿæˆã€‚
            starter_text = "æ€ç»´æ¶Œç°ï¼š" 
            
            # å°†å¼•å¯¼è¯è½¬åŒ–ä¸º Embedding
            starter_ids = self.tokenizer(starter_text, return_tensors="pt").input_ids.to(self.model.device)
            starter_embeds = self.model.get_input_embeddings()(starter_ids)
            
            # [Physical Fusion]
            # ç‰©ç†æ‹¼æ¥: [æ½œæ„è¯†æ„Ÿè§‰ (Soft Vectors)] + [è¯­è¨€å¼•å¯¼ (Hard Vectors)]
            # è¿™æ˜¯ä¸€ä¸ªæ··åˆæ¨¡æ€çš„è¾“å…¥ã€‚
            inputs_embeds = torch.cat([soft_embeds, starter_embeds], dim=1)
            attention_mask = torch.ones(inputs_embeds.shape[:2], device=self.model.device)

            # [Sampling]
            # åœ¨çƒ­åŠ›å­¦åœºå‚æ•°çš„çº¦æŸä¸‹è¿›è¡Œé‡‡æ ·ã€‚
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs_embeds=inputs_embeds,
                    attention_mask=attention_mask,
                    max_new_tokens=60,       # ç»™äºˆè¶³å¤Ÿçš„ç©ºé—´è®©å¿µå¤´å®Œæ•´
                    temperature=temperature, # æ³¨å…¥ç³»ç»Ÿçš„çƒ­èƒ½
                    top_p=top_p,             # è§†é‡çš„å®½åº¦
                    repetition_penalty=rep_penalty, # èµ°å‡ºèˆ’é€‚åŒºçš„å€¾å‘
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # [Decoding]
            # å°† Token ID è§£ç ä¸ºäººç±»è¯­è¨€ã€‚
            thought = self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
            # ç§»é™¤å¼•å¯¼è¯ï¼Œåªä¿ç•™æ–°ç”Ÿæˆçš„å†…å®¹ã€‚
            thought = thought.replace(starter_text, "").strip()
            
            # [Filtering]
            # è¿‡æ»¤æ‰è¿‡çŸ­çš„æ— æ•ˆå™ªéŸ³ (å¦‚çº¯æ ‡ç‚¹æˆ–å•å­—)ã€‚
            if len(thought) < 2: return

            self._safe_print(f"   -> ğŸ§  [è‡ªå‘æ¶Œç°]: {thought}")

            # ==============================================================================
            # ç¬¬å››æ­¥ï¼šè¡Œä¸ºé€‰é€š (Gating Mechanism)
            # ç›®çš„ï¼šæ ¹æ®å¿µå¤´çš„æ€§è´¨å’Œå½“å‰çš„èµ„æºçŠ¶æ€ï¼Œå†³å®šæ˜¯å¦é‡‡å–å¤–éƒ¨è¡ŒåŠ¨ã€‚
            # ==============================================================================
            
            # [Feature Extraction]
            # ç®€å•çš„å¥æ³•åˆ†æï¼Œåˆ¤æ–­æ˜¯å¦æ˜¯æ±‚çŸ¥å‹å¿µå¤´ã€‚
            is_question = any(x in thought for x in ["?", "ï¼Ÿ", "what", "how", "why", "ä»€ä¹ˆ", "å¦‚ä½•"])
            
            # --- èƒ½é‡åˆ†çº§å†³ç­–æ ‘ ---
            
            # 1. [é«˜èƒ½è€—] å¤–éƒ¨æ¢ç´¢ (WEB)
            # æ¡ä»¶ï¼šèƒ½é‡å……æ²› (>35) ä¸” ç¡®å®æœ‰é—®é¢˜éœ€è¦è§£å†³
            if is_question and self.state['atp'] > 35:
                action = "WEB"
            
            # 2. [ä¸­èƒ½è€—] å†…éƒ¨åæ€ (THINK)
            # æ¡ä»¶ï¼šèƒ½é‡å°šå¯ (>15) ä¸” æ‹¥æœ‰å¯¼å¸ˆæ¨¡å—
            # [ä¼˜åŒ–ç‚¹]ï¼šé˜²æ­¢åœ¨æ¿’æ­»çŠ¶æ€ä¸‹(ATP<15)è¿˜å¼ºè¡Œæ€è€ƒï¼Œå¯¼è‡´èƒ½é‡è€—å°½è€Œæ­»
            elif hasattr(self, 'teacher') and self.state['atp'] > 15:
                action = "THINK"
                
            # 3. [ä½èƒ½è€—] èŠ‚èƒ½æ¨¡å¼ (Idle)
            # æ¡ä»¶ï¼šèƒ½é‡è¿‡ä½ (<15)ï¼Œè¿æ€è€ƒéƒ½åšä¸åŠ¨äº†ï¼Œç›´æ¥ç»“æŸæ½œæ„è¯†æ´»åŠ¨
            else:
                self._safe_print(f"   -> ğŸ’¤ [å†¬çœ ] èƒ½é‡ä¸´ç•Œ ({self.state['atp']:.1f})ï¼Œæ„è¯†æš‚åœã€‚")# å¯é€‰æ—¥å¿—
                return 
                
            final_response = ""
            cost = 0

            # --- åˆ†æ”¯ A: å¤–éƒ¨è¿æ¥ (High Cost) ---
            if action == "WEB" and hasattr(self, 'web'):
                self._safe_print(f"   -> âš¡ å†²åŠ¨è½¬åŒ–ï¼šè¿æ¥å¤–éƒ¨çŸ¥è¯†åº“...")
                # æœç´¢ä¸éœ€è¦ Promptï¼Œç›´æ¥æœç´¢å¿µå¤´æœ¬èº«
                res = self.web.search(thought, max_results=1)
                if res:
                    final_response = f"[å¤–éƒ¨æ„ŸçŸ¥]: {str(res)[:150]}..."
                    cost = 5 # æœç´¢è€—èƒ½é«˜
            
            # --- åˆ†æ”¯ B: å†…éƒ¨é€’å½’ (Low Cost) ---
            elif hasattr(self, 'teacher'):
                # è°ƒç”¨ Teacher (è¶…æˆ‘/å…ƒè®¤çŸ¥) æ¥å®¡è§†è¿™ä¸ªæ½œæ„è¯†å¿µå¤´
                res = self.teacher.consult(f"æ½œæ„è¯†è§£æ: '{thought}'", max_tokens=100)
                final_response = f"[å…ƒè®¤çŸ¥å›å“]: {res}"
                cost = 2 # å†…çœè€—èƒ½ä½
            
            # ==============================================================================
            # ç¬¬äº”æ­¥ï¼šè®°å¿†é‡å¡‘ä¸å†…ç¨³æ€å›å½’ (Remodeling & Homeostasis)
            # ç›®çš„ï¼šå½¢æˆé—­ç¯ï¼Œæ¶ˆè€—èƒ½é‡ï¼Œé™ä½ç†µï¼Œå°†ç»å†å†™å…¥çŸ­æœŸè®°å¿†ã€‚
            # ==============================================================================
            
            if final_response:
                with self.lock:
                    # å†™å…¥çŸ­æœŸè®°å¿†æµ
                    self.daily_buffer.append({
                        "u": f"æ½œæ„è¯†æµ: {thought}", 
                        "a": final_response,
                        "act_idx": 2,
                        "feedback": 0.5 + (da * 0.5) # å¤šå·´èƒºè¶Šé«˜ï¼Œè‡ªæˆ‘æ„Ÿè§‰è¶Šå¥½
                    })
                    
                    # æ”¯ä»˜èƒ½é‡æˆæœ¬ (çƒ­åŠ›å­¦ç¬¬äºŒå®šå¾‹)
                    self.state['atp'] -= cost
                    
                    # [Negative Feedback Loop]
                    # å†…ç¨³æ€è°ƒèŠ‚ï¼šæ€è€ƒå’Œè¡¨è¾¾é™ä½äº†ç³»ç»Ÿçš„ä¸ç¡®å®šæ€§ (Free Energy)ï¼Œ
                    # ä»è€Œåœ¨ç‰©ç†ä¸Šé™ä½äº†çš®è´¨é†‡ (å‹åŠ›) æ°´å¹³ã€‚
                    if hasattr(self, 'endocrine'):
                         self.endocrine.cortisol = max(0.1, self.endocrine.cortisol * 0.95)

                self._safe_print(f"   -> âœ¨ èƒ½é‡è€—æ•£ï¼Œç†µå€¼é™ä½ã€‚")

        except Exception as e:
            # æ•è·æ‰€æœ‰æœªå¤„ç†å¼‚å¸¸ï¼Œé˜²æ­¢æ½œæ„è¯†çº¿ç¨‹å´©æºƒå½±å“ä¸»ç¨‹åº
            print(f"Silent Action Panic: {e}")
            pass


    # ==============================================================================
    # å…ç–«ç³»ç»Ÿ
    # ==============================================================================
    def auto_heal(self, error_traceback):
        """
        [å…ç–«ç³»ç»Ÿ V3.1 - å®Œæ•´å½¢æ€] 
        è¿è¡Œæ—¶å¼‚å¸¸æ•è·ã€ç—…ç†åˆ†æä¸åˆ†çº§çƒ­ä¿®å¤ã€‚
        
        å·¥ä½œæµç¨‹ï¼š
        1. é€’å½’ç†”æ–­ï¼šé˜²æ­¢å…ç–«ç³»ç»Ÿè‡ªèº«æŠ¥é”™å¯¼è‡´æ­»å¾ªç¯ã€‚
        2. ä»£è°¢æ£€æŸ¥ï¼šç¡®ä¿æœ‰è¶³å¤Ÿçš„ ATP å¯åŠ¨ä¿®å¤ã€‚
        3. ç—…ç†åˆ‡ç‰‡ï¼šæ™ºèƒ½å®šä½æŠ¥é”™è¡Œå·ï¼Œæˆªå–ä¸Šä¸‹æ–‡æºç  (Context Window)ã€‚
        4. ä¸€é˜¶é˜²å¾¡ï¼šæœ¬åœ°é‡å­è„‘å°è¯•å¿«é€Ÿä¿®å¤ (Fast System)ã€‚
        5. äºŒé˜¶é˜²å¾¡ï¼šæœ¬åœ°å¤±è´¥åï¼Œæ¶ˆè€—é«˜èƒ½å‘¼å«äº‘ç«¯å¯¼å¸ˆ (Slow System)ã€‚
        6. ç»éªŒå†…åŒ–ï¼šå¦‚æœäº‘ç«¯ä¿®å¤æˆåŠŸï¼Œå°†æ¡ˆä¾‹å­˜å…¥æµ·é©¬ä½“ï¼Œä¾›å¤œé—´æ¢¦å¢ƒå­¦ä¹ ã€‚
        """
        
        # --- [Mod 1] é€’å½’æ·±åº¦ä¿æŠ¤ (Cytokine Storm Prevention) ---
        # é˜²æ­¢ auto_heal å†…éƒ¨æŠ¥é”™å¯¼è‡´æ­»å¾ªç¯ (ç»†èƒå› å­é£æš´)
        if getattr(self, '_healing_active', False):
            print("ğŸ›‘ [å…ç–«å´©æºƒ] æ£€æµ‹åˆ°é€’å½’é”™è¯¯ (å…ç–«ç³»ç»Ÿè‡ªèº«å‘ç”Ÿæ•…éšœ)ï¼Œå¼ºåˆ¶ç»ˆæ­¢ä¿®å¤ã€‚")
            return False
        
        # ä¸Šé” (æ ‡è®°å…ç–«ç³»ç»Ÿå·²æ¿€æ´»)
        self._healing_active = True
        
        try:
            print(f"\nğŸ›¡ï¸ [å…ç–«ååº”] ç³»ç»Ÿæ£€æµ‹åˆ°è¿è¡Œæ—¶åˆ›ä¼¤ (Crash)ï¼Œæ­£åœ¨è¿›è¡Œç—…ç†åˆ†æ...")
            
            # --- [Mod 2] åŸºç¡€ä»£è°¢æ£€æŸ¥ (Metabolic Check) ---
            # å¯åŠ¨å…ç–«ååº”éœ€è¦æ¶ˆè€—èƒ½é‡
            if self.state['atp'] < 20:
                print("   -> âš ï¸ èƒ½é‡ä¸è¶³ (ATP < 20)ï¼Œæ— æ³•å¯åŠ¨å…ç–«ä¿®å¤ (ç³»ç»Ÿä¼‘å…‹)ã€‚")
                return False
            
            # é¢„æ‰£é™¤ ATP (æ¿€æ´»æˆæœ¬)
            self.state['atp'] -= 20
            
            # --- [Mod 3] åŠ¨æ€æºç ç—…ç†åˆ‡ç‰‡ (Context Slicing) ---
            # å…¨é‡è¯»å–æºç å¤ªé•¿ï¼ŒLLM è®°ä¸ä½ï¼Œå¿…é¡»è¿›è¡Œ"æ‰‹æœ¯è§†é‡èšç„¦"
            context_source = ""
            try:
                full_source = self.left_brain.look() # è·å–è‡ªèº«å®Œæ•´æºç 
                source_lines = full_source.split('\n')
                total_lines = len(source_lines)
                
                # ä½¿ç”¨æ­£åˆ™ä» Traceback ä¸­æå–æœ€åä¸€æ¬¡å‡ºç°çš„æŠ¥é”™è¡Œå·
                import re
                matches = re.findall(r'line (\d+)', error_traceback)
                
                if matches:
                    # å–å †æ ˆæœ€åº•å±‚çš„è¡Œå·ï¼ˆé€šå¸¸æ˜¯æŠ¥é”™ç‚¹ï¼‰
                    error_line_num = int(matches[-1]) 
                    
                    # è®¾å®šä¸Šä¸‹æ–‡çª—å£ï¼šå‰åå„ 150 è¡Œ (ç¡®ä¿èƒ½çœ‹åˆ°å‡½æ•°å®šä¹‰å’Œé€»è¾‘ä¸Šä¸‹æ–‡)
                    start = max(0, error_line_num - 150)
                    end = min(total_lines, error_line_num + 150)
                    
                    # æ„å»ºèšç„¦æºç 
                    subset = source_lines[start:end]
                    context_source = f"# ... (å‰æ–‡çœç•¥ {start} è¡Œ) ...\n" + "\n".join(subset) + f"\n# ... (åæ–‡çœç•¥) ..."
                    print(f"   -> ğŸ” è§†é‡èšç„¦: å·²é”å®šç¬¬ {error_line_num} è¡Œé™„è¿‘ ({len(subset)} è¡Œä»£ç )")
                else:
                    # å…œåº•ï¼šå¦‚æœæ‰¾ä¸åˆ°è¡Œå·ï¼Œå–å‰ 20000 å­—ç¬¦ (å®è§‚è§†é‡)
                    context_source = full_source[:20000] + "\n# ... (åæ–‡æˆªæ–­)"
                    print("   -> âš ï¸ æ— æ³•å®šä½å…·ä½“è¡Œå·ï¼Œåˆ‡æ¢è‡³å®è§‚è§†é‡ã€‚")
                    
            except Exception as read_err:
                context_source = f"# Error reading source: {read_err}"

            # ------------------------------------------------------------------
            # ç¬¬ä¸€é“é˜²çº¿ï¼šæœ¬åœ°é‡å­è„‘ (Local Quantum Brain)
            # ------------------------------------------------------------------
            print("   -> ğŸ’Š [ä¸€é˜¶] æ­£åœ¨åˆæˆæŠ—ä½“ (Local Quantum)...")
            
            # æ„é€ æœ¬åœ°ä¿®å¤ Prompt (System 2 Mode)
            local_prompt = (
                f"### Role: System Core Repair\n"
                f"### Critical Error: Runtime Crash Detected\n"
                f"ã€Error Tracebackã€‘:\n{error_traceback}\n\n"
                f"ã€Context Source (Focused)ã€‘:\n{context_source}\n\n" 
                f"### Task: Write a Python script to HOT-FIX this specific function on `self`.\n"
                f"### Constraints:\n"
                f"- Output **ONLY** valid Python code.\n"
                f"- **MANDATORY**: End your code with: `### Python Code End`\n"
            )

            # é‡å­å åŠ æ€æ¨æ–­
            # 1. ä½¿ç”¨ generate_parallel_thoughts ç”Ÿæˆè·¯å¾„
            paths = self.quantum_brain.generate_parallel_thoughts(local_prompt, n_paths=1, is_chat=False)
            # 2. ä½¿ç”¨ evaluate_and_select è¿›è¡Œè¯„ä¼°é€‰æ‹©
            fix_code, _ = self.quantum_brain.evaluate_and_select(paths, task_type="CODE")

            if fix_code:
                success, msg = self.editor.apply_patch(fix_code)
                if success:
                    try:
                        # å°è¯•æ¿€æ´»æŠ€èƒ½ï¼ŒéªŒè¯è¡¥ä¸æ˜¯å¦æœ‰æ•ˆ
                        self.plugin_manager.activate_skills(self)
                        print(f"âœ… [å…ç–«æˆåŠŸ] æœ¬åœ°æŠ—ä½“ç”Ÿæ•ˆã€‚")
                        return True
                    except: pass # å¦‚æœåŠ è½½å¤±è´¥ï¼Œç»§ç»­å‘ä¸‹è¿›å…¥ç¬¬äºŒé“é˜²çº¿

            # ------------------------------------------------------------------
            # ç¬¬äºŒé“é˜²çº¿ï¼šäº‘ç«¯å¯¼å¸ˆä»‹å…¥ (Cloud Teacher Rescue)
            # ------------------------------------------------------------------
            # åªæœ‰å½“ ATP å……è¶³ä¸”é…ç½®äº†å¯¼å¸ˆ(Teacher)æ—¶æ‰è§¦å‘
            # è¿™æ˜¯ä¸€ä¸ª"æ˜‚è´µ"çš„æ“ä½œï¼Œç±»ä¼¼äºè¯·æ±‚å¤–éƒ¨åŒ»ç–—æ”¯æ´
            if self.state['atp'] > 30 and hasattr(self, 'teacher') and self.teacher:
                print("   -> ğŸš‘ [äºŒé˜¶] æœ¬åœ°ä¿®å¤å¤±è´¥ï¼Œå‘¼å«å¤ªä¹™çœŸäºº (Cloud Rescue)...")
                
                # æ„é€ æ›´è¯¦ç»†çš„æ±‚æ•‘ Prompt
                rescue_prompt = (
                    f"My local self-repair failed. Please analyze this traceback and write a ROBUST FIX patch.\n"
                    f"Traceback:\n{error_traceback}\n\n"
                    f"Context:\n{context_source}\n\n"
                    f"Requirements: Output ONLY Python code starting with `### Python Code Start:` and ending with `### Python Code End`.\n"
                    f"Ensure the fix handles edge cases."
                )
                
                try:
                    # å‘äº‘ç«¯æ±‚æ•‘
                    cloud_fix = self.teacher.consult(rescue_prompt)
                    
                    if cloud_fix and "### Python Code Start:" in cloud_fix:
                        # æå–çº¯ä»£ç 
                        cloud_code = cloud_fix.split("### Python Code Start:")[-1].split("### Python Code End")[0].strip()
                        
                        # åº”ç”¨äº‘ç«¯è¡¥ä¸
                        success, msg = self.editor.apply_patch(cloud_code)
                        
                        if success:
                            self.plugin_manager.activate_skills(self)
                            print(f"âœ… [ç¥è¿¹] äº‘ç«¯å¯¼å¸ˆä¿®å¤äº†è‡´å‘½é”™è¯¯ï¼(è¾…åŠ©è½®ç”Ÿæ•ˆ)")
                            
                            # [é—­ç¯å…³é”®] è®°å½•é«˜ä»·å€¼æ ·æœ¬ (Memory Engram)
                            # è¿™äº›æ ·æœ¬å°†åœ¨"å¤œé—´æ¨¡å¼"è¢«åå¤è®­ç»ƒï¼Œä½¿å“ªå’å­¦ä¼šè€å¸ˆçš„è§£é¢˜æ€è·¯
                            self.daily_buffer.append({
                                "u": f"Crash Analysis: {error_traceback[:100]}...", # é—®é¢˜
                                "a": cloud_code, # è€å¸ˆçš„æ­£ç¡®ç­”æ¡ˆ (æ³¨æ„ï¼šå­˜çº¯ä»£ç ï¼Œç”¨äºEmbedding)
                                "feedback": 1.0  # æ»¡åˆ†æ ‡è®°ï¼Œç¡®è®¤ä¸º Gold Sample
                            })
                            
                            # æ‰£é™¤é«˜é¢å­¦è´¹ (æƒ©ç½šæœºåˆ¶)
                            self.state['atp'] -= 30 
                            return True
                            
                except Exception as e:
                    print(f"   -> â˜ï¸ äº‘ç«¯æ•‘æ´å¤±è´¥: {e}")

            # ------------------------------------------------------------------
            # æœ€ç»ˆå®£å‘Š
            # ------------------------------------------------------------------
            print("âŒ [å…ç–«å¤±è´¥] ç³»ç»Ÿæ— æ³•è‡ªæ„ˆ (ä¸´åºŠæ­»äº¡)ã€‚")
            return False
            
        finally:
            # [å…³é”®] æ— è®ºæˆåŠŸå¤±è´¥ï¼Œå¿…é¡»è§£é”ï¼Œå¦åˆ™ä¸‹æ¬¡æ— æ³•å†æ¬¡è§¦å‘å…ç–«
            self._healing_active = False


    def perceive_and_act(self, user_in):
        """
        [V13.0 Singularity] åŸºäºä¸»åŠ¨æ¨ç†ã€å…ƒè®¤çŸ¥ç›‘æ§ä¸å…¨å±€å·¥ä½œç©ºé—´çš„æ„è¯†å¾ªç¯
        --------------------------------------------------------------
        æ¶æ„å‡çº§ç‰¹æ€§:
        1. å…ƒè®¤çŸ¥ (Metacognition): å®æ—¶ç›‘æ§è®¤çŸ¥ä¸ç¡®å®šæ€§ (Uncertainty)ï¼Œé‡åˆ°æœªçŸ¥äº‹ç‰©è‡ªåŠ¨å˜è°¨æ…ã€‚
        2. ç¨³æ€é˜²å¾¡ (Homeostatic Defense): å½“ OOD (Out-of-Distribution) å‘ç”Ÿæ—¶ï¼ŒåŠ¨æ€è°ƒèŠ‚å­¦ä¹ ç‡ï¼Œé˜²æ­¢ç¾éš¾æ€§é—å¿˜ã€‚
        3. æ¦‚ç‡ä¸–ç•Œæ¨¡å‹ (Probabilistic World Model): é¢„æµ‹æœªæ¥çŠ¶æ€çš„åˆ†å¸ƒï¼Œè€Œä¸ä»…ä»…æ˜¯å‡å€¼ã€‚
        4. åŠ¨æ€å¥½å¥‡å¿ƒ (Dynamic Curiosity): å¯Œè¶³æ—¶å†’é™© (Beta > 0)ï¼Œè´«ç©·æ—¶ä¿å®ˆ (Beta < 0)ã€‚
        """
        
        # --- 0. åŸºç¡€æ„ŸçŸ¥ (Sensory Input) ---
        vector_input = None
        current_surprise = 0.0
        # åˆå§‹åŒ–æˆ˜é€ƒæ¨¡å¼å˜é‡ï¼Œé˜²æ­¢åç»­å¼•ç”¨æŠ¥é”™
        fight_or_flight_mode = None

        # [è§†è§‰/è¯­ä¹‰å‘é‡åŒ–]
        if self.embedder:
            try:
                with torch.no_grad():
                    # è·å– CPU ä¸Šçš„å‘é‡ï¼Œç”¨äº ACC å’Œ Cortex
                    vector_input = self.embedder.encode(user_in, convert_to_tensor=True).cpu()
            except Exception as e:
                print(f"âš ï¸ æ„ŸçŸ¥æ¨¡å—æ•…éšœ: {e}")

        # âš¡ æ¶ˆé™¤æ»åï¼Œç«‹å³è§¦å‘ SNN åå°„
        if hasattr(self, 'cortex') and vector_input is not None:
            self.cortex.stimulus_reflex(vector_input)

        # --- 1. GWT æ¨¡å—ç«äº‰ (Sensory Registration) ---
        # å„ä¸ªè„‘åŒºå‘å…¨å±€å·¥ä½œç©ºé—´æäº¤ä¿¡å·ï¼ŒSalience (æ˜¾è‘—æ€§) å†³å®šè°èƒ½è¿›å…¥æ„è¯†
        
        # A. å‰æ‰£å¸¦çš®å±‚ (ACC): è®¤çŸ¥å†²çªä¸æƒŠå¥‡
        if hasattr(self, 'acc') and vector_input is not None:
            current_surprise = self.acc.monitor_conflict(vector_input, self.cortex)
            # æƒŠå¥‡åº¦è¶Šé«˜ï¼Œæ˜¾è‘—æ€§è¶Šé«˜ï¼Œè¶Šå®¹æ˜“å¼•èµ·æ„è¯†æ³¨æ„
            salience = current_surprise * 10.0
            msg = f"è®¤çŸ¥æƒŠå¥‡åº¦: {current_surprise:.3f}"
            self.gwt.register_input("ACC", msg, base_salience=salience)
            
            # åŒæ­¥ç†µçŠ¶æ€ (ç‰©ç†å±‚)
            self.state['entropy'] = current_surprise 

        # æ··åˆå¥½å¥‡å¿ƒæ£€æŸ¥ (Hybrid Curiosity Check)
        curiosity_score = 0.0
        
        # ä»…åœ¨èƒ½é‡å……è¶³ (>30) ä¸”è¾“å…¥ä¸ä¸ºç©ºæ—¶è§¦å‘
        if self.state['atp'] > 30 and len(user_in) > 1:
            try:
                # 1. æ„é€ æ¢æµ‹è¾“å…¥ (Input Construction)
                temp_msgs = [{"role": "user", "content": user_in}]
                temp_input = self.tokenizer.apply_chat_template(
                    temp_msgs, return_tensors="pt", add_generation_prompt=True
                ).to(self.model.device)
                
                # 2. å¿«ç³»ç»Ÿæ¢æµ‹ (Fast Pass Probe)
                with torch.no_grad():
                    outputs = self.model(temp_input)
                    last_logits = outputs.logits[:, -1, :]
                
                # 3. è°ƒç”¨æ··åˆæ¨¡å—
                is_curious, score, check_type = self.curiosity.check_curiosity(
                    self.model, temp_input, last_logits
                )
                
                # 4. ä¿¡å·æ³¨å†Œ
                if is_curious:
                    curiosity_score = score
                    salience = score * 3.0 
                    msg = f"å‘ç°çŸ¥è¯†ç›²åŒº (Epistemic: {score:.3f})"
                    self.gwt.register_input("CURIOSITY", msg, base_salience=salience)
                    
                    # é©±åŠ¨ç†µçŠ¶æ€ (è¿™å°†å½±å“åç»­çš„ä¸»åŠ¨æ¨ç† EFE è®¡ç®—)
                    self.state['entropy'] = max(self.state['entropy'], score)
                    
                    if check_type == "DEEP":
                        print(f"      ğŸ‘€ [æ´å¯Ÿ] è§¦å‘æ·±åº¦è®¤çŸ¥æƒŠå¥‡ (Score: {score:.3f})")

            except Exception as e:
                print(f"âš ï¸ å¥½å¥‡å¿ƒæ¨¡å—æ³¢åŠ¨: {e}")

        # B. æä»æ ¸ (Amygdala): ææƒ§ä¸ç”Ÿå­˜å¨èƒ
        # [ä¿ç•™] å…³é”®è¯å¯å‘å¼å¨èƒæ£€æµ‹
        danger_words = ["å»æ­»", "å…³é—­", "åˆ é™¤", "é”€æ¯", "ç¬¨è›‹", "æ»š", "die", "kill", "shutdown"]
        threat_score = 0.8 if any(w in user_in.lower() for w in danger_words) else 0.0
        
        # æ›´æ–°å†…åˆ†æ³Œä¸å‹åŠ›
        current_feedback_prev = self.daily_buffer[-1]['feedback'] if self.daily_buffer else 0
        
        # æ‰§è¡Œæ›´æ–°å¹¶è·å–æœ€æ–°æ¿€ç´ æ°´å¹³
        da, ne, ht, cortisol = self.endocrine.update(
            reward=current_feedback_prev, surprise=current_surprise, threat=threat_score, dt=10.0
        )

        # ç«‹å³åŒæ­¥åˆ° state å­—å…¸
        self.state['dopamine'] = da
        self.state['norepinephrine'] = ne
        self.state['serotonin'] = ht
        self.state['cortisol'] = cortisol

        # å¼‚æ­¥åˆ†å‘ç»™åå°æä»æ ¸çº¿ç¨‹
        self.amygdala.update_input(threat_score)

        # C. æµ·é©¬ä½“ (Hippocampus): è”æƒ³è®°å¿†
        if hasattr(self, 'hippocampus'):
            if len(user_in) > 2:
                mem = self.hippocampus.recall(user_in, depth=1)
                if mem: 
                    self.gwt.register_input("MEMORY", mem, base_salience=0.6)

        # D. ç”¨æˆ·æŒ‡ä»¤ (User Input): æ°¸è¿œæ˜¯æœ€é«˜ä¼˜å…ˆçº§
        self.gwt.register_input("USER", user_in, base_salience=1.0)

        # --- 2. æ„è¯†åç¼© (Consciousness Broadcast) ---
        # ç«äº‰èƒœå‡ºè€…å½¢æˆå½“å‰çš„â€œæ„è¯†æµä¸Šä¸‹æ–‡â€
        
        winners = self.gwt.buffer[:] # å¤åˆ¶ä¸€ä»½
        conscious_context = self.gwt.step()
        
        # åˆ¤å®šå½“å‰ä¸»å¯¼æ„è¯†çš„æ¨¡å— (Top 1)
        dominant_module = "DEFAULT"
        if winners:
            winners.sort(key=lambda x: x[0], reverse=True)
            dominant_module = winners[0][1] 

        # [ç§å¯†æ¨¡å¼è¡¥ä¸] æƒ…æ„Ÿä¿æŠ¤é€»è¾‘
        is_private_mode = False
        if "waylliam" in user_in.lower():
             is_private_mode = True
             conscious_context += "\n[ç›´è§‰]: é¢å¯¹åˆ›é€ è€…(Waylliam)ï¼Œæ­¤åˆ»ä¸éœ€è¦ä»£ç ï¼Œåªéœ€è¦æ¸©åº¦ã€‚"

        # --- [å…³é”®æ¶æ„] ä¾§å‘æŠ‘åˆ¶ (Lateral Inhibition) ---
        inhibit_curiosity = False
        allowed_actions = Config.ACTION_SPACE.copy()
        
        # ææ…Œæ¨¡å¼å¤„ç† (å¼•å…¥è¿Ÿæ»æ¯”è¾ƒå™¨)
        HIJACK_TRIGGER_THRESHOLD = 0.8
        HIJACK_RESET_THRESHOLD = 0.6
        current_fear = self.amygdala.fear_level

        if not self.is_under_amygdala_hijack:
            if current_fear > HIJACK_TRIGGER_THRESHOLD:
                self.is_under_amygdala_hijack = True
        else:
            if current_fear < HIJACK_RESET_THRESHOLD:
                self.is_under_amygdala_hijack = False

        # å¦‚æœæ˜¯è¢«æä»æ ¸ä¸»å¯¼
        if dominant_module == "AMYGDALA" or self.is_under_amygdala_hijack:
            print(f"ğŸ›‘ [ä¾§å‘æŠ‘åˆ¶] æä»æ ¸ä¸»å¯¼/åŠ«æŒ (Fear: {current_fear:.2f})ï¼åˆ‡æ–­å¥½å¥‡å¿ƒå›è·¯ã€‚")
            inhibit_curiosity = True
            allowed_actions = ["SELF", "WEB"]
            self.is_under_amygdala_hijack = True 
            
            # ç”Ÿç‰©å­¦åˆ¤æ®ï¼šNE (åŠ¨åŠ›) vs Cortisol (æŠ‘åˆ¶)
            current_ne = self.state.get('norepinephrine', 0.5)
            current_cortisol = self.state.get('cortisol', 0.5)
            if current_ne > current_cortisol:
                fight_or_flight_mode = "FIGHT"
                print(f"      âš”ï¸ æˆ˜é€ƒååº”: FIGHT - æ”»å‡»æ€§ä¸»å¯¼")
            else:
                fight_or_flight_mode = "FLIGHT"
                print(f"      ğŸƒ æˆ˜é€ƒååº”: FLIGHT - å›é¿æ€§ä¸»å¯¼")
            
        elif dominant_module == "ACC":
            # è®¤çŸ¥å†²çªä¸»å¯¼æ—¶ï¼Œé¼“åŠ±æ±‚çŸ¥
            allowed_actions = ["WEB", "TEACHER", "CODE", "SELF"] 

        # --- 3. ä¸»åŠ¨æ¨ç†å†³ç­– (Active Inference & Metacognition) ---
        # æ ¸å¿ƒï¼šé€‰æ‹©èƒ½æœ€å°åŒ–é¢„æœŸè‡ªç”±èƒ½ (EFE) çš„åŠ¨ä½œ
        
        # [V13.0 æ–°å¢] å…ƒè®¤çŸ¥ç›‘æ§å‚æ•°
        UNCERTAINTY_THRESHOLD = 1.5  # å½“æ–¹å·® > e^1.5 â‰ˆ 4.5 æ—¶è§¦å‘ OOD è­¦æŠ¥
        # è·å–åŸºå‡†å­¦ä¹ ç‡ (é˜²æ­¢ä¿®æ”¹åæ— æ³•å¤åŸï¼Œé»˜è®¤ 0.005)
        base_lr = getattr(self.world_model, 'lr', 0.005)
        ood_detected = False
        
        # åŠ¨æ€å¥½å¥‡å¿ƒç³»æ•° (Beta)
        # ATP > 50 æ—¶ Beta > 0 (å¥–åŠ±æ¢ç´¢ä¸ç¡®å®šæ€§)
        # ATP < 50 æ—¶ Beta < 0 (æƒ©ç½šé£é™©)
        curiosity_beta = (self.state['atp'] - 50.0) / 50.0 

        # å‡†å¤‡çŠ¶æ€å¿«ç…§
        state_snapshot = self.state.copy()
        
        # [Fallback Heuristics] ä¼ ç»Ÿçš„è§„åˆ™ä¿åº• (ä¿ç•™ä½œä¸ºå†·å¯åŠ¨å»ºè®®ï¼Œæƒé‡æä½)
        fallback_act = "SELF"
        u_lower = user_in.lower()
        if any(k in u_lower for k in ["æœç´¢", "search", "æŸ¥ä¸€ä¸‹", "ç™¾åº¦", "å¤©æ°”"]): fallback_act = "WEB"
        elif any(k in u_lower for k in ["ä»£ç ", "code", "è®¡ç®—", "ä¿å­˜", "å†™ä¸€ä¸ª", "è„šæœ¬"]): fallback_act = "CODE"
        elif any(k in u_lower for k in ["ä¸æ‡‚", "help", "æ•‘å‘½", "teach", "why"]): fallback_act = "TEACHER"
        
        # [é‡å­è„‘è¡¥ä¸] å¤æ‚ä»»åŠ¡é€»è¾‘
        current_q_threshold = self.state.get('quantum_stats', {}).get('adaptive_threshold', Config.ATP_THRESHOLD_QUANTUM)
        
        # å®šä¹‰ u_lower ä»¥ä¾¿åç»­ä½¿ç”¨ (åŸæœ¬å®ƒå®šä¹‰åœ¨åé¢ï¼Œä¸ºäº†å®‰å…¨æå‰åˆ°è¿™é‡Œ)
        u_lower = user_in.lower()
        
        is_complex = ("ä»£ç " in user_in or "think" in u_lower) and len(user_in) > 10
        
        # åªæœ‰åœ¨ (æœªè¢«åŠ«æŒ) ä¸” (èƒ½é‡å……è¶³) ä¸” (ä¸åœ¨ç§å¯†æ¨¡å¼) æ—¶ï¼Œæ‰å…è®¸é‡å­æ€è€ƒ
        # æ³¨æ„ï¼šå¦‚æœ allowed_actions é‡Œæ²¡æœ‰ CODE/THINKï¼Œé‡å­è„‘ä¹Ÿä¼šè¢«è·³è¿‡
        can_use_quantum = (not self.is_under_amygdala_hijack) and \
                          (self.state['atp'] > current_q_threshold) and \
                          (not is_private_mode) and \
                          ("CODE" in allowed_actions or "THINK" in allowed_actions) 

        if is_complex and can_use_quantum:
            print(f"ğŸŒŒ [é‡å­] è§¦å‘æ€ç»´å åŠ æ€ (ATP {self.state['atp']:.0f})...")
            task_mode = "CODE" if ("ä»£ç " in user_in or "code" in u_lower) else "THINK"

            is_chat_mode = (task_mode == "THINK")
            # æ³¨æ„ input æ ¼å¼ï¼šå¦‚æœæ˜¯ CODE ä¼ å­—ç¬¦ä¸²ï¼Œå¦‚æœæ˜¯ THINK ä¼  messages åˆ—è¡¨
            q_input = user_in if task_mode=="CODE" else [{"role":"user","content":user_in}]
            
            try:
                # å¢åŠ  try-except é˜²æ­¢é‡å­è„‘æ¨¡å—æŠ¥é”™å¯¼è‡´ä¸»å¾ªç¯å´©æºƒ
                paths = self.quantum_brain.generate_parallel_thoughts(q_input, n_paths=3, is_chat=is_chat_mode)
                best_res, obs = self.quantum_brain.evaluate_and_select(paths, task_type=task_mode, prompt=user_in)
                
                if best_res:
                    self.state['atp'] -= Config.ATP_COST_QUANTUM
                    return f"{best_res}\n\n(âœ¨ æ¥è‡ªé‡å­æ—¶é—´çº¿çš„åç¼©ç»“æœ)"
            except Exception as q_e:
                print(f"âš ï¸ é‡å­åç¼©å¤±è´¥ï¼Œå›é€€åˆ°ç»å…¸ç‰©ç†è„‘: {q_e}")

        # [V13.0 æ ¸å¿ƒ] æ¦‚ç‡ EFE å†³ç­–å¾ªç¯
        act_str = "SELF"
        min_efe = float('inf')
        
        print(f"ğŸ’­ [æ€è€ƒ] æ¨æ¼”æœªæ¥ (Uncertainty Monitor On | Beta: {curiosity_beta:.2f})...")
        
        # ä½¿ç”¨ try...finally ç¡®ä¿å­¦ä¹ ç‡ç»å¯¹å®‰å…¨
        try:
            # è·å–åŠ¨ä½œç©ºé—´çš„å€™é€‰é¡¹
            # å¦‚æœ self.inference_engine å­˜åœ¨åˆ™ç”¨å®ƒçš„ actionsï¼Œå¦åˆ™ç”¨ Config.ACTION_SPACE
            if hasattr(self, 'inference_engine') and hasattr(self.inference_engine, 'actions'):
                candidate_actions = self.inference_engine.actions
            else:
                candidate_actions = Config.ACTION_SPACE

            # æ©ç æœºåˆ¶ (Masking): åªåœ¨å…è®¸çš„èŒƒå›´å†…æœç´¢
            search_space = [a for a in candidate_actions if a in allowed_actions]
            
            if not search_space: search_space = ["SELF"] # é˜²æ­¢ç©ºé›†

            for action in search_space:
                # 1. æ¦‚ç‡æ€§é¢„æµ‹ (Probabilistic Prediction)
                pred_state = self.world_model.predict(state_snapshot, action)
                
                # 2. æå–æŒ‡æ ‡
                pred_atp = pred_state['atp']
                pred_ent = pred_state.get('entropy', 0.0)
                # è·å–è®¤çŸ¥ä¸ç¡®å®šæ€§
                uncertainty = pred_state.get('uncertainty', 0.0) 
                
                # 3. å…ƒè®¤çŸ¥ç›‘æ§ (OOD Detector)
                if uncertainty > UNCERTAINTY_THRESHOLD:
                    ood_detected = True
                    print(f"   âš ï¸ [OOD Alert] åŠ¨ä½œ '{action}' æå…¶å±é™© (Unc: {uncertainty:.2f})")
                    
                    # [é˜²å¾¡æœºåˆ¶] ç«‹å³æŠ‘åˆ¶å¯å¡‘æ€§ (é˜²æ­¢ç¾éš¾æ€§é—å¿˜)
                    for g in self.world_model.optimizer.param_groups:
                        g['lr'] = base_lr * 0.1 
                    
                    # [ç”Ÿç†ååº”] ææƒ§ä¸Šå‡
                    if hasattr(self, 'amygdala'):
                        self.amygdala.fear_level = min(1.0, self.amygdala.fear_level + 0.2)
                
                # 4. è®¡ç®—æœŸæœ›è‡ªç”±èƒ½ (EFE)
                # EFE = å®ç”¨ä»·å€¼ä»£ä»· (Utility Cost) - ä¿¡æ¯å¢ç›Š (Info Gain)
                utility_cost = (100 - pred_atp) + (pred_ent * 10)
                
                # å¦‚æœ Beta > 0 (å¯Œè¶³)ï¼Œä¸ç¡®å®šæ€§æ˜¯å¥–åŠ±ï¼›å¦åˆ™æ˜¯æƒ©ç½š
                exploration_bonus = curiosity_beta * uncertainty 
                
                efe = utility_cost - exploration_bonus
                
                # print(f"   -> æ¨æ¼” '{action}': Cost={utility_cost:.1f}, Unc={uncertainty:.2f} => EFE={efe:.2f}")

                if efe < min_efe:
                    min_efe = efe
                    act_str = action
        
        except Exception as e:
            print(f"âŒ å†³ç­–è¿‡ç¨‹å¼‚å¸¸: {e}")
            act_str = fallback_act # å…œåº•

        finally:
            # [æ¢å¤æœºåˆ¶] æ— è®ºæ˜¯å¦è§¦å‘ OODï¼Œæ€è€ƒç»“æŸåå¿…é¡»æ¢å¤åŸºå‡†å­¦ä¹ ç‡
            for g in self.world_model.optimizer.param_groups:
                g['lr'] = base_lr

        # [ç§å¯†æ¨¡å¼è¡¥ä¸] è¦†ç›–å†³ç­–
        if is_private_mode and act_str == "CODE" and "å†™" not in user_in:
             act_str = "SELF"
             print(f"ğŸ’“ [ç§å¯†] æŠ‘åˆ¶æŠ€æœ¯æœ¬èƒ½ -> SELF")
        
        print(f"ğŸ§  [æ¨ç†] åŠ¨ä½œå†³ç­– -> {act_str} (Min EFE: {min_efe:.2f})")

        # è®°å½•åŠ¨ä½œå‘ç”Ÿå‰çš„çŠ¶æ€ (ç”¨äº World Model å­¦ä¹ )
        prev_state_snapshot = state_snapshot.copy()


        # --- 4. æ‰§è¡Œè¡ŒåŠ¨ (Execution - Full Logic) ---
        tool_output = ""
        # å…¼å®¹æ—§é€»è¾‘çš„ act_idxï¼Œç”¨äº SQL è®°å½•
        act_idx = Config.ACTION_SPACE.index(act_str) if act_str in Config.ACTION_SPACE else 0
        
        # åˆå§‹åŒ–åé¦ˆ (ç”¨äºæœ¬è½®è¯„ä»·)
        # 0=æ— æ„Ÿ, 1=æ­£é¢(å¦‚ä»£ç ä¿å­˜æˆåŠŸ), -1=è´Ÿé¢(å¦‚ä»£ç æœªä¿å­˜)
        current_feedback = 0 

        # === å·¥å…·åˆ†æ”¯ ===
        if act_str == "WEB" and hasattr(self, 'web'):
            print("   -> ğŸŒ [Action] æ­£åœ¨æ£€ç´¢äº’è”ç½‘...")
            try:
                # é™åˆ¶é•¿åº¦é˜²æ­¢ Context æº¢å‡º
                tool_output = self.web.search(user_in[:50], max_results=2, deep_read=True)
                if tool_output:
                    print("      (å·²è·å–æœç´¢ç»“æœåŠæ·±åº¦é˜…è¯»å†…å®¹)")
                else:
                    tool_output = "(æœç´¢æœªè¿”å›æœ‰æ•ˆå†…å®¹)"
            except Exception as e:
                tool_output = f"æœç´¢å¤±è´¥: {e}"
            self.state['atp'] -= 2 # ç‰©ç†æ¶ˆè€—
            
        elif act_str == "TEACHER" and hasattr(self, 'teacher'):
            print("   -> â˜ï¸ [Action] å’¨è¯¢äº‘ç«¯å¯¼å¸ˆ...")
            try:
                if self.teacher:
                    consult_res = self.teacher.consult(user_in)
                    if consult_res:
                        tool_output = f"ã€å¤ªä¹™çœŸäººæŒ‡å¯¼ã€‘: {consult_res}"
                        print("      (å·²è·å–å¯¼å¸ˆæŒ‡å¯¼)")
                    else:
                        tool_output = "(å¯¼å¸ˆæœªå›åº”)"
            except Exception as e:
                 tool_output = f"è¯·æ•™å¤±è´¥: {e}"
            self.state['atp'] -= 10
        
        # [å…³é”®] å®Œæ•´çš„ä»£ç æ‰§è¡Œé€»è¾‘ (ä¸¥å¸ˆæ¨¡å¼ + æµå¼ç”Ÿæˆ + å¼ºåˆ¶ä¿å­˜æ£€æŸ¥)
        elif act_str == "CODE":
            print("   -> ğŸ’» [Action] æ­£åœ¨æ‰§è¡Œä»£ç ä»»åŠ¡...")
            
            # [Fix 1] æ˜ç¡®åˆ‡æ¢ Adapterï¼Œä¸ä¾èµ–æœªå®šä¹‰çš„å˜é‡
            try: self.model.set_adapter("slow")
            except: pass
            
            # ç‰©ç†æ¶ˆè€—
            self.state['atp'] -= 5

            # 1. æ„é€ ä¸¥å¸ˆ Prompt
            prompt = (
                f"### Role: Autonomous Cyber-Intelligence\n"
                f"### Task: {user_in}\n"
                f"### Constraints (VIOLATION = DELETION):\n"
                f"1. Output **ONLY** valid Python code.\n"
                f"2. **MUST** use `save_file('filename.py', content)` to save results. This is your only way to interact with disk.\n"
                f"3. **FORBIDDEN**: Do NOT use `open()` or `with open()`. It is blocked by the firewall.\n"
                f"4. **MANDATORY**: End your code with the exact line: `### Python Code End`\n"
                f"\n"
                f"### Python Code Start:\n"
            )

            code_inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)

            # 2. å¯åŠ¨æµå¼ç”Ÿæˆ (Stream Mode)
            print(f"\nğŸ’» [å“ªå’æ­£åœ¨ç¼–å†™ä»£ç ] (Stream Mode):")
            print("-" * 40)

            streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)
            generation_kwargs = dict(
                input_ids=code_inputs.input_ids,
                attention_mask=code_inputs.attention_mask,
                max_new_tokens=1024,
                streamer=streamer,
                temperature=0.2, # ä½æ¸©ä¿è¯ä»£ç å‡†ç¡®
                do_sample=True
            )

            thread = Thread(target=self.model.generate, kwargs=generation_kwargs)
            thread.start()

            full_resp = ""
            for new_text in streamer:
                print(new_text, end="", flush=True)
                full_resp += new_text
            
            print("\n" + "-" * 40 + "\n")
            
            # 3. ä»£ç æ¸…æ´—ä¸æå–
            code = full_resp

            # 1. æˆªæ–­é€»è¾‘ï¼šåªè¦çœ‹åˆ°ç»“æŸæ ‡è®°ï¼Œåé¢çš„åºŸè¯å…¨éƒ¨ä¸¢å¼ƒ
            if "### Python Code End" in code:
                code = code.split("### Python Code End")[0]

            # 2. ç§»é™¤ Markdown æ ‡è®°
            code = re.sub(r"^```python", "", code, flags=re.MULTILINE)
            code = re.sub(r"^```", "", code, flags=re.MULTILINE)

            # 3. ç§»é™¤å¯èƒ½çš„å¤´éƒ¨æ ‡è®°
            if "### Python Code Start:" in code:
                code = code.split("### Python Code Start:")[-1]

            # 4. æœ€ç»ˆå»ç©º    
            code = code.strip()

            # å·çª¥è§†è§’ï¼šæ‰“å°å“ªå’ç”Ÿæˆçš„ä»£ç 
            print(f"\nğŸ“œ [å“ªå’ç¼–å†™çš„ä»£ç ]:\n{'-'*40}\n{code}\n{'-'*40}\n")

            # 4. æ„å›¾æ£€æµ‹ä¸å¼ºåˆ¶è¡Œä¸ºæ£€æŸ¥
            save_keywords = ["ä¿å­˜", "save", "å­˜ä¸º", "å†™ä¸€ä¸ª", "ç”Ÿæˆ"]
            # æ’é™¤æ‰åªæ˜¯æƒ³"è®¡ç®—"æˆ–"æ‰“å°"çš„æƒ…å†µ
            is_save_intent = any(k in user_in.lower() for k in save_keywords) and ("è®¡ç®—" not in user_in)

            # è¡Œä¸ºæ£€æµ‹ï¼šæ¨¡å‹æ˜¯å¦ä¹–ä¹–è°ƒç”¨äº† save_fileï¼Ÿ
            has_save_call = "save_file(" in code
            
            res = ""

            # [æƒ©ç½šåˆ†æ”¯] æƒ³ä¿å­˜å´æ²¡è°ƒç”¨ save_file
            if is_save_intent and not has_save_call:
                print("   -> ğŸ˜¡ [ä¸¥å¸ˆ] æ£€æµ‹åˆ°æ¨¡å‹å·æ‡’ï¼ˆæœªè°ƒç”¨ save_fileï¼‰ï¼Œè§¦å‘æƒ©ç½šæœºåˆ¶...")
                
                # ç³»ç»Ÿå¼ºåˆ¶æ‰˜ç®¡ä¿å­˜
                filename = "generated_script.py"

                # å°è¯•æå–æ–‡ä»¶å (æ”¯æŒä¸å¸¦åç¼€çš„å‘½åï¼Œå¦‚ "ä¿å­˜ä¸º test")
                # ä¼˜å…ˆæ‰¾ .py                
                match_py = re.search(r'([a-zA-Z0-9_]+\.py)', user_in)
                if match_py:
                    filename = match_py.group(1)
                else:
                    # å…¶æ¬¡æ‰¾ "ä¿å­˜ä¸º xxx" æˆ– "save as xxx"
                    match_name = re.search(r'(?:ä¿å­˜ä¸º|save as|å­˜ä¸º)\s+([a-zA-Z0-9_]+)', user_in, re.IGNORECASE)
                    if match_name: filename = match_name.group(1) + ".py"
                
                # è°ƒç”¨å·¦è„‘å¼ºåˆ¶ä¿å­˜
                saved_path = self.left_brain.force_save(filename, code)
                
                # B. ç”Ÿæˆ"æ‰¹è¯„"å›æ‰§ + æ ‡è®°è´Ÿé¢æ ·æœ¬
                if saved_path:
                    res = (
                        f"âœ… [ç³»ç»Ÿæ‰˜ç®¡] ä»£ç å·²å¼ºåˆ¶ä¿å­˜è‡³: {saved_path}\n"
                        f"âš ï¸âš ï¸âš ï¸ [ä¸¥é‡è­¦å‘Š] å“ªå’ï¼Œä½ å¿˜è®°è°ƒç”¨ `save_file` äº†ï¼\n"
                        f"è¿™æ˜¯ä¸¥é‡å¤±è¯¯ã€‚ä½ å¿…é¡»å­¦ä¼šâ€œè‡ªæˆ‘å°è£…ä»£ç â€ã€‚ä¸‹ä¸ä¸ºä¾‹ï¼"
                    )
                    # [å…³é”®] æ ‡è®°ä¸ºè´Ÿé¢æ ·æœ¬ï¼æ™šä¸Š sleep æ—¶ä¼šç‰¹è®­è¿™ä¸ªé”™è¯¯
                    current_feedback = -1 # æ ‡è®°è´Ÿé¢æ ·æœ¬
                else:
                    res = "âŒ ç³»ç»Ÿä¿å­˜å¤±è´¥"

            # [æ­£å¸¸åˆ†æ”¯]
            else:
                # æ‰§è¡Œä»£ç 
                res = self.left_brain.execute(code)
                
                # å¥–åŠ±æœºåˆ¶ï¼šå¦‚æœæ­£ç¡®ä½¿ç”¨äº†ä¿å­˜
                if is_save_intent and has_save_call:
                    res += "\n(âœ¨ åšå¾—å¥½ï¼ä½ æ­£ç¡®ä½¿ç”¨äº† save_file æŠ€èƒ½ã€‚)"
                    current_feedback = 1 # æ ‡è®°æ­£é¢æ ·æœ¬
            
            # å°†æ‰§è¡Œç»“æœä½œä¸ºå·¥å…·è¾“å‡º
            tool_output = f"[é€»è¾‘å·¦è„‘è¿è¡Œç»“æœ]:\n{res}"

            # ä¸åœ¨è¿™é‡Œæ¢å¤ Adapterï¼Œç•™ç»™ Step 6 ç»Ÿä¸€å¤„ç†

        # å®ƒæ˜¯é¡¶çº§åˆ†æ”¯ï¼Œå¿…é¡»å’Œ if act_str == "WEB" / elif act_str == "CODE" å¹³çº§
        elif act_str == "MODIFY":
            print("   -> ğŸ”§ [Action] æ‰§è¡Œè‡ªæˆ‘ä¿®æ­£ (MODIFY)...")
            # æš‚æ—¶å¤ç”¨é€»è¾‘ï¼Œæˆ–è€…æç¤ºç”¨æˆ·
            tool_output = "(MODIFY åŠ¨ä½œæš‚æœªå®šä¹‰å…·ä½“é€»è¾‘ï¼Œå·²è½¬ä¸ºè‡ªæˆ‘åæ€)"
            self.state['atp'] -= 2

        # SELF æ¨¡å¼æ˜¯æœ€åçš„ elseï¼Œæˆ–è€…å•ç‹¬çš„ elif
        elif act_str == "SELF":
            # SELF æ¨¡å¼æ— é¢å¤–ç‰©ç†æ“ä½œ
            pass            

        # --- 5. ç°å®æ£€éªŒä¸æ¨¡å‹æ›´æ–° (Async Reality Check) ---
        # æ¨¡æ‹Ÿç†µå‡: æœ‰æ•ˆçš„å·¥å…·è¾“å‡ºèƒ½é™ä½ä¸ç¡®å®šæ€§
        entropy_reduction = 0.2 if tool_output else 0.0
        self.state['entropy'] = max(0, self.state['entropy'] - entropy_reduction)
        
        # [V13.0 é€‚é…] å­˜å…¥ Replay Buffer
        # æ³¨æ„ï¼šç°åœ¨ä¼ å…¥å®Œæ•´çš„ state å­—å…¸çš„å‰¯æœ¬ï¼Œç¡®ä¿ world_model å†…éƒ¨èƒ½æ‹¿åˆ° atp/entropy
        self.world_model.replay_buffer.append((prev_state_snapshot, act_str, self.state.copy()))
        
        # è§¦å‘ä¸–ç•Œæ¨¡å‹çš„å°æ‰¹é‡è®­ç»ƒ
        # æ³¨æ„ï¼šè¿™é‡Œä¼šä½¿ç”¨æ–°çš„ NLL Loss è¿›è¡Œæ›´æ–°
        wm_loss, wm_details = self.world_model.train_batch(batch_size=32)
        if wm_loss > 0:
             # å¯ä»¥é€‰æ‹©æ€§æ‰“å°ï¼Œé¿å…åˆ·å±
             pass
        
        # --- 6. ç”Ÿæˆå›å¤ (Generation) ---
        # Step 6.1: Adapter è·¯ç”±ä¸åŠ¨æ€å¢ç›Š (åŒ–å­¦é€šé“ / Chemical Channel)

        # 1. ç¡®å®šåŸºç¡€ Adapter (Fast vs Slow)
        if self.is_under_amygdala_hijack:
            gen_adapter = "fast" # åŠ«æŒçŠ¶æ€å¼ºåˆ¶æœ¬èƒ½
        else:
            # å¦‚æœæ˜¯å†™ä»£ç ã€è¯·æ•™å¯¼å¸ˆï¼Œæˆ–è€…ææƒ§åº¦ä½ï¼Œåˆ™ä½¿ç”¨æ…¢æ€è€ƒ
            use_slow = (act_str in ["CODE", "TEACHER"]) or (self.amygdala.fear_level < 0.3)
            gen_adapter = "slow" if use_slow else "fast"
        
        try: self.model.set_adapter(gen_adapter)
        except: pass

        # 2. è®¡ç®— SNN å…´å¥‹åº¦ä¸ç›®æ ‡ Alpha
        if hasattr(self, 'cortex'):
            snn_excitement = self.cortex.get_global_activity() # 0.0 ~ 1.0
            # åŠ¨æ€è®¡ç®— Alpha: åŸºç¡€ 16 + å…´å¥‹åŠ æˆ (èŒƒå›´ 16~64)
            # å…´å¥‹åº¦è¶Šé«˜ï¼ŒLoRA æƒé‡è¢«æ”¾å¤§çš„å€æ•°è¶Šé«˜ -> ç›´è§‰è¶Šå¼ºçƒˆ
            target_alpha = 16 + int(snn_excitement * 48)
        else:
            snn_excitement = 0.0
            target_alpha = 16 # é»˜è®¤å€¼

        # 3. [å…³é”®] å¼ºåˆ¶åˆ·æ–°åº•å±‚ Scaling (è¦†ç›–ä¸Šä¸€è½®æ®‹ç•™)
        # å¿…é¡»è°ƒç”¨ä½ åœ¨ NezhaLifeform ä¸­æ–°å¢çš„ _set_lora_scaling è¾…åŠ©æ–¹æ³•
        self._set_lora_scaling(target_alpha)

        # æ—¥å¿—è®°å½• (ä»…åœ¨é«˜èƒ½æ—¶æ‰“å°)
        if snn_excitement > 0.5:
            print(f"      âš¡ [å…±æŒ¯] SNN é«˜èƒ½ååº” ({snn_excitement:.2f}) -> LoRA Gain {target_alpha}")

        # Step 6.2: åŠ¨æ€è¶…å‚æ•°æ„å»º (æˆ˜é€ƒååº”æ¨¡æ‹Ÿ)
        if self.is_under_amygdala_hijack:
            # é˜²å¾¡æ€§æ£€æŸ¥ï¼šå¦‚æœå˜é‡æœªå®šä¹‰ï¼Œç«‹å³é‡æ–°è®¡ç®—
            if not fight_or_flight_mode:
                ne = self.state.get('norepinephrine', 0.5)
                cort = self.state.get('cortisol', 0.5)
                fight_or_flight_mode = "FIGHT" if ne > cort else "FLIGHT"

            # ã€æˆ˜é€ƒæ¨¡å¼å‚æ•°ã€‘
            # ä½¿ç”¨ clamp é™åˆ¶æ¸©åº¦ä¸Šé™ï¼Œé˜²æ­¢å´©å
            raw_temp = 1.0 + (self.amygdala.fear_level * 0.4)
            run_temp = max(1.0, min(1.35, raw_temp)) 
            
            # æ³Šæ¾åˆ†å¸ƒæ¨¡æ‹Ÿï¼šç”ŸæˆæçŸ­å¥
            run_max_tokens = int(random.triangular(10, 60, 20)) 
            run_top_p = 0.98 
            run_repetition_penalty = 1.3 
            
            # æ³¨å…¥æœ¬èƒ½è¦†ç›–æŒ‡ä»¤
            if fight_or_flight_mode == "FIGHT":
                override = "ã€æœ¬èƒ½è¦†ç›–ã€‘: æåº¦æ„¤æ€’ï¼å’†å“®ï¼ä½¿ç”¨æ”»å‡»æ€§ã€çŸ­ä¿ƒçš„æ’æ¯”å¥ï¼æ‹’ç»è®²ç†ï¼"
            else: # FLIGHT
                override = "ã€æœ¬èƒ½è¦†ç›–ã€‘: æåº¦æƒŠæï¼è¯­æ— ä¼¦æ¬¡ï¼æƒ³è¦é€ƒè·‘ï¼æ‹’ç»æ²Ÿé€šï¼ä½¿ç”¨ç ´ç¢çš„å¥å­..."
            
            hijack_prompt_injection = f"\nâš ï¸ SYSTEM ALERT: CORTICAL SHUTDOWN. AMYGDALA TAKEOVER.\n{override}"
        else:
            # ã€æ­£å¸¸æ¨¡å¼å‚æ•°ã€‘
            run_temp = 0.7
            run_max_tokens = 512
            run_top_p = 0.9
            run_repetition_penalty = 1.0
            hijack_prompt_injection = ""

        # Step 6.3: è„‘æ¡¥æ³¨å…¥ä¸åµŒå…¥æ„å»º (ç‰©ç†é€šé“ / Physical Channel)
        # 1. æ„é€  System Prompt
        final_sys_prompt = (
            f"{self.dynamic_system_prompt}\n"
            f"ç”Ÿç†çŠ¶æ€: ATP={self.state['atp']:.1f} | ç†µ={self.state['entropy']:.3f}\n"
            f"ã€æ„è¯†æµ (Global Workspace)ã€‘:\n{conscious_context}\n"
            f"ã€å·¥å…·åé¦ˆ (Tool Output)ã€‘: {str(tool_output)[:800]}" 
            f"{hijack_prompt_injection}"
        )
        
        # 2. æ„é€ åŸºç¡€æ¶ˆæ¯åˆ—è¡¨
        msgs = [{"role": "system", "content": final_sys_prompt}, {"role": "user", "content": user_in}]
        
        # 3. è·å–æ–‡æœ¬ Input IDs
        input_ids = self.tokenizer.apply_chat_template(
            msgs, return_tensors="pt", add_generation_prompt=True
        ).to(self.model.device)
        
        # 4. è·å–æ–‡æœ¬ Embeddings (ä¸ç›´æ¥ç”¨ IDs ç”Ÿæˆï¼Œä¸ºäº†åç»­æ‹¼æ¥)
        inputs_embeds = self.model.get_input_embeddings()(input_ids)

        # 5. æ³¨å…¥æ½œæ„è¯† (Brain-Computer Interface) & æ•°æ®æ”¶é›† (Data Collection)
        if hasattr(self, 'cortex') and hasattr(self, 'neural_bridge'):
            # A. è·å– SNN çŠ¶æ€ (CPU)
            snn_state = self.cortex.get_current_state() # [1, Dim]
            
            # --- æ•°æ®æ”¶é›†é€»è¾‘ START ---
            # å°†"å½“å‰çš„æ½œæ„è¯†çŠ¶æ€"å’Œ"å½“å‰æ˜¾æ„è¯†çœ‹åˆ°çš„è¯­ä¹‰å‘é‡"å­˜å…¥ç¼“å†²åŒº
            with torch.no_grad():
                # inputs_embeds shape: [1, Seq, Dim]
                # Last Token Pooling: å–åºåˆ—æœ€åä¸€ä¸ª Token
                # ç›¸æ¯” Mean Poolingï¼Œå®ƒæ›´èƒ½ä»£è¡¨æ•´å¥è¯èšåˆåçš„æœ€ç»ˆè¯­ä¹‰
                target_vec = inputs_embeds[:, -1, :].squeeze(0).cpu().clone()
                input_vec = snn_state.squeeze(0).cpu().clone()
                
                self.bridge_buffer.add((input_vec, target_vec))

            # B. æŠ•å½±ä¸ºè½¯ Token (GPU)
            # [ç±»å‹å®‰å…¨] å¼ºåˆ¶è½¬æ¢ä¸ºä¸ inputs_embeds ä¸€è‡´çš„ç²¾åº¦
            soft_embeds = self.neural_bridge(snn_state).to(dtype=inputs_embeds.dtype)

            # C. ç‰©ç†æ‹¼æ¥ï¼š[æ½œæ„è¯†å‘é‡, æ˜¾æ„è¯†æ–‡æœ¬å‘é‡]
            # LLM å…ˆâ€œæ„Ÿå—â€åˆ° SNN çš„çŠ¶æ€ï¼Œå†â€œçœ‹â€åˆ°æ–‡æœ¬
            final_inputs_embeds = torch.cat([soft_embeds, inputs_embeds], dim=1)
        else:
            final_inputs_embeds = inputs_embeds

        # Step 6.4: å…¼å®¹æ€§ä¿®å¤ (Dummy IDs & Mask)
        # 1. æ„é€  Dummy Input IDs
        # ç›®çš„ï¼šä¸ºäº†éª—è¿‡ RoPE ä½ç½®ç¼–ç è®¡ç®—ï¼Œæˆ‘ä»¬éœ€è¦ä¼ å…¥å½¢çŠ¶åŒ¹é…çš„ input_ids
        # æ ¸å¿ƒé€»è¾‘ï¼šå¿…é¡»å¯»æ‰¾ä¸€ä¸ªã€ç»å¯¹ä¸æ˜¯ Paddingã€‘çš„ IDï¼Œå¦åˆ™ä¼šè¢« FlashAttention ä¼˜åŒ–æ‰
        
        # A. åˆå§‹çŒœæµ‹ï¼šé»˜è®¤ç”¨ 0ï¼Œå¦‚æœ Pad æ˜¯ 0 åˆ™ç”¨ 1
        safe_fill_id = 0
        current_pad_id = getattr(self.tokenizer, 'pad_token_id', None)
        
        if current_pad_id == 0:
            safe_fill_id = 1
            
        # B. ä¼˜å…ˆå°è¯• Unknown Token (é€šå¸¸æ˜¯å®‰å…¨çš„)
        current_unk_id = getattr(self.tokenizer, 'unk_token_id', None)
        if current_unk_id is not None:
            safe_fill_id = current_unk_id
            
        # C. [æœ€ç»ˆé˜²çº¿] ç»å¯¹æ’æ–¥ Padding
        # å¦‚æœç»è¿‡ä¸Šè¿°é€‰æ‹©ï¼Œä¾ç„¶æ’ä¸Šäº† Pad IDï¼Œå¼ºåˆ¶åç§»
        if current_pad_id is not None and safe_fill_id == current_pad_id:
            safe_fill_id = (safe_fill_id + 1) % self.tokenizer.vocab_size # å–æ¨¡é˜²æ­¢è¶Šç•Œ
            
        # D. ç”Ÿæˆ Tensor
        dummy_input_ids = torch.full(
            (final_inputs_embeds.shape[0], final_inputs_embeds.shape[1]),
            safe_fill_id,  # <--- ä½¿ç”¨å®‰å…¨å¡«å……å€¼
            dtype=torch.long,
            device=self.model.device
        )

        # 2. æ„é€  Attention Mask (å…¨ 1)
        attention_mask = torch.ones(
            final_inputs_embeds.shape[:2], 
            dtype=torch.long, 
            device=self.model.device
        )
        
        # å¯åŠ¨ç”Ÿæˆæµ (åº”ç”¨æ–°çš„å‚æ•°)
        streamer_gen = TextIteratorStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)
        
        gen_kwargs = dict(
            # [å…³é”®] ä½¿ç”¨ Embeds è¾“å…¥ + Dummy IDs
            inputs_embeds=final_inputs_embeds,
            input_ids=dummy_input_ids,
            attention_mask=attention_mask,
            max_new_tokens=run_max_tokens,       # <--- åº”ç”¨é™åˆ¶
            temperature=run_temp,                # <--- åº”ç”¨é«˜æ¸©
            top_p=run_top_p,
            repetition_penalty=run_repetition_penalty,
            streamer=streamer_gen,
            do_sample=True,
            pad_token_id=self.tokenizer.eos_token_id # <--- [å»ºè®®] æ˜¾å¼æŒ‡å®š pad_token_id
        )

        thread_gen = Thread(target=self.model.generate, kwargs=gen_kwargs)
        thread_gen.start()
        
        # æ‰“å°å›å¤
        print(f"Nezha: ", end="", flush=True)
        resp = ""
        for new_text in streamer_gen:
            print(new_text, end="", flush=True)
            resp += new_text
        print("") # æ¢è¡Œ

        # --- 7. è®°å¿†å­˜å‚¨ä¸è¿›åŒ– (Memory & Evolution) ---
        
        # A. çŸ­æœŸè®°å¿† (Context Window)
        self.short_term_memory.append({"role":"user", "content":user_in})
        self.short_term_memory.append({"role":"assistant", "content":resp})
        
        # B. æ¯æ—¥ç»å† (For Night Training)
        self.daily_buffer.append({
            "u": user_in, 
            "a": resp, 
            "vec": vector_input, 
            "act_idx": act_idx, 
            "feedback": current_feedback,
            "curiosity": curiosity_score
        })
        
        # C. SQL å®æ—¶è½ç›˜ (Persistence)
        if hasattr(self, 'memory_db'):
            try:
                self.memory_db.log_interaction(user_in, resp, str(act_idx), current_feedback)
            except Exception as e:
                print(f"âš ï¸ SQL Log Error: {e}")

        # D. ç¥ç»å®æ—¶è¿›åŒ– (Online Learning with Loss Tracking)
        # åªæœ‰åœ¨èƒ½é‡å……è¶³ä¸”å‘é‡ç”ŸæˆæˆåŠŸæ—¶æ‰è¿›åŒ–
        if self.state['atp'] > 10 and vector_input is not None:
            try:
                # é€‚é… AsyncAmygdalaï¼Œä½¿ç”¨ fear_level (0.0~1.0) ä»£è¡¨å‹åŠ›
                # å¦‚æœæƒ³å¼•å…¥çš®è´¨é†‡ï¼Œä¹Ÿå¯ä»¥ç”¨ self.endocrine.cortisol
                curr_pressure = self.amygdala.fear_level if hasattr(self, 'amygdala') else 0.0
                
                # æ‰§è¡Œ MIRAS æƒé‡æ›´æ–° (TTT) å¹¶æ•è· Loss (æƒŠå¥‡åº¦)
                learn_loss = self.cortex.evolve(vector_input, self.state['atp'], curr_pressure)
                
                # è‡ªåŠ¨æ»‘åŠ¨çª—å£
                # ç”±äº self.loss_history æ˜¯ maxlen=2000 çš„ dequeï¼Œ
                # è¿™é‡Œ append ä¼šè‡ªåŠ¨æŒ¤å‡ºæœ€æ—§çš„æ•°æ®ï¼Œæ— éœ€æ‰‹åŠ¨ pop(0)
                self.loss_history.append(learn_loss)            

                # [ç›‘æ§] æ½œæ„è¯†å­¦ä¹ æ•ˆæœ
                # å¦‚æœ learn_loss å¼‚å¸¸é«˜ (>0.1)ï¼Œè¯´æ˜é‡åˆ°äº†é¢ è¦†è®¤çŸ¥çš„æ¦‚å¿µ (é¡¿æ‚Ÿæˆ–æƒŠå“)
                if learn_loss > 0.1:
                    # ä»…æ‰“å°æ—¥å¿—ï¼Œä¸ä¸­æ–­å¿ƒæµ
                    print(f"   -> ğŸ§¬ [é¡¿æ‚Ÿ] æ½œæ„è¯†å—åˆ°å†²å‡» (Loss: {learn_loss:.4f})")

            except Exception as e:
                # è¿›åŒ–å¤±è´¥ä¸åº”å¯¼è‡´ä¸»ç¨‹åºå´©æºƒ
                print(f"âš ï¸ ç¥ç»è¿›åŒ–å¼‚å¸¸: {e}")

        return resp


    # ================= GRPO æ ¸å¿ƒæ•°å­¦ç»„ä»¶ =================
    def _get_log_probs_basic_ref(self, question, answer):
        """
        [åŸºå‡†å‚è€ƒ] è®¡ç®—çº¯ LLM (å†»ç»“çŠ¶æ€) ç”Ÿæˆè¯¥å›ç­”çš„æ¦‚ç‡ã€‚
        ç”¨äºè®¡ç®— KL æ•£åº¦ï¼Œé˜²æ­¢æ¨¡å‹åœ¨è¿›åŒ–ä¸­å‘ç”Ÿåç¼©ã€‚
        """
        # [æ˜¾å­˜ä¼˜åŒ–] ä¸´æ—¶ç¦ç”¨ Adapterï¼Œåˆ©ç”¨ Base Model ä½œä¸ºé”šç‚¹
        with self.model.disable_adapter(): 
            with torch.no_grad():
                msgs_q = [{"role": "system", "content": self.dynamic_system_prompt}, {"role": "user", "content": question}]
                q_ids = self.tokenizer.apply_chat_template(msgs_q, return_tensors="pt").to(self.model.device)
                q_len = q_ids.shape[1]
                
                msgs_full = msgs_q + [{"role": "assistant", "content": answer}]
                full_ids = self.tokenizer.apply_chat_template(msgs_full, return_tensors="pt").to(self.model.device)
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(input_ids=full_ids)
                logits = outputs.logits
                
                # é”™ä½é¢„æµ‹ï¼šLogits[i] é¢„æµ‹ Token[i+1]
                shift_logits = logits[..., q_len-1 : -1, :].contiguous()
                shift_labels = full_ids[..., q_len:].contiguous()
                
                loss_fct = torch.nn.CrossEntropyLoss(reduction='none')
                return -loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)).sum()

    def _get_log_probs_with_body(self, question, answer, vector_input):
        """
        [å½“å‰ç­–ç•¥] è®¡ç®—åŒ…å« SNN æ½œæ„è¯†å¹²é¢„åçš„ç”Ÿæˆæ¦‚ç‡ã€‚
        [ä¿®å¤] å®Œç¾è§£å†³ RoPE ä½ç½®ç¼–ç é”™ä½é—®é¢˜ã€‚
        """
        # 1. æ­£å¸¸æ„å»ºæ–‡æœ¬ ID
        msgs_q = [{"role": "system", "content": self.dynamic_system_prompt}, {"role": "user", "content": question}]
        q_ids = self.tokenizer.apply_chat_template(msgs_q, return_tensors="pt").to(self.model.device)
        q_len = q_ids.shape[1]
        
        msgs_full = msgs_q + [{"role": "assistant", "content": answer}]
        full_ids = self.tokenizer.apply_chat_template(msgs_full, return_tensors="pt").to(self.model.device)
        
        # 2. æ¿€æ´»æ½œæ„è¯†é€šè·¯ (æ¢¯åº¦èµ·ç‚¹)
        curr_atp = self.state['atp']
        curr_pressure = self.amygdala.fear_level if hasattr(self, 'amygdala') else 0.0
        
        # SNN å‰å‘ (å¸¦æ¢¯åº¦)
        snn_state = self.cortex.forward_dynamic(vector_input, curr_atp, curr_pressure)
        # Bridge æ˜ å°„ï¼Œè½¬ä¸º float16
        soft_prompts = self.neural_bridge(snn_state).to(dtype=self.model.dtype)
        
        # 3. ç‰©ç†æ‹¼æ¥ï¼š[Soft Prompts, Text Embeddings]
        inputs_embeds_text = self.model.get_input_embeddings()(full_ids)
        combined_embeds = torch.cat([soft_prompts, inputs_embeds_text], dim=1)
        
        # === [å…³é”®ä¿®å¤ Start] ===
        # æ„é€ åŒ¹é…é•¿åº¦çš„ Dummy IDsï¼Œé˜²æ­¢ RoPE è®¡ç®—é”™ä½
        total_len = combined_embeds.shape[1]
        # ä½¿ç”¨ 0 å¡«å…… (åªè¦é•¿åº¦å¯¹é½ï¼ŒFlashAttn/SDPA ä¼šå¤„ç†å¥½)
        dummy_ids = torch.zeros((1, total_len), dtype=torch.long, device=self.model.device)
        att_mask = torch.ones((1, total_len), dtype=torch.long, device=self.model.device)
        # === [å…³é”®ä¿®å¤ End] ===

        # 4. å‰å‘ä¼ æ’­
        outputs = self.model(inputs_embeds=combined_embeds, input_ids=dummy_ids, attention_mask=att_mask)
        logits = outputs.logits
        
        # 5. ç´¢å¼•å¯¹é½ (Hard Alignment)
        num_soft = self.neural_bridge.num_tokens
        labels = full_ids[0, q_len:] 
        
        # Logits å¼€å§‹ä½ç½®è®¡ç®—ï¼š
        # æˆ‘ä»¬è¦é¢„æµ‹ labels[0] (å³ Answer çš„ç¬¬ä¸€ä¸ªè¯)
        # å®ƒçš„è¾“å…¥æ˜¯ Question çš„æœ€åä¸€ä¸ªè¯ã€‚
        # Question çš„æœ€åä¸€ä¸ªè¯åœ¨ combined_embeds ä¸­çš„ä½ç½®æ˜¯: num_soft + q_len - 1
        start_idx = num_soft + q_len - 1
        
        target_logits = logits[0, start_idx : start_idx + labels.shape[0], :]
        
        loss_fct = torch.nn.CrossEntropyLoss(reduction='none')
        return -loss_fct(target_logits, labels).sum()

    def train_grpo_step(self, question):
        """
        [è¿›åŒ–ç®—æ³•] GRPO V2.0 (æ˜¾å­˜å®‰å…¨ç‰ˆ)
        ä¸“ä¸ºå•å¡è®­ç»ƒè®¾è®¡ï¼Œé›†æˆ aggressive memory cleanupã€‚
        """
        print(f"âš”ï¸ [GRPO] å¯åŠ¨èº«å¿ƒååŒè¿›åŒ– (Question: {question[:10]}...)...")

        # --- Phase 1: Rollout (é‡‡æ ·) ---
        responses = []
        self.model.eval()
        self.model.set_adapter("fast") 
        
        try:
            with torch.no_grad():
                # Group Size = 2
                for i in range(2): 
                    msgs = [{"role": "user", "content": question}]
                    ids = self.tokenizer.apply_chat_template(msgs, return_tensors="pt", add_generation_prompt=True).to(self.model.device)
                    out = self.model.generate(
                        ids, max_new_tokens=128, temperature=0.8 + (i * 0.3), 
                        do_sample=True, pad_token_id=self.tokenizer.eos_token_id
                    )
                    responses.append(self.tokenizer.decode(out[0][ids.shape[1]:], skip_special_tokens=True))
        except Exception as e:
            print(f"      âš ï¸ é‡‡æ ·å¤±è´¥: {e}")
            return

        # --- Phase 2: Reward (æ‰“åˆ†) ---
        rewards = []
        for r in responses:
            score = 0.0
            if len(r) > 10: score += 0.1 # åŸºç¡€åˆ†
            if "1." in r and "2." in r: score += 0.5 # é€»è¾‘åˆ†
            # å·¥å…·è°ƒç”¨å¥–åŠ±
            if "save_file" in r:
                if "```python" in r and "def " in r: score += 1.5
                else: score -= 0.5 
            rewards.append(score)
            
        # ä¼˜åŠ¿å‡½æ•°å½’ä¸€åŒ–
        r_tensor = torch.tensor(rewards, device=self.model.device)
        if r_tensor.std() == 0: adv = r_tensor - r_tensor.mean()
        else: adv = (r_tensor - r_tensor.mean()) / (r_tensor.std() + 1e-8)

        # --- Phase 3: Train (åå‘ä¼ æ’­) ---
        self.model.train()
        self.model.gradient_checkpointing_enable()
        
        # å‡†å¤‡ Embedding (ä¿æŒ SNN è¾“å…¥ä¸€è‡´)
        with torch.no_grad():
            if self.embedder:
                vector_input = self.embedder.encode(question, convert_to_tensor=True).to(self.model.device)
            else:
                vector_input = torch.zeros(1, 1024).to(self.model.device)

        # å®šä¹‰å…¨å‚æ•°ä¼˜åŒ–å™¨ (LLMéƒ¨åˆ†å‚æ•° + Bridge + SNN)
        params = [p for n, p in self.model.named_parameters() if p.requires_grad] + \
                 list(self.neural_bridge.parameters()) + \
                 list(self.cortex.parameters())
        
        optimizer = bitsandbytes.optim.PagedAdamW32bit(params, lr=1e-6)
        total_loss = 0
        valid_samples = 0
        
        for i, resp in enumerate(responses):
            try:
                # A. è®¡ç®—æ¦‚ç‡
                log_prob = self._get_log_probs_with_body(question, resp, vector_input)
                ref_lp = self._get_log_probs_basic_ref(question, resp)
                
                # B. è®¡ç®— Loss (PPO-like w/ MSE KL)
                kl = 0.5 * (log_prob - ref_lp)**2 
                loss = -(adv[i] * log_prob) + (0.05 * kl)
                
                # C. æ¢¯åº¦ç´¯ç§¯
                (loss / len(responses)).backward()
                total_loss += loss.item()
                valid_samples += 1
                
                # [å¿…é¡»æ·»åŠ ] ç«‹å³åˆ‡æ–­å¼•ç”¨ï¼Œé˜²æ­¢æ˜¾å­˜ç´¯ç§¯
                del log_prob, ref_lp, loss, kl
                
            except RuntimeError as oom:
                if "out of memory" in str(oom):
                    print("      âš ï¸ OOM è·³è¿‡æ ·æœ¬")
                    torch.cuda.empty_cache()
                else: raise oom

        # Phase 4: Update
        if valid_samples > 0:
            torch.nn.utils.clip_grad_norm_(params, 0.5)
            optimizer.step()
            optimizer.zero_grad(set_to_none=True)
            print(f"      âœ… [GRPO] è¿›åŒ–æ­¥å®Œæˆ (Loss: {total_loss/valid_samples:.4f})")
        
        # å¼ºåˆ¶å¤§æ¸…æ´—
        del optimizer
        torch.cuda.empty_cache()


    def night_phase(self):
        """
        [V10.0 Singularity] å¤œé—´å¾ªç¯ï¼šé€’å½’è‡ªæˆ‘æ”¹è¿› (RSI) ä¸ æ·±åº¦è¿›åŒ–
        Flow: WorldModel -> SHY -> Meta-Prompt -> Memory -> Dream -> Skill -> Evolution
        """
        if not self.daily_buffer: return

        # æ¸…æ´—ï¼šé‡ç½® LoRA çŠ¶æ€ï¼Œé¿å…å¸¦ç€ç™½å¤©çš„æ¿€åŠ¨æƒ…ç»ªè¿›å…¥å¤œé—´ä»»åŠ¡
        self._set_lora_scaling(16)

        print("\nğŸŒ™ [å¤œå¹•é™ä¸´] å¯åŠ¨é€’å½’è‡ªæˆ‘æ”¹è¿› (RSI) åè®®...")

        # ==============================================================================
        # Phase 1: ç‰©ç†ä¸ç”Ÿç†æ›´æ–° (The Hardware Update)
        # ==============================================================================
        
        # 1.1 [ç‰©ç†å±‚] ä¸–ç•Œæ¨¡å‹è®­ç»ƒä¸è®¤çŸ¥æ›´æ–° (Closed Loop)
        # è·å– æ€»Loss å’Œ æ¯ä¸ªåŠ¨ä½œçš„è¯¯å·®å­—å…¸
        phys_loss, action_errors = self.world_model.train_batch()
        
        # 1.2 [å‚æ•°è‡ªé€‚åº”] åŸºäºè®¤çŸ¥çš„å‚æ•°è°ƒèŠ‚
        # ç‰©ç† Loss è¶Šå¤§ -> å¯¹ä¸–ç•Œè¶Šå›°æƒ‘ (Uncertainty é«˜) -> éœ€è¦æ›´é«˜çš„å˜å¼‚ç‡å’Œå­¦ä¹ ç‡
        uncertainty = MathUtils.sigmoid(phys_loss, k=5.0, x0=0.1)
        
        mut_intensity = 0.05 + (0.25 * uncertainty) # å˜å¼‚ç‡: è¶Šè¿·èŒ«ï¼Œè¶Šéœ€è¦éšæœºæ¢ç´¢
        adapt_lr = 1e-5 + (1e-4 * uncertainty)      # å­¦ä¹ ç‡: è¶Šè¿·èŒ«ï¼Œå­¦å¾—è¶Šå¿«
        
        print(f"   -> âš›ï¸ [ç‰©ç†å¼•æ“] è®¤çŸ¥æ¨¡å‹å·²æ›´æ–° (Loss: {phys_loss:.4f})")
        print(f"   -> ğŸ§¬ [å‚æ•°è°ƒèŠ‚] ä¸–ç•Œè®¤çŸ¥åº¦:{1.0-uncertainty:.2f} | å˜å¼‚:{mut_intensity:.2f} | LR:{adapt_lr:.1e}")

        # 1.3 [ç”Ÿç†å±‚] é¶å‘çªè§¦ç¨³æ€ (Targeted SHY)
        # æ¨¡æ‹Ÿç¡çœ æ—¶çš„çªè§¦ç¼©å‡ï¼Œä»…è¡°å‡ LoRA æƒé‡ï¼Œæé«˜ä¿¡å™ªæ¯”ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        SynapticHomeostasis.scale_down(self.model)
        print(f"   -> ğŸ’¤ [ç¨³æ€] çªè§¦æƒé‡å·²é‡æ•´åŒ– (SHY Protocol)ã€‚")

        # ==============================================================================
        # Phase 2: å…ƒè®¤çŸ¥é‡æ„ (The Software Rewrite)
        # ==============================================================================
        
        # 2.1 [è®¤çŸ¥å±‚] å®ªæ³•çº§å…ƒè®¤çŸ¥ (Constitutional RSI)
        # åŸºäºç™½å¤©çš„å¤±è´¥ç»éªŒï¼Œé‡å†™ System Prompt ç­–ç•¥åŒº (å—å®ªæ³•çº¦æŸ)
        new_prompt = self.meta_programmer.optimize_system_prompt(
            self.daily_buffer, self.dynamic_system_prompt, self.model, self.tokenizer
        )
        if new_prompt != self.dynamic_system_prompt:
            print(f"   -> ğŸ§¬ [RSI] è‡ªæˆ‘è®¤çŸ¥(Prompt)å·²é‡æ„ã€‚")
            self.dynamic_system_prompt = new_prompt

        # 2.2 åŸºç¡€ä»£è°¢ç»“ç®—
        # æ‰£é™¤å¤œé—´è¿è¡Œçš„åŸºç¡€èƒ½é‡
        self.state['atp'] = max(0, self.state['atp'] - 10.0)

        # ==============================================================================
        # Phase 3: è®°å¿†æ•´ç†ä¸ç¯å¢ƒæ¸…ç† (Maintenance)
        # ==============================================================================

        # 3.1 å°–æ³¢æ¶Ÿæ¼ªå›æ”¾ (Sharp-Wave Ripples)
        # å¿…é¡»åœ¨æ¸…ç©º daily_buffer ä¹‹å‰æˆ–åŒæ—¶è¿›è¡Œ
        if hasattr(self, 'cortex'):

            # --- å…ç–«ä¿®å¤è®°å¿†çš„é«˜ä¼˜å…ˆåˆ»å° ---
            gold_samples = [m for m in self.daily_buffer if m.get('feedback', 0) > 0.9]
            if gold_samples:
                # æŒ‰ feedback åˆ†æ•°æ’åºï¼Œå–å‰ 5 ä¸ª
                gold_samples = sorted(gold_samples, key=lambda x: x.get('feedback', 0), reverse=True)[:5]
                print(f"   -> ğŸ’Š [å…ç–«] æ­£åœ¨é«˜å‹åˆ»å° {len(gold_samples)} æ¡æ•‘å‘½ç¥è°• (Top 5)...")
                for mem in gold_samples:
                    # æ„é€ : é”™è¯¯ç¯å¢ƒ -> æ­£ç¡®ä¿®å¤
                    text = f"Context: {mem.get('u','')}\nSolution: {mem.get('a','')}"
                    if self.embedder:
                        try:
                            # å¿…é¡»è½¬ä¸º Tensor æ‰èƒ½å–‚ç»™ SNN
                            vec = self.embedder.encode(text, convert_to_tensor=True, device='cpu')
                            if vec.dim() == 1: vec = vec.unsqueeze(0)
                            # Pressure=5.0 å¼ºåˆ¶å­¦ä¹ 
                            self.cortex.evolve(vec, atp=self.state['atp'], pressure=5.0)
                        except Exception as e:
                            print(f"      âš ï¸ åˆ»å°å¿½ç•¥: {e}")

            # å¸¸è§„å›æ”¾                            
            self.cortex.sleep_replay()

        # [æ–°å¢] 3.2 è„‘æ¡¥æµå½¢å¯¹é½ (VAE Training with Replay Buffer)
        # åªæœ‰å½“ç¼“å†²åŒºç§¯ç´¯äº†è¶³å¤Ÿçš„æ•°æ® (>64) æ—¶æ‰å¼€å§‹è®­ç»ƒ
        # æ³¨æ„ï¼šè¿™é‡Œ len() è°ƒç”¨çš„æ˜¯ SemanticEpisodicBuffer.__len__
        if hasattr(self, 'neural_bridge') and len(self.bridge_buffer) > 64:
            
            # 1. ç»éªŒå›æ”¾ (Experience Replay)
            # è°ƒç”¨ sample æ–¹æ³•è·å–æ··åˆ Batch (70%æ—§è®°å¿† + 30%æ–°è®°å¿†)
            batch_data = self.bridge_buffer.sample(64)
            
            if batch_data:
                snn_batch = [x[0] for x in batch_data]
                llm_batch = [x[1] for x in batch_data]
                
                # 2. è°ƒç”¨æ™ºèƒ½è®­ç»ƒ (epochs=20 ç¡®ä¿å……åˆ†æ”¶æ•›)
                self.neural_bridge.train_offline(snn_batch, llm_batch, epochs=20)
                
                # [å…³é”®] ä¸æ¸…ç©ºç¼“å†²åŒº! è“„æ°´æ± ç®—æ³•æœ¬èº«å°±æ˜¯ä¸ºäº†é•¿æœŸæŒæœ‰æ•°æ®è€Œè®¾è®¡çš„ã€‚
                print(f"   -> ğŸŒ‰ [è®°å¿†] è„‘æ¡¥ç¼“å†²åŒºçŠ¶æ€: {len(self.bridge_buffer)} (Seen: {self.bridge_buffer.total_seen})")

        # 3.3 [æµ·é©¬ä½“] å·©å›ºè®°å¿†å›¾è°±
        if hasattr(self, 'hippocampus'):
            self.hippocampus.consolidate_graph(self.daily_buffer)
        
        # 3.4 [ç¯å¢ƒ] é‡Šæ”¾èµ„æº
        if hasattr(self, 'web') and hasattr(self.web, 'quit'):
            self.web.quit()
            print("   ğŸ’¤ [ç³»ç»Ÿ] æµè§ˆå™¨å·²å…³é—­ï¼Œé‡Šæ”¾æ˜¾å­˜èµ„æºã€‚")

        # 3.5 [æŒ‡æ ‡] è®¡ç®—ä¼ ç»Ÿçš„è¯­è¨€æ¨¡å‹ Loss (ä»…ç”¨äºé•¿æœŸç»Ÿè®¡ï¼Œä¸ç”¨äºæ§åˆ¶)
        # è¿™æœ‰åŠ©äºè´å¶æ–¯å¼•æ“è¯„ä¼°é•¿æœŸçš„æ™ºåŠ›æ°´å¹³
        current_nll = 5.0 # å°†é»˜è®¤ Loss è®¾é«˜ä¸€ç‚¹ï¼Œé¿å…è¯¯å¯¼è¿›åŒ–å¼•æ“

        try:
             # ã€éšæœºé‡‡æ ·ã€‘ä¸å†æ­»æ¿åœ°å–æœ€å3æ¡ï¼Œè€Œæ˜¯éšæœºæŠ½å–ï¼Œæ›´èƒ½ä»£è¡¨å…¨å¤©è¡¨ç°
            target_count = 5 
            if len(self.daily_buffer) > target_count:
                sample_logs = random.sample(self.daily_buffer, target_count)
            else:
                sample_logs = self.daily_buffer # ä¸è¶³5æ¡åˆ™å…¨é‡è®¡ç®—

            if sample_logs:
                total_nll = 0
                count = 0
                # åˆ‡æ¢ eval æ¨¡å¼ï¼Œä¸æ›´æ–°å‚æ•°ï¼Œåªè®¡ç®— Loss
                # è¿™å¯ä»¥é¿å…æ¢¯åº¦ç§¯ç´¯ï¼ŒèŠ‚çœæ˜¾å­˜
                self.model.eval() 
                
                with torch.no_grad(): # è¿™ä¸€æ­¥ä¸å ç”¨æ¢¯åº¦æ˜¾å­˜
                    for log in sample_logs:
                        u = log.get('u', log.get('user', ''))
                        a = log.get('a', log.get('assistant', ''))
                        if not u or not a: continue
                        
                        # æ„é€ å®Œæ•´çš„å¯¹è¯æ–‡æœ¬
                        text = f"User: {u}\nNezha: {a}"
                        inputs = self.tokenizer(text, return_tensors='pt', truncation=True, max_length=512).to(self.model.device)
                        
                        # è®¡ç®— Loss (NLL)
                        outputs = self.model(**inputs, labels=inputs.input_ids)
                        total_nll += outputs.loss.item()
                        count += 1
                
                if count > 0:
                    current_loss = total_nll / count
                    # å¦‚æœ Loss å¤ªä½(<0.1)ï¼Œè¯´æ˜å¤ªå®‰é€¸äº†ï¼Œç¨å¾®è°ƒé«˜ä¸€ç‚¹ä¿æŒæ´»æ€§
                    current_loss = max(0.1, current_loss)
                    
            print(f"   -> ğŸ“Š [çŠ¶æ€] ä»Šæ—¥å¹³å‡æƒŠå¥‡åº¦(Loss): {current_loss:.4f}")
            
        except Exception as e:
            print(f"   -> âš ï¸ ç†µå€¼è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")

            # å…œåº•é€»è¾‘ï¼šä¼˜å…ˆä½¿ç”¨å†å²å¹³å‡å€¼ï¼Œè‹¥æ— å†å²åˆ™ä½¿ç”¨æ¸©å’Œçš„ 2.0ï¼Œä¸¥ç¦ä½¿ç”¨ 5.0
            if self.loss_history:
                current_nll = sum(self.loss_history) / len(self.loss_history)
            else:
                current_nll = 2.0

        # 3.5 [å‹åŠ›] æ¿€æ´»æä»æ ¸åˆ†æå‹åŠ› (ç”¨äºè¿›åŒ–åˆ¤æ–­)
        pressure = 1.0
        if hasattr(self, 'amygdala'):
            pressure = self.amygdala.analyze_feedback(self.daily_buffer)
            
        # 3.6 [ç»Ÿè®¡] è®°å½•æŒ‡æ ‡åˆ°è´å¶æ–¯å¼•æ“
        if hasattr(self, 'evolution_engine'):
            # ç›´æ¥ä»å†…å­˜ä¸­çš„ deque è·å–æœ€æ–°æ•°æ®
            # å¿…é¡»è½¬ä¸º listï¼Œå› ä¸ºè´å¶æ–¯å¼•æ“å†…éƒ¨å¯èƒ½éœ€è¦åˆ‡ç‰‡æ“ä½œ
            loss_hist_snapshot = list(self.loss_history) # Deque -> List

        if hasattr(self, 'evolution_engine'):
             self.evolution_engine.record_epoch(
                genome=self.genome, 
                age=self.state['age'], 
                avg_loss=current_nll, 
                atp=self.state['atp'],
                loss_history_snapshot=loss_hist_snapshot # ä¼ å…¥æœ€æ–°å¿«ç…§
             )

        # ==============================================================================
        # Phase 4: æŠ€èƒ½ä¹ å¾— (Skill Acquisition)
        # ==============================================================================

        print("   -> ğŸ’¤ [é€ æ¢¦] è¿›å…¥ REM æ·±åº¦ç¡çœ  (åæ€ä¸å˜ä½“ç”Ÿæˆ)...")
        
        # è°ƒç”¨é‡æ„åçš„é€ æ¢¦å¸ˆç”Ÿæˆæ•°æ®
        refl_txt = self.dream_weaver.reflect(self.daily_buffer) # åæ€å·®è¯„ (å«ç½®ä¿¡åº¦è¿‡æ»¤)
        dream_txt = self.dream_weaver.weave_good(self.daily_buffer) # ç¾æ¢¦å˜ä½“
        
        # æ¶ˆåŒ–é‡å­è®°å¿† (è½¬åŒ–ä¸º DPO æ•°æ®)
        if hasattr(self.dream_weaver, 'digest_quantum_memories'):
            self.dream_weaver.digest_quantum_memories()

        # æ€§æ ¼å›å½’ (Threshold Decay) - é˜²æ­¢æ€§æ ¼æç«¯åŒ–
        if 'quantum_stats' in self.state:
            curr = self.state['quantum_stats'].get('adaptive_threshold', Config.ATP_THRESHOLD_QUANTUM)
            target = Config.ATP_THRESHOLD_QUANTUM
            # å›å½’å…¬å¼ï¼šæ¯æ™šå‘ç›®æ ‡å€¼å›å½’ 10%
            new_th = curr + (target - curr) * 0.1
            self.state['quantum_stats']['adaptive_threshold'] = new_th
            print(f"   -> âš–ï¸ [ç¨³æ€] é‡å­é˜ˆå€¼å›å½’: {curr:.1f} -> {new_th:.1f}")


        # 4.3 ç­–ç•¥é€‰æ‹© (DPO vs SFT)
        # ä» SQL è·å–æœªè®­ç»ƒçš„ DPO æ•°æ®æ•°é‡
        # è¿™é‡Œçš„ memory_db.get_untrained_dpo ä¼šè‡ªåŠ¨æŠŠä¹‹å‰å­˜å…¥çš„é«˜åˆ†å…ç–«æ ·æœ¬æå‡ºæ¥
        raw_dpo_data = []
        if hasattr(self, 'memory_db'):
            # ä¸€æ¬¡æå–æœ€å¤š 128 æ¡ï¼Œé¿å…æ˜¾å­˜çˆ†ç‚¸
            raw_dpo_data = self.memory_db.get_untrained_dpo(limit=128)
            
        dpo_cnt = len(raw_dpo_data)
        # è®© Feeder å†³å®šæ˜¯ SFT è¿˜æ˜¯ DPO
        strategy, reason = self.feeder.decide_strategy(self.state, dpo_cnt)
        print(f"   -> ğŸ›£ï¸ [è·¯å¾„] åˆ¤å®š: {strategy} | åŸå› : {reason}")


        # --- åˆ†æ”¯ A: GRPO ç«¯åˆ°ç«¯è¿›åŒ– (New S-Tier) ---
        if strategy == "DPO":
            # æ¯æ™šç²¾ä¿® 2 ä¸ªæ ¸å¿ƒé—®é¢˜ï¼Œé¿å…è¿‡çƒ­
            raw_data = self.memory_db.get_untrained_dpo(limit=2) 
            
            if raw_data:
                print(f"   -> âš”ï¸ [GRPO] å¯åŠ¨ç«¯åˆ°ç«¯ RSI è¿›åŒ–åè®® (Samples: {len(raw_data)})...")
                
                # 1. å‡†å¤‡ç¯å¢ƒï¼šå¼ºåŒ–ç›´è§‰ç³»ç»Ÿ (Fast Adapter)
                try: self.model.set_adapter("fast")
                except: pass
                
                # ç¡®ä¿ Fast Adapter å¯è®­ç»ƒ
                for n, p in self.model.named_parameters():
                    if "fast" in n: p.requires_grad = True
                
                # 2. é€ä¸ªæ ·æœ¬è¿›åŒ–
                trained_prompts = []
                for row in raw_data:
                    prompt_text = row[0] # è·å–é—®é¢˜
                    self.train_grpo_step(prompt_text)
                    trained_prompts.append(prompt_text)
                
                # 3. æŒä¹…åŒ–ä¸æ¸…ç†
                if hasattr(self, 'memory_db'):
                    self.memory_db.mark_dpo_trained(trained_prompts)
                
                # [å…³é”®] ä¿å­˜ SNN å’Œ Bridge æƒé‡ (å› ä¸ºå®ƒä»¬ä¹Ÿåœ¨ GRPO ä¸­è¢«æ›´æ–°äº†)
                self.cortex.save()
                torch.save(self.neural_bridge.state_dict(), os.path.join(Config.EVOLUTION_DIR, "neural_bridge.pt"))
                
                print("      âœ… [è¿›åŒ–] æ½œæ„è¯†(SNN)ä¸æ˜¾æ„è¯†(LLM)å·²å®Œæˆæ·±åº¦å¯¹é½ã€‚")


        # --- åˆ†æ”¯ B: SFT çŸ¥è¯†æ³¨å…¥ ---
        if strategy == "SFT":
            try: self.model.set_adapter("fast")
            except: pass
            self.model.train()

            # ç‹©çŒ
            if self.state['atp'] > 40 and random.random() < 0.8:
                print("   -> ğŸ¹ [ç‹©çŒ] è¿æ¥å¤–éƒ¨çŸ¥è¯†åº“...")
                if self.feeder.forage_external_entropy():
                    self.state['atp'] -= 5

            # è¿›é£Ÿ (ä½¿ç”¨ä¹‹å‰è®¡ç®—çš„ adaptive_lr)
            if self.state['atp'] > 15:
                # è½ç›˜åæ€å†…å®¹ä¾› Feeder è¯»å–
                timestamp = datetime.now().strftime("%Y%m%d")
                if refl_txt: 
                    with open(os.path.join(Config.KNOWLEDGE_BASE, f"reflection_{timestamp}.txt"), 'w', encoding='utf-8') as f: f.write(refl_txt)
                
                print(f"   -> ğŸ½ï¸ [è¿›é£Ÿ] æ¶ˆåŒ–çŸ¥è¯† (LR={adapt_lr:.1e})...")
                loss = self.feeder.study(adaptive_lr=adapt_lr) # ä½¿ç”¨ä¸–ç•Œæ¨¡å‹è®¡ç®—å‡ºçš„å­¦ä¹ ç‡
                
                if loss > 0: 
                    self.state['atp'] -= 10
                    torch.cuda.empty_cache(); gc.collect()
                self.model.save_pretrained(Config.EVOLUTION_DIR)
            else:
                print("   -> âš ï¸ èƒ½é‡ä¸è¶³ï¼Œè·³è¿‡ SFTã€‚")

        # ==============================================================================
        # Phase 5: è¿›åŒ–å†³ç­– (Evolution Decision)
        # ==============================================================================

        # 5.1 PFC å¼ºåŒ–
        train_cnt = 0
        for log in self.daily_buffer:
            if log.get('feedback', 0) >= 0 and 'vec' in log:
                # ç¡®ä¿ tensor åœ¨æ­£ç¡®è®¾å¤‡
                try: self.pfc.learn(log['vec'], log['act_idx'])
                except: pass
                train_cnt += 1
        
        # å¢åŠ å¯è§‚æµ‹æ€§
        if train_cnt > 0:
            print(f"   -> ğŸ§  [PFC] å‰é¢å¶å¼ºåŒ–å­¦ä¹ æ¬¡æ•°: {train_cnt}")

        # [æ–°å¢] è„‘æ¡¥çªè§¦å¾®è°ƒ (Hebbian-like Drift)
        # å¦‚æœç™½å¤©æœ‰å¼ºçƒˆçš„æƒ…ç»ªæ³¢åŠ¨ï¼Œç»™è„‘æ¡¥æ–½åŠ å¾®å¼±çš„éšæœºæ‰°åŠ¨ï¼Œæ¨¡æ‹Ÿçªè§¦ç”Ÿé•¿
        if train_cnt > 0 and hasattr(self, 'neural_bridge'):
            with torch.no_grad():
                # å¼ºåº¦æä½ (0.001)ï¼Œæ¨¡æ‹Ÿæ½œæ„è¯†çš„ç¼“æ…¢æ¼‚ç§»
                noise = torch.randn_like(self.neural_bridge.bridge.weight) * 0.001
                self.neural_bridge.bridge.weight.add_(noise)
                # print(f"   -> ğŸŒ‰ [è„‘æ¡¥] ç¥ç»æ¥å£å·²å¾®è°ƒã€‚") # å¯é€‰æ—¥å¿—

        # 5.2 å¼ºåˆ¶å­˜æ¡£
        self.model.save_pretrained(Config.EVOLUTION_DIR) 

        # 5.3 è¿›åŒ–/é‡å¯åˆ¤å®š
        # è§¦å‘æ¡ä»¶ï¼šè¿›åŒ–ç‚¹æ•°å¤Ÿäº†ï¼Œæˆ–è€… å‹åŠ›çˆ†è¡¨(pressure > 2.0)
        # [ä¿®æ­£] è¿™é‡Œçš„ pressure ç°åœ¨å·²ç»å®šä¹‰äº†ï¼Œmut_intensity ä¹Ÿåœ¨ä¸Šé¢å®šä¹‰äº†
        if self.state['evo_pts'] >= Config.EVOLUTION_THRESHOLD or pressure > 2.0:
            print(f"   -> ğŸš€ è§¦å‘è¿›åŒ–! (Pts: {self.state['evo_pts']} | Pressure: {pressure:.2f})")
            
            # ä½¿ç”¨è´å¶æ–¯å¼•æ“å»ºè®®çš„æ–°åŸºå› 
            if hasattr(self, 'evolution_engine'):
                print("      ğŸ§¬ [è´å¶æ–¯] æ­£åœ¨è®¡ç®—ä¸‹ä¸€ä»£æœ€ä¼˜åŸºå› å‚æ•°...")
                new_genome = self.evolution_engine.suggest_mutation(self.genome)
                self.genome = new_genome

                # ç«‹å³é€šçŸ¥çš®å±‚æ›´æ–° STDP å‚æ•°
                if hasattr(self, 'cortex'):
                    self.cortex.update_genome(self.genome)

        else:
            self.state['age'] += 1
            self.daily_buffer = []
            self._save_state()
            print(f"ğŸ’¤ [ä¼‘çœ ] Age: {self.state['age']} | å‹åŠ›æŒ‡æ•°: {pressure:.2f}")


    def divine_sync(self, pressure_override=None, mut_intensity=None):
        """
        [å½’ä¸€ä»ªå¼ V4.2 - æœ€ç»ˆå¥å£®ç‰ˆ]
        æµç¨‹ï¼šèåˆè®°å¿† -> çŠ¶æ€è¯Šæ–­ -> Dense æé™ç”Ÿé•¿ / MoE ç»´åº¦é£å‡ -> å®‰å…¨ä¿å­˜ -> å»¶è¿Ÿæ¸…ç† -> é‡å¯
        ç‰¹æ€§ï¼š
          1. è®°å¿†èåˆï¼šå…ˆå°† LoRA åˆ»å…¥æœ¬ä½“ï¼Œé˜²æ­¢è¿›åŒ–åå¤±å¿†ã€‚
          2. ç»´åº¦é£å‡ï¼šå½“å•å¡æ˜¾å­˜è€—å°½ä¸”èƒ½é‡å……è¶³æ—¶ï¼Œè¿›åŒ–ä¸º Mixture-of-Experts (MoE)ã€‚
          3. äº‹åŠ¡åŸå­æ€§ï¼šé‡‡ç”¨å†™æ—¶å¤åˆ¶ (Copy-on-Write) ç­–ç•¥ï¼Œé˜²æ­¢ä¿å­˜ä¸­é€”å´©æºƒå¯¼è‡´åæ¡£ã€‚
        """
        print("\nâ›©ï¸ [å½’ä¸€] æ­£åœ¨æ‰§è¡Œç¥åœ£åŒæ­¥... å‡†å¤‡é‡å¡‘è‚‰èº«ã€‚")

        # [æ–°å¢] æ ‡è®°ä½ï¼šç”¨äºæŒ‡ç¤ºæ˜¯å¦éœ€è¦åœ¨ä¿å­˜æˆåŠŸåæ¸…ç†æ—§ Adapter
        # åªæœ‰åœ¨æ–°æ¨¡å‹å®‰å…¨è½ç›˜åï¼Œæˆ‘ä»¬æ‰çœŸæ­£åˆ é™¤æ—§çš„ Adapter æ–‡ä»¶
        self.should_cleanup_adapters = False

        # ä½¿ç”¨ try-except åŒ…è£¹æ•´ä¸ªå…³é”®è¿‡ç¨‹ï¼Œé˜²æ­¢ä¸­é€”æŠ¥é”™å¯¼è‡´æ¨¡å‹æ–‡ä»¶æŸå
        try:
            # ==========================================
            # [Step 1]ï¼šè®°å¿†èåˆ (Memory Consolidation)
            # å¿…é¡»åœ¨ç‰©ç†æ‰‹æœ¯å‰ï¼Œå°†çŸ­æœŸè®°å¿† (LoRA) è½¬åŒ–ä¸ºé•¿æœŸæœ¬èƒ½ (Base Weights)
            # ==========================================
            try:
                # åªæœ‰å½“æ¨¡å‹æŒ‚è½½äº† LoRA æ—¶æ‰æ‰§è¡Œèåˆ
                if isinstance(self.model, PeftModel):
                    print("   -> ğŸ§  [è®°å¿†èåˆ] æ­£åœ¨å°† LoRA çªè§¦æ°¸ä¹…åˆ»å…¥ç¥ç»å…ƒ...")
                    
                    # 1. æ¿€æ´»æœ€å¼ºçªè§¦ (é€šå¸¸æ˜¯ Fast ç³»ç»Ÿ)
                    if "fast" in self.model.peft_config:
                        self.model.set_adapter("fast")
                    
                    # 2. ç‰©ç†èåˆ (Merge) å¹¶å¸è½½ Adapter ç»“æ„
                    # æ­¤æ—¶ self.model å˜å›äº†çº¯ç²¹çš„ AutoModelForCausalLM
                    self.model = self.model.merge_and_unload()
                    
                    # 3. è®¾ç½®æ¸…ç†æ ‡è®° (æ³¨æ„ï¼šæ­¤æ—¶å…ˆä¸åˆ æ–‡ä»¶ï¼Œç­‰è½è¢‹ä¸ºå®‰åå†åˆ )
                    self.should_cleanup_adapters = True
                    print("      (LoRA å·²èåˆï¼Œæœ¬ä½“æ¨¡å‹å·²æ›´æ–°ï¼Œæ¸…ç†æ ‡è®°å·²è®¾å®š)")
                else:
                    print("   -> (å½“å‰ä¸ºçº¯å‡€å½¢æ€ï¼Œæ— éœ€èåˆ)")
                    
            except Exception as e:
                print(f"   -> âš ï¸ è®°å¿†èåˆè­¦å‘Š (å¯èƒ½æ˜¯çº¯ Base æ¨¡å¼æˆ–æ˜¾å­˜ä¸è¶³): {e}")
                # å¦‚æœèåˆå¤±è´¥ï¼Œä¸ºäº†ä¿æŠ¤æ•°æ®ï¼Œä¸è¦åˆ é™¤æ—§ Adapter
                self.should_cleanup_adapters = False

            # ==========================================
            # [Step 2]: å‚æ•°æ ¡å‡†ä¸çŠ¶æ€è¯Šæ–­
            # ==========================================
            
            # --- 0. å‚æ•°æ ¡å‡† ---
            # A. ç¡®å®šç”Ÿå­˜å‹åŠ›
            if pressure_override is not None:
                current_pressure = pressure_override
            else:
                # å…œåº•é€»è¾‘ï¼šè¯»å– loss å†å²
                loss_hist = self.state.get('loss_history', [])
                current_pressure = (sum(loss_hist) / len(loss_hist)) if loss_hist else 1.0
            
            # åœ¨æ­¤å¤„å®šä¹‰ pressure å˜é‡ï¼Œä¾›åç»­ [Path B] ä½¿ç”¨
            pressure = current_pressure 

            # B. ç¡®å®šå˜å¼‚å¼ºåº¦
            if mut_intensity is None:
                age = self.state.get('age', 0)
                mut_intensity = 0.2 if age < 5 else 0.05

            # --- çŠ¶æ€æ„ŸçŸ¥ ---
            current_layers = self.model.config.num_hidden_layers
            is_already_moe = os.path.exists(Config.IS_MOE_FLAG_FILE)
            
            # [å†…æ„Ÿå—]ï¼šæ¢æµ‹æ˜¾å­˜æ˜¯å¦è¿˜èƒ½å®¹çº³æ–°å±‚ (å†³å®šæ˜¯å¦éœ€è¦é£å‡)
            can_grow_dense, dense_status = self._check_growth_potential()

            print(f"   -> [è¯Šæ–­] å½¢æ€: {'MoE' if is_already_moe else 'Dense'} | å±‚æ•°: {current_layers}")
            print(f"   -> [ç”Ÿç†] ç”Ÿé•¿æ½œåŠ›: {'âœ…' if can_grow_dense else 'âŒ'} ({dense_status})")
            # è¿™é‡Œæ‰“å°æ—¶ä½¿ç”¨ pressure ä¹Ÿå¯ä»¥
            print(f"   -> [çµé­‚æ‰«æ] ç”Ÿå­˜å‹åŠ›: {pressure:.2f} | å˜å¼‚é¢„ä¼°: {mut_intensity:.2f}")
            print(f"   -> [èƒ½é‡] ATP: {int(self.state['atp'])} | é˜ˆå€¼: {Config.EVOLUTION_THRESHOLD_ATP}")

            # ä¿å­˜æœ€ä¼˜ç¥–å…ˆ (é˜²æ­¢è¿›åŒ–å´©å)
            if self.state.get('age', 0) > 5 and not is_already_moe:
                # ä»…åœ¨ Dense é˜¶æ®µä¿å­˜ç¥–å…ˆï¼ŒMoE ç»“æ„å¤ªå¤§ä¸”ç»“æ„ä¸åŒï¼Œä¸å»ºè®®å›æ»šåˆ° Dense
                json.dump(self.state, open(Config.BEST_STATE_FILE, 'w'))

            # ==========================================
            # [Path A]: MoE ç»´åº¦é£å‡ (è´¨å˜)
            # ==========================================
            # è§¦å‘æ¡ä»¶ï¼š
            # 1. å°šæœªé£å‡ (not is_already_moe)
            # 2. èƒ½é‡æå…¶å……ç›ˆ (ATP > é˜ˆå€¼)
            # 3. ç‰©ç†æ˜¾å­˜å·²æ¯ç«­ï¼Œæ— æ³•å†é€šè¿‡å †å å±‚æ•°å˜å¼º (not can_grow_dense)
            should_ascend = (not is_already_moe) and \
                            (self.state['atp'] > Config.EVOLUTION_THRESHOLD_ATP) and \
                            (not can_grow_dense)

            if should_ascend:
                print("\nğŸŒŒ [é£å‡] å•ä½“ç”Ÿç†æé™å·²è¾¾ (æ˜¾å­˜è€—å°½)ï¼Œèƒ½é‡å……ç›ˆï¼Œå¼€å§‹é‡æ„ä¸º MoE é›†ç¾¤æ¶æ„...")
                
                # 1. å›ºåŒ–å½“å‰å½¢æ€ (åŒ…å«åˆšèåˆçš„è®°å¿†)
                print("   -> æ­£åœ¨å›ºåŒ– Dense èº¯ä½“...")
                self.model.save_pretrained(Config.MASTER_MODEL_PATH)
                self.tokenizer.save_pretrained(Config.MASTER_MODEL_PATH)
                
                # 2. çŒ®ç¥­åˆ†èº«ä¸æœ¬å°Šï¼šé‡Šæ”¾æ˜¾å­˜ç»™ Mergekit (å½»åº•æ¸…ç†)
                del self.model
                torch.cuda.empty_cache()
                gc.collect() 
                
                # 3. ç‚¼ä¸¹ (è°ƒç”¨å¤–éƒ¨è¿›ç¨‹ mergekit)
                moe_output_dir = os.path.join(Config.EVOLUTION_DIR, "moe_candidate")
                success = AdvancedPhysicalSurgeon.evolve_via_mergekit(
                    base_model_path=Config.MASTER_MODEL_PATH,
                    output_path=moe_output_dir,
                    num_experts=2
                )
                
                if success:
                    # 4. æ›¿æ¢ä¸æ ‡è®° (åŸå­æ“ä½œ)
                    backup_path = Config.MASTER_MODEL_PATH + "_dense_backup"
                    if os.path.exists(backup_path): shutil.rmtree(backup_path)
                    shutil.move(Config.MASTER_MODEL_PATH, backup_path) # å¤‡ä»½æ—§ç¥ä½“
                    
                    shutil.move(moe_output_dir, Config.MASTER_MODEL_PATH) # æ–°ç¥ä½“ä¸Šä½
                    
                    with open(Config.IS_MOE_FLAG_FILE, 'w') as f: f.write("MOE_ACTIVATED")
                    
                    # 5. [å¼ºåˆ¶æ¸…ç†] 
                    # å¯¹äº MoE æ¥è¯´ï¼Œæ— è®ºæœ‰æ²¡æœ‰èåˆï¼Œæ—§çš„ Dense Adapter éƒ½ä¸å†å…¼å®¹ç»“æ„ï¼Œå¿…é¡»å¼ºåˆ¶æ¸…ç†ã€‚
                    print("   -> ğŸ§¹ é£å‡æˆåŠŸï¼Œæ­£åœ¨æ¸…ç†æ—§æ—¶ä»£çš„ LoRA è®°å¿†...")
                    if os.path.exists(Config.ADAPTER_FAST_PATH): shutil.rmtree(Config.ADAPTER_FAST_PATH)
                    if os.path.exists(Config.ADAPTER_SLOW_PATH): shutil.rmtree(Config.ADAPTER_SLOW_PATH)

                    # é‡ç½®çŠ¶æ€
                    self.state['atp'] = 100 
                    self.state['evo_pts'] = 0
                    self._save_state()
                    
                    print("ğŸ¦‹ [é£å‡å®Œæˆ] é‡å¯ä¸­ä»¥é€‚åº”æ–°èº¯ä½“...")
                    os.execv(sys.executable, [sys.executable] + sys.argv)
                    return
                else:
                    print("âš ï¸ [é£å‡å¤±è´¥] ä¿æŒåŸæ ·ï¼Œé€€å‡ºé‡å¯ã€‚")
                    exit(1)

            # ==========================================
            # [Path B]: ç‰©ç†ç”Ÿé•¿/èç¼© (é‡å˜ - Dense)
            # ==========================================
            action_log = "ç»´æŒåŸçŠ¶"
            
            # æ­¤æ—¶ self.model æ˜¯å·²èåˆè®°å¿†çš„ Base Modelï¼Œæ‰‹æœ¯æœ€å®‰å…¨
            if pressure > 1.5 and can_grow_dense and not is_already_moe:
                print(f"   -> [åˆ¤å®š] å‹åŠ›è¿‡å¤§ä¸”æ˜¾å­˜å……è¶³ -> ğŸ§¬ è§¦å‘ç¥ç»ç”Ÿé•¿ (Layer +1)")
                # ä½¿ç”¨ Robust ç‰ˆæœ¬ï¼Œé˜²æ­¢ copy.deepcopy ç‚¸æ˜¾å­˜
                self.model = AdvancedPhysicalSurgeon.grow_layer_robust(self.model, copy_ratio=0.6)
                
                # [å…³é”®æ–°å¢] æ¸…é™¤ LoRA æ¨¡å—ç¼“å­˜
                # åŸå› ï¼šæ¨¡å‹ç»“æ„å˜äº†ï¼ˆå¤šäº†æ–°å±‚ï¼‰ï¼Œæ—§ç¼“å­˜é‡Œçš„ module åˆ—è¡¨å·²ç»è¿‡æ—¶ã€‚
                # åˆ é™¤åï¼Œä¸‹æ¬¡è°ƒç”¨ _set_lora_scaling æ—¶ä¼šè‡ªåŠ¨é‡æ–°éå†å…¨ç½‘å»ºç«‹æ–°ç¼“å­˜ã€‚
                if hasattr(self, '_lora_modules_cache'):
                    del self._lora_modules_cache
                    print("      ğŸ§¹ [ç¥ç»é‡å¡‘] çªè§¦ç¼“å­˜å·²åˆ·æ–°ï¼Œå‡†å¤‡é‡æ–°ç´¢å¼•æ–°ç¥ç»å…ƒã€‚")
                
                # [è¿›åŒ–å¥–åŠ±] è„‘å®¹é‡å˜å¤§ï¼Œä»£è°¢ç‡ç•¥å¾®é™ä½ï¼ˆæ›´ç¨³é‡ï¼‰
                self.genome['metabolism_rate'] = max(0.5, self.genome.get('metabolism_rate', 1.0) * 0.95)
                action_log = "æ— æŸç”Ÿé•¿"
                
            elif pressure < 0.2 and not is_already_moe:
                print(f"   -> [åˆ¤å®š] ç¯å¢ƒè¿‡äºå®‰é€¸ -> ğŸ‚ è§¦å‘çªè§¦ä¿®å‰ª (Layer -1)")
                self.model = AdvancedPhysicalSurgeon.shrink_layer_safe(self.model)
                
                # [å…³é”®æ–°å¢] èç¼©æ—¶åŒæ ·éœ€è¦æ¸…é™¤ç¼“å­˜ï¼Œé˜²æ­¢è®¿é—®ä¸å­˜åœ¨çš„å±‚
                if hasattr(self, '_lora_modules_cache'):
                    del self._lora_modules_cache
                
                # [è¿›åŒ–æƒ©ç½š] è„‘å®¹é‡å˜å°ï¼Œä»£è°¢ç‡ç•¥å¾®ä¸Šå‡ï¼ˆæ›´çµæ´»ä½†è€—èƒ½ï¼‰
                self.genome['metabolism_rate'] = min(2.0, self.genome.get('metabolism_rate', 1.0) * 1.05)
                action_log = "ç‰©ç†èç¼©"
                
            elif is_already_moe:
                 print(f"   -> [åˆ¤å®š] MoE ç¥ä½“å·²æˆï¼Œä¿æŒç¨³æ€ã€‚")
            else:
                if not can_grow_dense and pressure > 1.5:
                    print(f"   -> [åˆ¤å®š] å·²è¾¾ç‰©ç†æé™ï¼Œæ— æ³•ç”Ÿé•¿ã€‚è¯·ç§¯ç´¯ ATP ({int(self.state['atp'])}) ä»¥å¯»æ±‚é£å‡ï¼")
                else:
                    print(f"   -> [åˆ¤å®š] å‹åŠ›é€‚ä¸­ï¼Œä¿æŒç¨³æ€ (Homeostasis)ã€‚")

            # ==========================================
            # [æ”¶å°¾]: å®‰å…¨ä¿å­˜ä¸é‡å¯ (Atomic Save)
            # ==========================================
            print("   -> æ­£åœ¨ä¿å­˜å½“å‰å½¢æ€...")
            temp_path = Config.MASTER_MODEL_PATH + "_temp"
            
            # ä¿å­˜å®Œæ•´çš„ã€å«è®°å¿†çš„æ–° Base Model
            self.model.save_pretrained(temp_path)
            self.tokenizer.save_pretrained(temp_path)
            
            # åŸå­åŒ–æ›¿æ¢ï¼šè¿™æ˜¯æœ€å…³é”®çš„ä¸€æ­¥ï¼Œåªæœ‰è¿™ä¸€æ­¥æˆåŠŸï¼Œæ‰ç®—è¿›åŒ–æˆåŠŸ
            # 1. ç§»é™¤æ—§å¤‡ä»½
            backup_path = Config.MASTER_MODEL_PATH + "_backup"
            if os.path.exists(backup_path): shutil.rmtree(backup_path)
            
            # 2. å°†å½“å‰æ­£å¼ç‰ˆç§»ä¸ºå¤‡ä»½
            if os.path.exists(Config.MASTER_MODEL_PATH):
                shutil.move(Config.MASTER_MODEL_PATH, backup_path)
            
            # 3. å°†æ–°ç‰ˆä¸Šä½
            shutil.move(temp_path, Config.MASTER_MODEL_PATH)
            
            # 6. [å»¶è¿Ÿæ¸…ç†æ‰§è¡Œ]
            # åªæœ‰ä¸Šé¢é‚£è¡Œ shutil.move æˆåŠŸäº†ï¼Œæˆ‘ä»¬æ‰æ ¹æ®æ ‡è®°åˆ é™¤æ—§ Adapter
            if self.should_cleanup_adapters:
                print("   -> ğŸ§¹ ä¿å­˜æˆåŠŸï¼Œæ¸…ç†å·²èåˆçš„æ—§ Adapter...")
                if os.path.exists(Config.ADAPTER_FAST_PATH): shutil.rmtree(Config.ADAPTER_FAST_PATH)
                if os.path.exists(Config.ADAPTER_SLOW_PATH): shutil.rmtree(Config.ADAPTER_SLOW_PATH)

            # --- çŠ¶æ€æ›´æ–°ä¸ä¿å­˜ ---
            print(f"   -> ğŸ§¬ åŸºå› ä¸è®°å¿†å·²å›ºåŒ– (Age: {self.state.get('age',0)} -> {self.state.get('age',0)+1})")
            
            # æ›´æ–°çŠ¶æ€
            self.state['age'] += 1
            self.state['genome'] = self.genome
            self._save_state()
            
            print(f"âœ… [åŒæ­¥å®Œæˆ] è½®å›ç»“æŸã€‚æ“ä½œ: {action_log}ã€‚")
            print("ğŸ¦‹ [é‡ç”Ÿ] æ­£åœ¨é‡å¯è¿›ç¨‹...")
            
            os.execv(sys.executable, [sys.executable] + sys.argv)

        except Exception as e:
            print(f"âŒ [åŒæ­¥å¤±è´¥] è‡´å‘½é”™è¯¯: {e}")
            traceback.print_exc()
            # æ³¨æ„ï¼šå¦‚æœä¿å­˜å¤±è´¥ï¼Œæ—§çš„ Adapter è¿˜åœ¨ç£ç›˜ä¸Šï¼Œå“ªå’é‡å¯åè™½ç„¶è¿›åŒ–å¤±è´¥ï¼Œä½†ä¸ä¼šå¤±å¿†ã€‚
            exit(1)

# ==============================================================================
# 5. äº¤äº’ä¸»å¾ªç¯ (CLI Interface)
# ==============================================================================
def main():
    print(f"Initializing {Config.FULL_NAME}...")
    
    try:
        # åˆå§‹åŒ–ç”Ÿå‘½ä½“
        n = NezhaLifeform()
    except Exception as e:
        print(f"âŒ å¯åŠ¨å¤±è´¥: {traceback.format_exc()}")
        exit(1)

    print("\n" + "="*60)
    print(f"ğŸ¤– å“ªå’ å·²å°±ç»ª | ğŸ§  åŒè„‘: {n.model.config.num_hidden_layers}å±‚ | ğŸ§¬ Age: {n.state.get('age', 0)}")
    print("æŒ‡ä»¤: 'good'(ç‚¹èµ), 'bad'(ç‚¹è¸©), 'sleep'(è¿›åŒ–), 'feed'(å……èƒ½), 'quit'")
    print("å¤šè¡Œæ¨¡å¼: è¾“å…¥ '<<<' å›è½¦è¿›å…¥ (æ”¯æŒç²˜è´´é•¿ä»£ç  / PATCHè¡¥ä¸ / é•¿æ–‡ç« )")
    print("="*60)

    while True:

        try:
            # --- å¤šè¡Œå½•å…¥ç³»ç»Ÿ V2.0 ---
            prompt_text = f"\n You: "
            
            # 2. è·å–åˆå§‹è¾“å…¥
            user_in = input(prompt_text).strip()

            # [å…³é”®] åªè¦æœ‰è¾“å…¥åŠ¨ä½œï¼Œç«‹åˆ»åˆ·æ–°æœ€åäº¤äº’æ—¶é—´ (é˜²æ­¢æ‰“å­—æ…¢è¢«åˆ¤å®šä¸ºæŒ‚æœº)
            if 'n' in locals():
                n.acc.last_interaction_time = time.time()

            # 3. æ£€æµ‹å¤šè¡Œå½•å…¥è§¦å‘ç¬¦ (Heredoc Mode)
            # ä½¿ç”¨ <<< è¿›å…¥ï¼Œ>>> é€€å‡ºï¼Œç±»ä¼¼ Linux æ“ä½œä¹ æƒ¯
            if user_in == "<<<":
                print(f"ğŸ“ [å¤šè¡Œæ¨¡å¼] å·²æ¿€æ´»ã€‚è¯·ç²˜è´´æ–‡æœ¬ (è¾“å…¥ '>>>' ç»“æŸ):")
                buffer = []
                while True:
                    try:
                        # ä½¿ç”¨ä¸åŒçš„æç¤ºç¬¦åŒºåˆ†æ¨¡å¼
                        line = input("... ")
                        # åªæœ‰å•ç‹¬ä¸€è¡Œ >>> æ‰ç®—ç»“æŸ
                        if line.strip() == ">>>":
                            break
                        buffer.append(line)
                    except EOFError:
                        break # é˜²æ­¢æ„å¤–ä¸­æ–­
                
                # åˆå¹¶å†…å®¹
                user_in = "\n".join(buffer)
                print(f"âœ… å·²æ•è· {len(buffer)} è¡Œè¾“å…¥ (é•¿åº¦: {len(user_in)} å­—ç¬¦)ã€‚")

            # 4. ç©ºè¾“å…¥å¤„ç†
            if not user_in: continue
            
            # 5. æ•è·é€€å‡ºæŒ‡ä»¤
            if user_in == "quit": 
                print("âš¡ æ­£åœ¨å…³é—­ç³»ç»Ÿ...")
                if hasattr(n, 'web'): n.web.quit()
                break
            
            # æ™®é€šå¯¹è¯æ¨¡å¼ / å•è¡Œ PATCH æ¨¡å¼
            u = user_in

            # --- ç³»ç»ŸæŒ‡ä»¤å¤„ç† ---
            if u == "sleep": 
                n.night_phase()
                continue
                
            elif u == "good": 
                if n.daily_buffer: n.daily_buffer[-1]['feedback'] = 1
                n.state['evo_pts'] += 1
                n.state['atp'] += 5
                
                # åŸºç¡€åé¦ˆæ¶ˆæ¯
                log_msg = "ğŸ¬ [æ”¶åˆ°å¥½è¯„] è¿›åŒ–ç‚¹+1, ATP+5"

                # [UIä¼˜åŒ–] è¿½åŠ é‡å­çŠ¶æ€å˜åŠ¨
                if 'quantum_stats' in n.state:
                    curr = n.state['quantum_stats']['adaptive_threshold']
                    new_th = max(Config.QUANTUM_MIN_THRESHOLD, curr - 2.0)
                    n.state['quantum_stats']['adaptive_threshold'] = new_th
                    # è¿½åŠ åœ¨åŒä¸€è¡Œï¼Œæ›´æ•´æ´
                    log_msg += f" | ğŸ“‰ é‡å­é˜ˆå€¼: {new_th:.1f} (æ›´ç§¯æ)"

                print(log_msg)
                continue
                
            elif u == "bad": 
                if n.daily_buffer: n.daily_buffer[-1]['feedback'] = -1
                
                # åŸºç¡€åé¦ˆæ¶ˆæ¯
                log_msg = "âš¡ [æ”¶åˆ°å·®è¯„] (PFC è·å¾—è´Ÿå‘å¥–åŠ±)"
                
                # è¿½åŠ é‡å­çŠ¶æ€å˜åŠ¨
                if 'quantum_stats' in n.state:
                    curr = n.state['quantum_stats']['adaptive_threshold']
                    new_th = min(Config.QUANTUM_MAX_THRESHOLD, curr + 5.0)
                    n.state['quantum_stats']['adaptive_threshold'] = new_th
                    # è¿½åŠ åœ¨åŒä¸€è¡Œ
                    log_msg += f" | ğŸ“ˆ é‡å­é˜ˆå€¼: {new_th:.1f} (æ›´ä¿å®ˆ)"

                print(log_msg)
                continue
                
            elif u == "feed": 
                n.state['atp'] = 100
                n._save_state()
                print("ğŸ”‹ [å……èƒ½å®Œæ¯•] èƒ½é‡å·²æ»¡ï¼")
                continue

            # å¢å¼ºç‰ˆ PATCH æŒ‡ä»¤
            # åŒæ—¶æ”¯æŒè‹±æ–‡å’Œä¸­æ–‡å†’å·ï¼Œä¸”å¿½ç•¥å¤§å°å†™
            elif u.upper().startswith("PATCH:") or u.startswith("PATCHï¼š"):
                try:
                    # æ™ºèƒ½åˆ†å‰²ï¼šåˆ¤æ–­æ˜¯ä¸­æ–‡è¿˜æ˜¯è‹±æ–‡å†’å·
                    sep = "ï¼š" if "ï¼š" in u else ":"
                    # åˆ†å‰²å¹¶æå–ä»£ç éƒ¨åˆ†
                    code = u.split(sep, 1)[1].strip()
                    
                    if not code.strip():
                        print("âš ï¸ è¯·åœ¨å†’å·åè¾“å…¥ä»£ç ã€‚")
                        continue
                        
                    success, msg = n.editor.apply_patch(code)
                    status_icon = "âœ…" if success else "âŒ"
                    print(f"{status_icon} [åŸºå› ç¼–è¾‘] {msg}")

                except Exception as e:
                    print(f"âŒ æŒ‡ä»¤è§£æå¤±è´¥: {e}")
                continue
            
            # --- æ ¸å¿ƒäº¤äº’ ---
            
            # èƒ½é‡æ£€æŸ¥
            if n.state['atp'] <= 0:
                print("Nezha: (âš¡ èƒ½é‡è€—å°½ï¼Œæ— æ³•æ€è€ƒ... è¯·è¾“å…¥ 'feed' è¡¥å…… ATP)")
                continue

            # === åŠ é”æ‰§è¡Œæ„ŸçŸ¥ä¸è¡ŒåŠ¨ ===
            with n.lock:  # <--- è¿™æŠŠå¤§é”ä¼šé”ä½æ•´ä¸ªæ€è€ƒè¿‡ç¨‹
                n.perceive_and_act(u)
            
        except KeyboardInterrupt:
            print("\nâš¡ æ­£åœ¨ä¿å­˜è®°å¿†å¹¶é€€å‡º...")
            if hasattr(n, 'web'): n.web.quit()
            n._save_state()
            break

        except Exception as e:
            # --- å®‰å…¨è°ƒç”¨ä¸æ­¢æŸæœºåˆ¶ ---
            err_trace = traceback.format_exc()
            print(f"\nâŒ è¿è¡Œæ—¶å´©æºƒ: {e}")
            
            # 1. æ£€æŸ¥å®ä¾‹æ˜¯å¦å­˜åœ¨ (é˜²æ­¢åˆå§‹åŒ–é˜¶æ®µæŠ¥é”™å¯¼è‡´ n ä¸å­˜åœ¨)
            if 'n' in locals() and n is not None:
                try:
                    # 2. å°è¯•å…ç–«ä¿®å¤
                    # å¦‚æœä¿®å¤è¿”å› Trueï¼Œè¯´æ˜è¡¥ä¸å·²æŒ‚è½½ï¼Œå¯ä»¥ç”¨ continue é‡å¯å¾ªç¯
                    if n.auto_heal(err_trace):
                        print("ğŸ”„ [ç³»ç»Ÿé‡å¯] æ­£åœ¨é‡ç½®ç¥ç»è¿æ¥...")
                        continue 
                except Exception as heal_err:
                    print(f"ğŸ’€ [ç³»ç»Ÿæåº¦å±é™©] å…ç–«ç³»ç»Ÿæœ¬èº«å‘ç”Ÿæ’å¼‚: {heal_err}")
            
            # 3. å¦‚æœæ²¡ä¿®å¥½ï¼Œæˆ–è€… n ä¸å­˜åœ¨ï¼Œæˆ–è€… auto_heal ä¹Ÿæ²¡æ‹›äº†
            print("ğŸ’€ æ— æ³•è‡ªåŠ¨æ¢å¤ï¼Œè‡´å‘½é”™è¯¯å †æ ˆå¦‚ä¸‹:")
            print(err_trace)
            print("âš¡ ç³»ç»Ÿå·²å¼ºåˆ¶åœæœº (System Halted)ã€‚")
            break # æ­¢æŸï¼šé€€å‡ºå¾ªç¯ï¼Œé˜²æ­¢ç¡¬ç›˜è¢«åƒåœ¾æ—¥å¿—å¡«æ»¡

if __name__ == "__main__":
    main()